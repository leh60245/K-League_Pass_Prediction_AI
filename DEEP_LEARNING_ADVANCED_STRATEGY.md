# ë”¥ëŸ¬ë‹ ê³ ê¸‰ ì „ëµ: ì „ë¬¸ê°€ ë¶„ì„ ë° í•´ê²° ë°©ì•ˆ

## ğŸ” í˜„ì¬ ìƒí™© ì‹¬ì¸µ ë¶„ì„

### ì‹¤í—˜ ê²°ê³¼
```
LightGBM V4:    1.50m  âœ…
LSTM Original:  15.98m âŒ
LSTM Fixed:     14.78m âŒ (1.2m ê°œì„ , ì—¬ì „íˆ 10ë°° ì°¨ì´)
```

### ì˜ˆì¸¡ê°’ ë¶„ì„
```python
# LSTM ì˜ˆì¸¡ í†µê³„
end_x: í‰ê·  65.38m, í‘œì¤€í¸ì°¨ 18.73
end_y: í‰ê·  35.46m, í‘œì¤€í¸ì°¨ 19.99

# ë¬¸ì œì 
â†’ ê±°ì˜ ë¹„ìŠ·í•œ ê°’(ì¤‘ì•™ê°’)ë§Œ ì˜ˆì¸¡
â†’ ì‹¤ì œ íŒ¨í„´ í•™ìŠµ ì‹¤íŒ¨
â†’ "ì•ˆì „í•œ í‰ê· ê°’ìœ¼ë¡œ íšŒê·€" í˜„ìƒ
```

---

## ğŸš¨ ê·¼ë³¸ ì›ì¸ (Root Cause)

### 1. **Wide Formatì˜ ì¹˜ëª…ì  í•œê³„** â­â­â­â­â­
```python
# í˜„ì¬ ë°ì´í„° êµ¬ì¡°
[start_x_0, start_x_1, ..., start_x_19,  # 20ê°œ ì»¬ëŸ¼
 start_y_0, start_y_1, ..., start_y_19,  # 20ê°œ ì»¬ëŸ¼
 ...]

# ë¬¸ì œ
â†’ LSTMì€ (batch, seq_len, features)ë¥¼ ì›í•¨
â†’ ì‹œê°„ ì¶•ì´ feature ì¶•ì— í¼ì³ì ¸ ìˆìŒ
â†’ LSTMì˜ recurrent íŠ¹ì„±ì„ ì „í˜€ í™œìš© ëª»í•¨!
```

**ë¹„ìœ :**
- LightGBM: í‰ë©´ ì§€ë„ë¥¼ ë³´ê³  ê¸¸ ì°¾ê¸° (ê°€ëŠ¥)
- LSTM: í‰ë©´ ì§€ë„ë¥¼ "ì‹œê°„ ìˆœì„œ"ë¡œ í•™ìŠµí•˜ë ¤ í•¨ (ë¶ˆê°€ëŠ¥)

### 2. **ë°ì´í„° ë¶€ì¡± (Sample Efficiency)**
```
Train Samples: ~12,000
LSTM Parameters: 300,000+
â†’ ì‹¬ê°í•œ Underfitting
```

LightGBMì€ 15,000 ìƒ˜í”Œë¡œ ì¶©ë¶„í•˜ì§€ë§Œ, ë”¥ëŸ¬ë‹ì€ ìµœì†Œ 100k+ í•„ìš”!

### 3. **ì •ë³´ ì†ì‹¤**
- NaN â†’ 0: íŒ¨ë”©ê³¼ ì‹¤ì œ 0 êµ¬ë¶„ ë¶ˆê°€
- ì •ê·œí™”: ìƒëŒ€ì  ê´€ê³„ ì†ì‹¤
- Wide format: ì‹œê°„ì  ì˜ì¡´ì„± ì™„ì „ ì†ì‹¤

---

## ğŸ’¡ ê³ ê¸‰ í•´ê²° ì „ëµ

### ğŸ¥‡ ì „ëµ A: Long Format ì™„ì „ ì¬ì„¤ê³„ â­â­â­â­â­

**í•µì‹¬:** Wide â†’ True Sequence

#### Before (Wide Format)
```python
# Episode 1: [x0, x1, x2, ..., x19, y0, y1, ...]
# â†’ 1ê°œ ìƒ˜í”Œ, 600+ features
```

#### After (Long Format)
```python
# Episode 1: 
# [(x0, y0, type0, ...),    # ì‹œì  0
#  (x1, y1, type1, ...),    # ì‹œì  1
#  ...
#  (x19, y19, type19, ...)] # ì‹œì  19
# â†’ (seq_len=20, features=15)
```

**íŒŒì¼:** `preprocessing_long_format.py` (ì´ë¯¸ ìƒì„±ë¨)

**ì˜ˆìƒ ê°œì„ :**
- 14.78m â†’ **3~5m** (3~5ë°° ê°œì„ )
- LSTMì´ ì§„ì§œ ì‹œí€€ìŠ¤ë¥¼ í•™ìŠµ!

#### ì‹¤í–‰ ë°©ë²•:
```bash
# 1. Long format ì „ì²˜ë¦¬
python preprocessing_long_format.py

# 2. Long format í•™ìŠµ (ë³„ë„ ì‘ì„± í•„ìš”)
python train_lstm_long_format.py
```

---

### ğŸ¥ˆ ì „ëµ B: Transformer + Attention â­â­â­â­â­

**í•µì‹¬:** Self-Attentionìœ¼ë¡œ ì¤‘ìš”í•œ ì´ë²¤íŠ¸ ìë™ ì„ íƒ

#### Why Transformer?
1. **Parallel Processing**: LSTMë³´ë‹¤ ë¹ ë¦„
2. **Long-range Dependencies**: ë©€ë¦¬ ë–¨ì–´ì§„ ì´ë²¤íŠ¸ë„ ì—°ê²°
3. **Attention Weights**: ì–´ë–¤ ì´ë²¤íŠ¸ê°€ ì¤‘ìš”í•œì§€ ì‹œê°í™” ê°€ëŠ¥

**íŒŒì¼:** `model_transformer.py` (ì´ë¯¸ ìƒì„±ë¨)

**ì£¼ìš” íŠ¹ì§•:**
```python
class TransformerPassPredictor:
    - Positional Encoding (ì‹œê°„ ì •ë³´ ëª…ì‹œ)
    - Multi-Head Attention (ë‹¤ì–‘í•œ íŒ¨í„´)
    - Attention Pooling (ì¤‘ìš” ì´ë²¤íŠ¸ ê°€ì¤‘ì¹˜)
```

**ì˜ˆìƒ ê°œì„ :**
- 14.78m â†’ **2~4m** (3~7ë°° ê°œì„ )
- LightGBMê³¼ ê²½ìŸ ê°€ëŠ¥!

---

### ğŸ¥‰ ì „ëµ C: Knowledge Distillation â­â­â­â­

**í•µì‹¬:** LightGBM(Teacher)ì˜ ì§€ì‹ì„ LSTM(Student)ì— ì „ë‹¬

#### ì•„ì´ë””ì–´
```python
# Step 1: Teacher(LightGBM) ì˜ˆì¸¡
lgbm_pred = lightgbm_model.predict(X)  # 1.5m ì„±ëŠ¥

# Step 2: Student(LSTM) í•™ìŠµ
loss = alpha * MSE(lstm_pred, true_target) + \
       (1-alpha) * MSE(lstm_pred, lgbm_pred)
       
# alpha=0.7: 30%ëŠ” LightGBMì—ì„œ ë°°ì›€
```

**ì¥ì :**
- LightGBMì˜ ì¢‹ì€ ì˜ˆì¸¡ì„ "íŒíŠ¸"ë¡œ í™œìš©
- Soft targetsë¡œ í•™ìŠµ ì•ˆì •í™”

**ì˜ˆìƒ ê°œì„ :**
- 14.78m â†’ **5~8m** (2~3ë°° ê°œì„ )

**êµ¬í˜„:**
```python
class DistillationLoss(nn.Module):
    def __init__(self, alpha=0.7, temperature=2.0):
        super().__init__()
        self.alpha = alpha
        self.temperature = temperature
    
    def forward(self, student_pred, true_target, teacher_pred):
        # Hard target loss
        hard_loss = F.mse_loss(student_pred, true_target)
        
        # Soft target loss (with temperature)
        soft_loss = F.mse_loss(
            student_pred / self.temperature,
            teacher_pred / self.temperature
        )
        
        return self.alpha * hard_loss + (1 - self.alpha) * soft_loss
```

---

### ğŸ† ì „ëµ D: ì•™ìƒë¸”ì˜ ìµœì¢… ì§„í™” â­â­â­â­â­

**í•µì‹¬:** LightGBM + (Long LSTM) + (Transformer)

#### Level 1: ë‹¨ìˆœ í‰ê· 
```python
final = 0.7 * lgbm + 0.2 * lstm + 0.1 * transformer
```

#### Level 2: Stacking (Meta-Learner)
```python
# Base models
lgbm_pred = lgbm.predict(X)
lstm_pred = lstm.predict(X)
trans_pred = transformer.predict(X)

# Meta features
meta_X = np.column_stack([lgbm_pred, lstm_pred, trans_pred])

# Meta model (ê°„ë‹¨í•œ Ridge)
meta_model = Ridge(alpha=1.0)
meta_model.fit(meta_X, y_true)

# Final prediction
final = meta_model.predict(meta_X)
```

**ì˜ˆìƒ ìµœì¢… ì„±ëŠ¥:**
- **1.2~1.4m** (LightGBM 1.5më³´ë‹¤ ì¢‹ìŒ!)
- Test ì ìˆ˜: **12~13ì ëŒ€**

---

## ğŸ“Š ì „ëµë³„ ë¹„êµí‘œ

| ì „ëµ | êµ¬í˜„ ë‚œì´ë„ | ì‹œê°„ | ì˜ˆìƒ ê°œì„  | ì„±ê³µ í™•ë¥  | ìš°ì„ ìˆœìœ„ |
|-----|-----------|------|----------|----------|---------|
| **A. Long Format** | ì¤‘ | 2~3ì‹œê°„ | 14.78m â†’ 3~5m | 80% | ğŸ¥‡ 1ìˆœìœ„ |
| **B. Transformer** | ì¤‘ìƒ | 3~4ì‹œê°„ | 14.78m â†’ 2~4m | 70% | ğŸ¥ˆ 2ìˆœìœ„ |
| **C. Distillation** | í•˜ | 1~2ì‹œê°„ | 14.78m â†’ 5~8m | 60% | ğŸ¥‰ 3ìˆœìœ„ |
| **D. Stacking** | í•˜ | 30ë¶„ | 1.5m â†’ 1.2m | 90% | ğŸ† ìµœì¢… |

---

## ğŸš€ ì‹¤í–‰ ë¡œë“œë§µ (ê¶Œì¥)

### Phase 1: Long Format (ìµœìš°ì„ ) â±ï¸ 3ì‹œê°„
```bash
# Step 1: ì „ì²˜ë¦¬
python preprocessing_long_format.py

# Step 2: í•™ìŠµ (ì‘ì„± í•„ìš”)
# â†’ train_lstm_long_format.py ì‘ì„±
python train_lstm_long_format.py

# ì˜ˆìƒ: 3~5m ë‹¬ì„±
```

### Phase 2: Transformer (ë³‘í–‰) â±ï¸ 4ì‹œê°„
```bash
# model_transformer.py ê¸°ë°˜ í•™ìŠµ
python train_transformer.py

# ì˜ˆìƒ: 2~4m ë‹¬ì„±
```

### Phase 3: Stacking (ìµœì¢…) â±ï¸ 30ë¶„
```bash
# LightGBM + Long LSTM + Transformer ì•™ìƒë¸”
python create_stacking_ensemble.py

# ì˜ˆìƒ: 1.2~1.4m ë‹¬ì„±
```

---

## ğŸ’» ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œ

### Quick Win 1: Knowledge Distillation (ê°€ì¥ ë¹ ë¦„)

```python
# train_lstm_v4_distillation.py

# 1. LightGBM ì˜ˆì¸¡ ë¡œë”©
with open('lightgbm_model_v4_5fold.pkl', 'rb') as f:
    lgbm_models = pickle.load(f)

# 2. LightGBM ì˜ˆì¸¡ ìƒì„±
lgbm_pred_x = np.mean([m.predict(X_val) for m in lgbm_models['models_x']], axis=0)
lgbm_pred_y = np.mean([m.predict(X_val) for m in lgbm_models['models_y']], axis=0)
lgbm_pred = np.column_stack([lgbm_pred_x, lgbm_pred_y])

# 3. Distillation Loss
class DistillationLoss(nn.Module):
    def __init__(self, alpha=0.7):
        super().__init__()
        self.alpha = alpha
        self.mse = nn.MSELoss()
    
    def forward(self, student_pred, true_target, teacher_pred):
        hard_loss = self.mse(student_pred, true_target)
        soft_loss = self.mse(student_pred, teacher_pred)
        return self.alpha * hard_loss + (1 - self.alpha) * soft_loss

# 4. í•™ìŠµ
criterion = DistillationLoss(alpha=0.7)
for epoch in range(epochs):
    output = model(X)
    loss = criterion(output, y_true, torch.from_numpy(lgbm_pred).to(device))
    ...
```

### Quick Win 2: Stacking Ensemble

```python
# create_stacking_simple.py

import numpy as np
from sklearn.linear_model import Ridge

# 1. Base model ì˜ˆì¸¡ ë¡œë”©
lgbm_val_pred = np.load('lgbm_val_pred.npy')  # (N, 2)
lstm_val_pred = np.load('lstm_val_pred.npy')  # (N, 2)

# 2. Meta features
meta_X = np.column_stack([
    lgbm_val_pred.flatten(),  # (N*2,)
    lstm_val_pred.flatten()
])  # (N, 4)

# 3. Meta model
meta_model = Ridge(alpha=1.0)
meta_model.fit(meta_X, y_val.flatten())

# 4. Test ì˜ˆì¸¡
lgbm_test_pred = ...
lstm_test_pred = ...
meta_X_test = np.column_stack([lgbm_test_pred.flatten(), lstm_test_pred.flatten()])
final_pred = meta_model.predict(meta_X_test).reshape(-1, 2)

# ì˜ˆìƒ: 1.2~1.4m
```

---

## ğŸ¯ ìµœì¢… ê¶Œì¥ì‚¬í•­

### í˜„ì‹¤ì ì¸ ì ‘ê·¼ (ì‹œê°„ ëŒ€ë¹„ íš¨ê³¼)

1. **ì¦‰ì‹œ (30ë¶„):** Stacking Ensemble
   - LightGBM + LSTM Fixed
   - ì˜ˆìƒ: 1.4m â†’ Test 13ì ëŒ€

2. **ë‹¨ê¸° (3ì‹œê°„):** Long Format
   - ì§„ì§œ ì‹œí€€ìŠ¤ í•™ìŠµ
   - ì˜ˆìƒ: 3~5m â†’ Stacking ì‹œ 1.2m

3. **ì¤‘ê¸° (1ì£¼):** Transformer
   - SOTA ì•„í‚¤í…ì²˜
   - ì˜ˆìƒ: 2~4m â†’ ìµœì¢… 1.0~1.2m

### ì´ìƒì ì¸ ì ‘ê·¼ (ìµœê³  ì„±ëŠ¥)

```
LightGBM (1.5m) + Long LSTM (3m) + Transformer (2m)
â†’ Stacking â†’ 1.0~1.2m
â†’ Test: 11~12ì ëŒ€ (Top 10%)
```

---

## ğŸ”¬ ì¶”ê°€ ì‹¤í—˜ ì•„ì´ë””ì–´

### 1. Graph Neural Network (GNN)
- íŒ¨ìŠ¤ë¥¼ Graphë¡œ ëª¨ë¸ë§
- ì„ ìˆ˜ ê°„ ê´€ê³„ë¥¼ Edgeë¡œ í‘œí˜„
- ì˜ˆìƒ: 2~3m

### 2. Temporal Convolutional Network (TCN)
- 1D Convë¡œ ì‹œí€€ìŠ¤ í•™ìŠµ
- LSTMë³´ë‹¤ ë¹ ë¥´ê³  íš¨ê³¼ì 
- ì˜ˆìƒ: 3~5m

### 3. Multi-Task Learning
```python
# ë™ì‹œì— ì—¬ëŸ¬ task í•™ìŠµ
outputs = model(X)
end_x_pred = outputs[:, 0]
end_y_pred = outputs[:, 1]
pass_success_pred = outputs[:, 2]  # ì¶”ê°€ task

loss = mse_loss(end_pred, target) + bce_loss(success_pred, success_label)
```

---

## ğŸ“ ê²°ë¡ 

### í˜„ì¬ LSTMì´ ì•ˆ ë˜ëŠ” ì´ìœ 
1. **Wide Format**: LSTMì˜ ì¥ì ì„ ì „í˜€ í™œìš© ëª»í•¨ (ì¹˜ëª…ì !)
2. **ë°ì´í„° ë¶€ì¡±**: 12k ìƒ˜í”Œì€ ë”¥ëŸ¬ë‹ì— í„±ì—†ì´ ë¶€ì¡±
3. **í‰ê· ê°’ íšŒê·€**: ëª¨ë¸ì´ "ì•ˆì „í•œ ì¤‘ì•™ê°’"ë§Œ í•™ìŠµ

### í•´ê²° ë°©ë²•
1. **Long Format**: Wide ë²„ë¦¬ê³  ì§„ì§œ ì‹œí€€ìŠ¤ ì‚¬ìš© (í•„ìˆ˜!)
2. **Transformer**: Attentionìœ¼ë¡œ ì¤‘ìš” ì´ë²¤íŠ¸ ì„ íƒ
3. **Distillation**: LightGBM ì§€ì‹ í™œìš©
4. **Stacking**: ìµœì¢… ì•™ìƒë¸”

### í˜„ì‹¤ì  ëª©í‘œ
- **Short-term**: Stackingìœ¼ë¡œ 1.4m â†’ Test 13ì ëŒ€
- **Mid-term**: Long Formatìœ¼ë¡œ 3m â†’ Stacking 1.2m â†’ Test 12ì ëŒ€
- **Long-term**: Transformer ì¶”ê°€ â†’ 1.0m â†’ Test 11ì ëŒ€

### ìµœìš°ì„  Action
```bash
# 1. Long Format ì „ì²˜ë¦¬ (ê°€ì¥ ì¤‘ìš”!)
python preprocessing_long_format.py

# 2. Stacking Ensemble (ê°€ì¥ ë¹ ë¦„)
python create_stacking_simple.py
```

---

**ì‘ì„±ì¼**: 2025-12-18  
**ì „ë¬¸ê°€ íŒë‹¨**: Wide Format ë²„ë¦¬ê³  Long Format ì¬ì„¤ê³„ í•„ìˆ˜!  
**ì˜ˆìƒ ìµœì¢… ì„±ëŠ¥**: 1.0~1.2m (LightGBMë³´ë‹¤ ì¢‹ìŒ)

