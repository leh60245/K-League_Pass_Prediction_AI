"""
K-League Pass Prediction - PyTorch LSTM/GRU with Multi-Head Attention (V5)

í•µì‹¬ ê°œì„ ì‚¬í•­:
âœ… Multi-Head Attention ì¶”ê°€ (ì¤‘ìš” ì‹œì  í•™ìŠµ)
âœ… Padding Mask í™œìš© (ì‹¤ì œ ë°ì´í„° êµ¬ë¶„)
âœ… Bidirectional RNN (ì–‘ë°©í–¥ ì •ë³´ í™œìš©)
âœ… ì „ì²´ í”¼ì²˜ ì •ê·œí™”/í‘œì¤€í™”
âœ… ê¹Šì€ Output Head (ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ)
âœ… Residual Connection
âœ… LayerNorm ì¶”ê°€

ì‘ì„±ì¼: 2025-12-19
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import GroupKFold
import warnings
import re
from tqdm import tqdm

warnings.filterwarnings('ignore')

# Device ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”§ Using device: {device}")


class SoccerDatasetV5(Dataset):
    """
    V5: ê³ ë„í™”ëœ Dataset
    - Padding Mask ìƒì„±
    - ì „ì²´ í”¼ì²˜ í‘œì¤€í™”
    - ì¶”ê°€ ì‹œí€€ìŠ¤ í”¼ì²˜ ìƒì„±
    """

    def __init__(self, data, K=20, is_train=True, scaler=None):
        self.data = data.reset_index(drop=True)
        self.K = K
        self.is_train = is_train
        self.scaler = scaler

        # íƒ€ê²Ÿ ì¶”ì¶œ ë° ì •ê·œí™”
        if 'target_x' in data.columns and 'target_y' in data.columns:
            self.targets = data[['target_x', 'target_y']].values.astype(np.float32)
            self.targets[:, 0] /= 105.0
            self.targets[:, 1] /= 68.0
        else:
            self.targets = None

        # ë©”íƒ€ ì •ë³´ ì œì™¸
        exclude_cols = ['game_episode', 'game_id', 'target_x', 'target_y', 'final_team_id']
        feature_data = data.drop(columns=[c for c in exclude_cols if c in data.columns])

        # ì»¬ëŸ¼ ë¶„ë¥˜
        self.numerical_features, self.categorical_features = self._classify_columns(feature_data.columns)

        # 3D í…ì„œ ë³€í™˜
        self.numerical_tensor = self._prepare_numerical_features(feature_data)
        self.categorical_tensor = self._prepare_categorical_features(feature_data)

        # Padding Mask ìƒì„± (ì¤‘ìš”!)
        self.padding_mask = self._create_padding_mask()

        print(f"âœ… Dataset V5 ì¤€ë¹„ ì™„ë£Œ:")
        print(f"   - ìƒ˜í”Œ ìˆ˜: {len(self.data)}")
        print(f"   - ìˆ˜ì¹˜í˜• í”¼ì²˜: {len(self.numerical_features)} â†’ Shape: {self.numerical_tensor.shape}")
        print(f"   - ë²”ì£¼í˜• í”¼ì²˜: {len(self.categorical_features)} â†’ Shape: {self.categorical_tensor.shape}")
        print(f"   - Padding Mask: {self.padding_mask.shape}")

    def _classify_columns(self, columns):
        """ì»¬ëŸ¼ ë¶„ë¥˜"""
        pattern = re.compile(r'^(.+)_(\d+)$')
        feature_names = set()

        for col in columns:
            match = pattern.match(col)
            if match:
                feature_names.add(match.group(1))

        categorical_keywords = ['type_id', 'res_id', 'team_id_enc', 'is_home', 'is_last', 'period_id']
        categorical_features = []
        numerical_features = []

        for feat in sorted(feature_names):
            if any(keyword in feat for keyword in categorical_keywords):
                categorical_features.append(feat)
            else:
                numerical_features.append(feat)

        return numerical_features, categorical_features

    def _prepare_numerical_features(self, data):
        """ìˆ˜ì¹˜í˜• í”¼ì²˜ë¥¼ 3D í…ì„œë¡œ ë³€í™˜ + ì •ê·œí™”"""
        tensors = []

        # ì¢Œí‘œ ì •ê·œí™” í‚¤ì›Œë“œ
        x_coord_keywords = ['start_x', 'end_x', 'dx']
        y_coord_keywords = ['start_y', 'end_y', 'dy']

        for feat_name in self.numerical_features:
            cols = [f"{feat_name}_{i}" for i in range(self.K)]
            cols = [c for c in cols if c in data.columns]

            if not cols:
                continue

            feat_data = data[cols].values.astype(np.float32)

            # íŒ¨ë”©
            if feat_data.shape[1] < self.K:
                padding = np.zeros((feat_data.shape[0], self.K - feat_data.shape[1]), dtype=np.float32)
                feat_data = np.concatenate([feat_data, padding], axis=1)

            # ì •ê·œí™”
            if any(kw in feat_name for kw in x_coord_keywords):
                feat_data = feat_data / 105.0
            elif any(kw in feat_name for kw in y_coord_keywords):
                feat_data = feat_data / 68.0
            elif 'speed' in feat_name.lower():
                # ì†ë„: ìµœëŒ€ê°’ìœ¼ë¡œ ì •ê·œí™” (ì˜ˆ: ìµœëŒ€ 30m/s)
                feat_data = np.clip(feat_data / 30.0, 0, 1)
            elif 'angle' in feat_name.lower() or 'direction' in feat_name.lower():
                # ê°ë„: -Ï€ ~ Ï€ â†’ -1 ~ 1
                feat_data = feat_data / np.pi
            elif 'time' in feat_name.lower():
                # ì‹œê°„ì°¨: ìµœëŒ€ê°’ìœ¼ë¡œ ì •ê·œí™”
                max_time = np.nanmax(feat_data) if not np.all(np.isnan(feat_data)) else 1.0
                if max_time > 0:
                    feat_data = feat_data / max_time

            # NaN â†’ 0.0
            feat_data = np.nan_to_num(feat_data, nan=0.0)

            tensors.append(feat_data)

        result = np.stack(tensors, axis=-1) if tensors else np.zeros((len(data), self.K, 0), dtype=np.float32)
        return torch.from_numpy(result)

    def _prepare_categorical_features(self, data):
        """ë²”ì£¼í˜• í”¼ì²˜ë¥¼ 3D í…ì„œë¡œ ë³€í™˜"""
        tensors = []

        for feat_name in self.categorical_features:
            cols = [f"{feat_name}_{i}" for i in range(self.K)]
            cols = [c for c in cols if c in data.columns]

            if not cols:
                continue

            feat_data = data[cols].values.astype(np.float32)

            if feat_data.shape[1] < self.K:
                padding = np.zeros((feat_data.shape[0], self.K - feat_data.shape[1]), dtype=np.float32)
                feat_data = np.concatenate([feat_data, padding], axis=1)

            feat_data = np.nan_to_num(feat_data, nan=0.0)
            tensors.append(feat_data)

        result = np.stack(tensors, axis=-1) if tensors else np.zeros((len(data), self.K, 0), dtype=np.float32)
        return torch.from_numpy(result).long()

    def _create_padding_mask(self):
        """
        Padding Mask ìƒì„±
        Returns:
            (N, K) bool tensor - True: Padding, False: Valid
        """
        # ëª¨ë“  ìˆ˜ì¹˜í˜• í”¼ì²˜ê°€ 0ì¸ ì‹œì  = Padding
        mask = (self.numerical_tensor.sum(dim=-1) == 0)  # (N, K)
        return mask

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        num_feat = self.numerical_tensor[idx]
        cat_feat = self.categorical_tensor[idx]
        padding_mask = self.padding_mask[idx]

        if self.targets is not None:
            target = torch.from_numpy(self.targets[idx])
            return num_feat, cat_feat, padding_mask, target
        else:
            return num_feat, cat_feat, padding_mask


class SoccerRNNWithAttention(nn.Module):
    """
    V5: Multi-Head Attention + Bidirectional RNN + Residual Connection
    """

    def __init__(self,
                 num_numerical_features,
                 categorical_vocab_sizes,
                 embedding_dims,
                 hidden_dim=256,
                 num_layers=2,
                 dropout=0.3,
                 use_lstm=False,
                 bidirectional=True,
                 num_heads=8):
        super(SoccerRNNWithAttention, self).__init__()

        self.num_numerical_features = num_numerical_features
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.use_lstm = use_lstm
        self.bidirectional = bidirectional
        self.num_heads = num_heads

        # Embedding layers
        self.embeddings = nn.ModuleDict()
        total_embedding_dim = 0

        for feat_name, vocab_size in categorical_vocab_sizes.items():
            emb_dim = embedding_dims[feat_name]
            self.embeddings[feat_name] = nn.Embedding(vocab_size, emb_dim, padding_idx=0)
            total_embedding_dim += emb_dim

        # Input dimension
        input_dim = num_numerical_features + total_embedding_dim

        # Input Projection
        self.input_projection = nn.Linear(input_dim, hidden_dim)
        self.input_norm = nn.LayerNorm(hidden_dim)

        # RNN Layer
        rnn_hidden = hidden_dim
        if use_lstm:
            self.rnn = nn.LSTM(
                hidden_dim,
                rnn_hidden,
                num_layers=num_layers,
                dropout=dropout if num_layers > 1 else 0,
                batch_first=True,
                bidirectional=bidirectional
            )
        else:
            self.rnn = nn.GRU(
                hidden_dim,
                rnn_hidden,
                num_layers=num_layers,
                dropout=dropout if num_layers > 1 else 0,
                batch_first=True,
                bidirectional=bidirectional
            )

        # RNN ì¶œë ¥ ì°¨ì› (ì–‘ë°©í–¥ì´ë©´ 2ë°°)
        rnn_output_dim = rnn_hidden * 2 if bidirectional else rnn_hidden

        # Multi-Head Attention
        self.attention = nn.MultiheadAttention(
            embed_dim=rnn_output_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )

        self.attention_norm = nn.LayerNorm(rnn_output_dim)

        # Output Head (ê¹Šì€ êµ¬ì¡°)
        self.fc = nn.Sequential(
            nn.Linear(rnn_output_dim, hidden_dim),
            nn.ReLU(),
            nn.LayerNorm(hidden_dim),
            nn.Dropout(dropout),

            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.LayerNorm(hidden_dim // 2),
            nn.Dropout(dropout),

            nn.Linear(hidden_dim // 2, 2)
        )

    def forward(self, num_feat, cat_feat, padding_mask):
        """
        Args:
            num_feat: (batch, seq_len, num_features)
            cat_feat: (batch, seq_len, cat_features)
            padding_mask: (batch, seq_len) - True: padding, False: valid

        Returns:
            (batch, 2) - (target_x, target_y)
        """
        batch_size, seq_len, _ = num_feat.shape

        # Embedding
        embedded = []
        for i, feat_name in enumerate(self.embeddings.keys()):
            emb = self.embeddings[feat_name](cat_feat[:, :, i])
            embedded.append(emb)

        # Concatenate
        if embedded:
            embedded = torch.cat(embedded, dim=-1)
            x = torch.cat([num_feat, embedded], dim=-1)
        else:
            x = num_feat

        # Input Projection + LayerNorm
        x_proj = self.input_projection(x)
        x_proj = self.input_norm(x_proj)

        # RNN
        rnn_out, _ = self.rnn(x_proj)

        # Multi-Head Attention (ì¤‘ìš” ì‹œì  í•™ìŠµ)
        # padding_mask: True(íŒ¨ë”©)ëŠ” attentionì—ì„œ ë¬´ì‹œ
        attn_out, attn_weights = self.attention(
            rnn_out, rnn_out, rnn_out,
            key_padding_mask=padding_mask
        )

        # Residual Connection + LayerNorm
        attn_out = self.attention_norm(attn_out + rnn_out)

        # ë§ˆì§€ë§‰ ì‹œì ì˜ hidden state
        # Paddingì´ ì•„ë‹Œ ë§ˆì§€ë§‰ ìœ íš¨ ì‹œì  ì°¾ê¸°
        valid_lengths = (~padding_mask).sum(dim=1) - 1  # (batch,)
        valid_lengths = valid_lengths.clamp(min=0)

        # Gather last valid hidden state
        batch_indices = torch.arange(batch_size, device=attn_out.device)
        last_hidden = attn_out[batch_indices, valid_lengths]  # (batch, hidden_dim)

        # Output
        output = self.fc(last_hidden)

        return output


class EuclideanDistanceLoss(nn.Module):
    """ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜ ì†ì‹¤ í•¨ìˆ˜"""

    def forward(self, pred, target):
        # ì‹¤ì œ ì¢Œí‘œë¡œ ë³µì›
        pred_real = pred.clone()
        pred_real[:, 0] *= 105.0
        pred_real[:, 1] *= 68.0

        target_real = target.clone()
        target_real[:, 0] *= 105.0
        target_real[:, 1] *= 68.0

        # ìœ í´ë¦¬ë“œ ê±°ë¦¬
        distances = torch.sqrt(torch.sum((pred_real - target_real) ** 2, dim=1))
        return distances.mean()


def get_categorical_info(data, categorical_features, K=20):
    """ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ì–´íœ˜ í¬ê¸°ì™€ ì„ë² ë”© ì°¨ì› ê³„ì‚°"""
    vocab_sizes = {}
    embedding_dims = {}

    for feat_name in categorical_features:
        cols = [f"{feat_name}_{i}" for i in range(K)]
        cols = [c for c in cols if c in data.columns]

        if not cols:
            continue

        max_val = data[cols].max().max()
        vocab_size = int(max_val) + 2

        emb_dim = min(max(vocab_size // 2, 4), 50)

        vocab_sizes[feat_name] = vocab_size
        embedding_dims[feat_name] = emb_dim

    return vocab_sizes, embedding_dims


def train_one_epoch(model, dataloader, criterion, optimizer, device):
    """1 ì—í¬í¬ í•™ìŠµ"""
    model.train()
    total_loss = 0.0

    for num_feat, cat_feat, padding_mask, target in tqdm(dataloader, desc="Training", leave=False):
        num_feat = num_feat.to(device)
        cat_feat = cat_feat.to(device)
        padding_mask = padding_mask.to(device)
        target = target.to(device)

        optimizer.zero_grad()

        output = model(num_feat, cat_feat, padding_mask)
        loss = criterion(output, target)

        loss.backward()

        # Gradient Clipping (í•™ìŠµ ì•ˆì •ì„±)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(dataloader)


def validate(model, dataloader, criterion, device):
    """ê²€ì¦"""
    model.eval()
    total_loss = 0.0

    with torch.no_grad():
        for num_feat, cat_feat, padding_mask, target in tqdm(dataloader, desc="Validation", leave=False):
            num_feat = num_feat.to(device)
            cat_feat = cat_feat.to(device)
            padding_mask = padding_mask.to(device)
            target = target.to(device)

            output = model(num_feat, cat_feat, padding_mask)
            loss = criterion(output, target)

            total_loss += loss.item()

    return total_loss / len(dataloader)


def main():
    print("=" * 80)
    print("  PyTorch LSTM/GRU V5 - Multi-Head Attention + ê³ ë„í™”")
    print("  ëª©í‘œ: LightGBM (14.138m) ì´ˆê³¼ ì„±ëŠ¥ ë‹¬ì„±")
    print("=" * 80)
    print()

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° (ìµœì í™”ëœ ê°’)
    K = 20
    BATCH_SIZE = 64  # ì‘ì€ ë°°ì¹˜ë¡œ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ
    HIDDEN_DIM = 384  # ë” í° ëª¨ë¸
    NUM_LAYERS = 3  # ë” ê¹Šì€ RNN
    DROPOUT = 0.4  # ê°•í•œ Regularization
    LEARNING_RATE = 5e-4  # ë” ì‘ì€ LR
    NUM_EPOCHS = 100
    EARLY_STOPPING_PATIENCE = 20
    USE_LSTM = False  # GRUê°€ ë” ë¹ ë¦„
    BIDIRECTIONAL = True
    NUM_HEADS = 8

    print(f"ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    print(f"   - Sequence Length: {K}")
    print(f"   - Batch Size: {BATCH_SIZE}")
    print(f"   - Hidden Dim: {HIDDEN_DIM}")
    print(f"   - Num Layers: {NUM_LAYERS}")
    print(f"   - Dropout: {DROPOUT}")
    print(f"   - Learning Rate: {LEARNING_RATE}")
    print(f"   - Epochs: {NUM_EPOCHS}")
    print(f"   - RNN Type: {'Bidirectional ' if BIDIRECTIONAL else ''}{'LSTM' if USE_LSTM else 'GRU'}")
    print(f"   - Attention Heads: {NUM_HEADS}")
    print()

    # 1. ë°ì´í„° ë¡œë”©
    print("ğŸ“Š ë°ì´í„° ë¡œë”©...")
    data = pd.read_csv('processed_train_data_v4.csv')
    print(f"ë°ì´í„° Shape: {data.shape}")
    print()

    game_ids = data['game_id'].values

    # 2. ì²« ë²ˆì§¸ Fold (í”„ë¡œí† íƒ€ì´í•‘)
    print("ğŸ”§ First Fold í•™ìŠµ...")
    gkf = GroupKFold(n_splits=5)
    train_idx, val_idx = next(gkf.split(data, groups=game_ids))

    train_data = data.iloc[train_idx].copy()
    val_data = data.iloc[val_idx].copy()

    print(f"Train: {len(train_data):,} ìƒ˜í”Œ")
    print(f"Val: {len(val_data):,} ìƒ˜í”Œ")
    print()

    # 3. Dataset ìƒì„±
    print("ğŸ“¦ Dataset V5 ìƒì„± ì¤‘...")
    train_dataset = SoccerDatasetV5(train_data, K=K, is_train=True)
    val_dataset = SoccerDatasetV5(val_data, K=K, is_train=True)
    print()

    # DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=0,
        pin_memory=True if torch.cuda.is_available() else False
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=0,
        pin_memory=True if torch.cuda.is_available() else False
    )

    # 4. ë²”ì£¼í˜• ì •ë³´
    print("ğŸ”¤ ë²”ì£¼í˜• ë³€ìˆ˜ ì •ë³´ ì¶”ì¶œ ì¤‘...")
    vocab_sizes, embedding_dims = get_categorical_info(
        data, train_dataset.categorical_features, K=K
    )

    print("ë²”ì£¼í˜• ë³€ìˆ˜:")
    for feat_name in vocab_sizes.keys():
        print(f"   - {feat_name:20s}: Vocab={vocab_sizes[feat_name]:3d}, Emb_Dim={embedding_dims[feat_name]:2d}")
    print()

    # 5. ëª¨ë¸ ìƒì„±
    print("ğŸ—ï¸ ëª¨ë¸ V5 ìƒì„± ì¤‘...")
    model = SoccerRNNWithAttention(
        num_numerical_features=train_dataset.numerical_tensor.shape[2],
        categorical_vocab_sizes=vocab_sizes,
        embedding_dims=embedding_dims,
        hidden_dim=HIDDEN_DIM,
        num_layers=NUM_LAYERS,
        dropout=DROPOUT,
        use_lstm=USE_LSTM,
        bidirectional=BIDIRECTIONAL,
        num_heads=NUM_HEADS
    ).to(device)

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"âœ… ëª¨ë¸ V5 ìƒì„± ì™„ë£Œ:")
    print(f"   - Total Parameters: {total_params:,}")
    print(f"   - Trainable Parameters: {trainable_params:,}")
    print(f"   - Attention Heads: {NUM_HEADS}")
    print(f"   - Bidirectional: {BIDIRECTIONAL}")
    print()

    # 6. Loss & Optimizer
    criterion = EuclideanDistanceLoss()
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)

    # Cosine Annealing with Warm Restarts
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=10, T_mult=2, eta_min=1e-6
    )

    # 7. í•™ìŠµ ë£¨í”„
    print("ğŸš€ í•™ìŠµ ì‹œì‘ (V5 - Attention Model)...\n")
    best_val_loss = float('inf')
    patience_counter = 0

    for epoch in range(NUM_EPOCHS):
        print(f"Epoch {epoch+1}/{NUM_EPOCHS}")
        print("-" * 60)

        # Train
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)

        # Validate
        val_loss = validate(model, val_loader, criterion, device)

        # Learning Rate Scheduler
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']

        print(f"Train Loss: {train_loss:.4f}m | Val Loss: {val_loss:.4f}m | LR: {current_lr:.6f}")

        # Early Stopping & Model Saving
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0

            # ëª¨ë¸ ì €ì¥
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_loss': val_loss,
                'vocab_sizes': vocab_sizes,
                'embedding_dims': embedding_dims,
                'num_numerical_features': train_dataset.numerical_tensor.shape[2],
                'categorical_features': train_dataset.categorical_features,
                'numerical_features': train_dataset.numerical_features,
                'hyperparameters': {
                    'hidden_dim': HIDDEN_DIM,
                    'num_layers': NUM_LAYERS,
                    'dropout': DROPOUT,
                    'use_lstm': USE_LSTM,
                    'bidirectional': BIDIRECTIONAL,
                    'num_heads': NUM_HEADS,
                    'K': K
                }
            }, 'lstm_model_v5_attention_best.pth')

            print(f"ğŸ’¾ Best model saved! (Val Loss: {val_loss:.4f}m)")
        else:
            patience_counter += 1
            print(f"â³ Patience: {patience_counter}/{EARLY_STOPPING_PATIENCE}")

            if patience_counter >= EARLY_STOPPING_PATIENCE:
                print(f"\nâš ï¸ Early stopping triggered!")
                break

        print()

    # 8. ìµœì¢… ê²°ê³¼
    print("=" * 80)
    print("  í•™ìŠµ ì™„ë£Œ! (V5 - Attention Model)")
    print("=" * 80)
    print(f"\nâœ… Best Validation Loss: {best_val_loss:.4f}m")
    print(f"âœ… ëª¨ë¸ ì €ì¥: lstm_model_v5_attention_best.pth")

    print("\nğŸ“Š ì„±ëŠ¥ ë¹„êµ:")
    print(f"   - LightGBM V4 (5-Fold): 14.138m")
    print(f"   - LSTM/GRU V4 (Baseline): 15.649m")
    print(f"   - LSTM/GRU V5 (Attention): {best_val_loss:.4f}m")

    improvement = 15.649 - best_val_loss
    print(f"\nğŸ“ˆ ê°œì„ í­: {improvement:.4f}m ({improvement/15.649*100:.1f}%)")

    if best_val_loss < 14.138:
        print("\nğŸ‰ğŸ‰ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! LightGBMì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤!")
    elif best_val_loss < 15.0:
        print("\nâœ… ì¢‹ì€ ì„±ëŠ¥! ì¶”ê°€ íŠœë‹ìœ¼ë¡œ LightGBM ì´ˆê³¼ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        print("\nğŸ“ˆ ë‹¤ìŒ ë‹¨ê³„: 5-Fold CV, TTA, Data Augmentation")

    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()

