"""
LightGBM V4 - Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”

ëª©í‘œ: V4ì˜ ì ì¬ë ¥ ìµœëŒ€í•œ í™œìš©
ì˜ˆìƒ ê°œì„ : 0.2-0.5ì 
ì‹œê°„: 3-5ì‹œê°„ (100 trials)
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GroupKFold
import optuna
from optuna.samplers import TPESampler
import pickle
import warnings
warnings.filterwarnings('ignore')


def euclidean_distance(y_true, y_pred):
    """ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°"""
    distances = np.sqrt((y_true[:, 0] - y_pred[:, 0])**2 +
                       (y_true[:, 1] - y_pred[:, 1])**2)
    return distances.mean()


class LightGBMOptimizer:
    def __init__(self, X_train, y_train_x, y_train_y, game_ids):
        self.X_train = X_train
        self.y_train_x = y_train_x
        self.y_train_y = y_train_y
        self.game_ids = game_ids
        self.best_score = float('inf')

    def objective(self, trial):
        """Optuna objective function"""

        # í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ê³µê°„
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'verbosity': -1,
            'boosting_type': 'gbdt',

            # í•™ìŠµë¥ 
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),

            # íŠ¸ë¦¬ êµ¬ì¡°
            'num_leaves': trial.suggest_int('num_leaves', 31, 255),
            'max_depth': trial.suggest_int('max_depth', 5, 15),
            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 100),

            # ì •ê·œí™”
            'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),
            'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),

            # ìƒ˜í”Œë§
            'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),
            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),
            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),

            # ê¸°íƒ€
            'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0.0, 15.0),
        }

        # 5-Fold Cross Validation
        gkf = GroupKFold(n_splits=5)
        fold_scores = []

        for fold, (train_idx, val_idx) in enumerate(gkf.split(self.X_train, groups=self.game_ids)):
            X_tr, X_val = self.X_train.iloc[train_idx], self.X_train.iloc[val_idx]
            y_tr_x, y_val_x = self.y_train_x[train_idx], self.y_train_x[val_idx]
            y_tr_y, y_val_y = self.y_train_y[train_idx], self.y_train_y[val_idx]

            # X ì¢Œí‘œ ëª¨ë¸
            dtrain_x = lgb.Dataset(X_tr, label=y_tr_x)
            dvalid_x = lgb.Dataset(X_val, label=y_val_x, reference=dtrain_x)

            model_x = lgb.train(
                params,
                dtrain_x,
                num_boost_round=3000,
                valid_sets=[dvalid_x],
                callbacks=[
                    lgb.early_stopping(stopping_rounds=100, verbose=False),
                    lgb.log_evaluation(0)
                ]
            )

            # Y ì¢Œí‘œ ëª¨ë¸
            dtrain_y = lgb.Dataset(X_tr, label=y_tr_y)
            dvalid_y = lgb.Dataset(X_val, label=y_val_y, reference=dtrain_y)

            model_y = lgb.train(
                params,
                dtrain_y,
                num_boost_round=3000,
                valid_sets=[dvalid_y],
                callbacks=[
                    lgb.early_stopping(stopping_rounds=100, verbose=False),
                    lgb.log_evaluation(0)
                ]
            )

            # ê²€ì¦
            pred_x = model_x.predict(X_val, num_iteration=model_x.best_iteration)
            pred_y = model_y.predict(X_val, num_iteration=model_y.best_iteration)
            y_pred = np.column_stack([pred_x, pred_y])
            y_val = np.column_stack([y_val_x, y_val_y])

            eucl_dist = euclidean_distance(y_val, y_pred)
            fold_scores.append(eucl_dist)

        mean_score = np.mean(fold_scores)

        # Best score ì—…ë°ì´íŠ¸
        if mean_score < self.best_score:
            self.best_score = mean_score
            print(f"\nğŸ¯ New Best Score: {mean_score:.4f}m")
            print(f"   Params: {params}")

        return mean_score


def main():
    print("=" * 80)
    print("  LightGBM V4 - Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”")
    print("  ëª©í‘œ: 0.2-0.5ì  ì¶”ê°€ ê°œì„ ")
    print("=" * 80)
    print()

    # 1. ë°ì´í„° ë¡œë”©
    print("ğŸ“Š ë°ì´í„° ë¡œë”©...")
    data = pd.read_csv('processed_train_data_v4.csv')
    print(f"ë°ì´í„°: {data.shape}\n")

    # 2. í”¼ì²˜/íƒ€ê²Ÿ ë¶„ë¦¬
    print("ğŸ“Š í”¼ì²˜/íƒ€ê²Ÿ ë¶„ë¦¬...")

    y_train_x = data['target_x'].values
    y_train_y = data['target_y'].values
    game_ids = data['game_id'].values

    drop_cols = ['game_episode', 'game_id', 'target_x', 'target_y', 'final_team_id']
    X_train = data.drop(columns=[c for c in drop_cols if c in data.columns])
    X_train = X_train.fillna(0)

    for col in X_train.columns:
        if X_train[col].dtype == 'object':
            X_train[col] = pd.to_numeric(X_train[col], errors='coerce').fillna(0)

    print(f"í”¼ì²˜ ìˆ˜: {X_train.shape[1]}")
    print(f"ìƒ˜í”Œ ìˆ˜: {len(X_train):,}\n")

    # 3. Optuna ìµœì í™”
    n_trials = 20  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© (ì¢‹ì€ ê²°ê³¼ë©´ 100ìœ¼ë¡œ í™•ì¥)
    print("ğŸ”§ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘...")
    print(f"   - Trials: {n_trials} (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)")
    print("   - 5-Fold CV")
    print("   - ì˜ˆìƒ ì‹œê°„: 30-60ë¶„\n")

    optimizer = LightGBMOptimizer(X_train, y_train_x, y_train_y, game_ids)

    study = optuna.create_study(
        direction='minimize',
        sampler=TPESampler(seed=42),
        study_name='lightgbm_v4_optimization'
    )

    study.optimize(
        optimizer.objective,
        n_trials=n_trials,
        timeout=None,
        show_progress_bar=True
    )

    # 4. ìµœì  íŒŒë¼ë¯¸í„° ì¶œë ¥
    print("\n" + "=" * 80)
    print("  ìµœì í™” ì™„ë£Œ!")
    print("=" * 80)

    best_params = study.best_params
    best_score = study.best_value

    print(f"\nğŸ† Best Score: {best_score:.4f}m")
    print(f"\nğŸ“Š Best Parameters:")
    for key, value in best_params.items():
        print(f"   {key:20s}: {value}")

    # 5. ê°œì„ ë„ ë¶„ì„
    baseline_score = 14.36  # V4 baseline
    improvement = baseline_score - best_score
    improvement_pct = (improvement / baseline_score) * 100

    print(f"\nğŸ“ˆ ê°œì„ ë„ ë¶„ì„:")
    print(f"   Baseline (V4):     {baseline_score:.4f}m")
    print(f"   Optimized (V4.1):  {best_score:.4f}m")
    print(f"   Improvement:       {improvement:.4f}m ({improvement_pct:.2f}%)")

    if improvement > 0.2:
        print("\nğŸ‰ ìš°ìˆ˜í•œ ê°œì„ ! ì¦‰ì‹œ V4.1ë¡œ ì¬í•™ìŠµ ê¶Œì¥")
    elif improvement > 0.1:
        print("\nâœ… ì˜ë¯¸ ìˆëŠ” ê°œì„ ! V4.1 ì¬í•™ìŠµ ê³ ë ¤")
    else:
        print("\nğŸ“Š ì†Œí­ ê°œì„ . ë‹¤ë¥¸ ì „ëµ ë³‘í–‰ ê¶Œì¥")

    # 6. ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥
    best_params_full = {
        'objective': 'regression',
        'metric': 'rmse',
        'verbosity': -1,
        **best_params
    }

    with open('best_params_v4_optuna.pkl', 'wb') as f:
        pickle.dump({
            'params': best_params_full,
            'score': best_score,
            'improvement': improvement,
            'study': study
        }, f)

    print(f"\nğŸ’¾ ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥: best_params_v4_optuna.pkl")

    # 7. Optuna ì‹œê°í™” ì •ë³´
    print("\n" + "=" * 80)
    print("  Optuna ë¶„ì„")
    print("=" * 80)

    print(f"\nì´ ì‹œë„ íšŸìˆ˜: {len(study.trials)}")
    print(f"ì™„ë£Œëœ ì‹œë„: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}")
    print(f"ì‹¤íŒ¨í•œ ì‹œë„: {len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL])}")

    # ì¤‘ìš” íŒŒë¼ë¯¸í„° ë¶„ì„
    print("\nğŸ“Š íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ (ì¶”ì •):")
    try:
        importances = optuna.importance.get_param_importances(study)
        for i, (param, importance) in enumerate(sorted(importances.items(), key=lambda x: x[1], reverse=True)[:10], 1):
            print(f"   {i:2d}. {param:20s}: {importance:.4f}")
    except:
        print("   íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ ê³„ì‚° ì‹¤íŒ¨ (trials ë¶€ì¡±)")

    # 8. ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
    print("\n" + "=" * 80)
    print("  ë‹¤ìŒ ë‹¨ê³„")
    print("=" * 80)

    print("\n1. ìµœì  íŒŒë¼ë¯¸í„°ë¡œ V4.1 ì¬í•™ìŠµ:")
    print("   python train_lightgbm_v4_with_best_params.py")

    print("\n2. Test ì¶”ë¡  ë° ì œì¶œ:")
    print("   python inference_v4.py")

    print("\n3. ë‹¤ë¥¸ ìµœì í™” ì „ëµ:")
    print("   - K ê°’ ìµœì í™” (K=15,25,30)")
    print("   - Feature Selection")
    print("   - XGBoost/CatBoost ì‹¤í—˜")

    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()

