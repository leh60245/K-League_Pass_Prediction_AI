"""
ëª¨ë¸ í•™ìŠµ ìœ í‹¸ë¦¬í‹°
í”¼ì²˜ ì„¤ì • ìë™ ë¡œë”© ë° ë°ì´í„° ì¤€ë¹„ í—¬í¼ í•¨ìˆ˜ë“¤

ì‹¤ë¬´ íŒ¨í„´: ê³µí†µ ë¡œì§ì„ í•¨ìˆ˜ë¡œ ë¶„ë¦¬í•˜ì—¬ ì¬ì‚¬ìš©ì„± í–¥ìƒ
"""

import pandas as pd
import numpy as np
import os
from feature_config import FeatureConfig


def load_data_and_features(data_path='processed_train_data.csv',
                           config_path='feature_config.json',
                           verbose=True):
    """
    ë°ì´í„°ì™€ í”¼ì²˜ ì„¤ì •ì„ í•¨ê»˜ ë¡œë”©

    Returns:
        data (DataFrame): ì „ì²˜ë¦¬ëœ ë°ì´í„°
        feature_cols (list): í”¼ì²˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        target_cols (list): íƒ€ê²Ÿ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        config (FeatureConfig): í”¼ì²˜ ì„¤ì • ê°ì²´
    """
    if verbose:
        print("=" * 80)
        print("  ë°ì´í„° ë° í”¼ì²˜ ì„¤ì • ë¡œë”©")
        print("=" * 80)

    # 1. ë°ì´í„° ë¡œë”©
    if verbose:
        print(f"\nğŸ“Š ë°ì´í„° ë¡œë”©: {data_path}")

    if not os.path.exists(data_path):
        raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")

    data = pd.read_csv(data_path)
    if verbose:
        print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {data.shape}")

    # 2. í”¼ì²˜ ì„¤ì • ë¡œë”©
    if verbose:
        print(f"\nğŸ”§ í”¼ì²˜ ì„¤ì • ë¡œë”©: {config_path}")

    if not os.path.exists(config_path):
        if verbose:
            print(f"âš ï¸  ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìë™ ìƒì„± ì¤‘...")

        from feature_config import create_feature_config_from_data
        config = create_feature_config_from_data(
            data_path=data_path,
            preprocessor_path='preprocessor.pkl'
        )
    else:
        config = FeatureConfig(config_path)
        if verbose:
            print(f"âœ… í”¼ì²˜ ì„¤ì • ë¡œë”© ì™„ë£Œ")

    # 3. í”¼ì²˜/íƒ€ê²Ÿ ì¶”ì¶œ
    feature_cols = config.get_feature_columns()
    target_cols = config.get_target_columns()

    # ì‹¤ì œ ë°ì´í„°ì— ì¡´ì¬í•˜ëŠ” í”¼ì²˜ë§Œ ì‚¬ìš©
    available_features = [col for col in feature_cols if col in data.columns]
    missing_features = [col for col in feature_cols if col not in data.columns]

    if verbose:
        print(f"\nğŸ“Š í”¼ì²˜ ì •ë³´:")
        print(f"  - ì„¤ì • íŒŒì¼ í”¼ì²˜: {len(feature_cols)}ê°œ")
        print(f"  - ì‚¬ìš© ê°€ëŠ¥ í”¼ì²˜: {len(available_features)}ê°œ")
        if missing_features:
            print(f"  - ëˆ„ë½ëœ í”¼ì²˜: {len(missing_features)}ê°œ")
            if len(missing_features) <= 5:
                for f in missing_features:
                    print(f"    Â· {f}")
        print(f"  - íƒ€ê²Ÿ: {', '.join(target_cols)}")

    if verbose:
        print("\n" + "=" * 80)

    return data, available_features, target_cols, config


def prepare_train_val_split(data, feature_cols, target_cols,
                            val_ratio=0.2, random_seed=42, verbose=True):
    """
    Train/Validation ë¶„í•  (ê²Œì„ ê¸°ë°˜)

    Returns:
        X_train, y_train, X_val, y_val
    """
    if verbose:
        print("ğŸ“Š Train/Val Split (ê²Œì„ ê¸°ë°˜)...")

    # ê²Œì„ ID ê¸°ë°˜ ë¶„í• 
    games = data['game_id'].unique()
    np.random.seed(random_seed)
    np.random.shuffle(games)

    n_val_games = int(len(games) * val_ratio)
    val_games = games[:n_val_games]

    val_mask = data['game_id'].isin(val_games)
    train_mask = ~val_mask

    # í”¼ì²˜/íƒ€ê²Ÿ ì¶”ì¶œ
    X_train = data.loc[train_mask, feature_cols].fillna(0).values
    y_train = data.loc[train_mask, target_cols].values
    X_val = data.loc[val_mask, feature_cols].fillna(0).values
    y_val = data.loc[val_mask, target_cols].values

    if verbose:
        print(f"  - Train: {len(games) - n_val_games} ê²Œì„, {X_train.shape[0]:,} ì—í”¼ì†Œë“œ")
        print(f"  - Val: {n_val_games} ê²Œì„, {X_val.shape[0]:,} ì—í”¼ì†Œë“œ")
        print(f"  - í”¼ì²˜: {X_train.shape[1]}ê°œ\n")

    return X_train, y_train, X_val, y_val


def euclidean_distance(y_true, y_pred):
    """
    ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚° (í‰ê· )

    Args:
        y_true: (N, 2) - ì‹¤ì œ ì¢Œí‘œ [x, y]
        y_pred: (N, 2) - ì˜ˆì¸¡ ì¢Œí‘œ [x, y]

    Returns:
        float: í‰ê·  ìœ í´ë¦¬ë“œ ê±°ë¦¬ (m)
    """
    if y_true.ndim == 1:
        y_true = y_true.reshape(-1, 2)
    if y_pred.ndim == 1:
        y_pred = y_pred.reshape(-1, 2)

    distances = np.sqrt(
        (y_true[:, 0] - y_pred[:, 0])**2 +
        (y_true[:, 1] - y_pred[:, 1])**2
    )
    return np.mean(distances)


def print_performance_summary(train_eucl, val_eucl, baseline_eucl=20.37,
                             target_eucl=18.0, verbose=True):
    """
    ì„±ëŠ¥ ìš”ì•½ ì¶œë ¥

    Args:
        train_eucl: Train ìœ í´ë¦¬ë“œ ê±°ë¦¬
        val_eucl: Validation ìœ í´ë¦¬ë“œ ê±°ë¦¬
        baseline_eucl: ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ (ê¸°ë³¸ê°’: 20.37m)
        target_eucl: ëª©í‘œ ì„±ëŠ¥ (ê¸°ë³¸ê°’: 18m)
    """
    if not verbose:
        return

    improvement = baseline_eucl - val_eucl
    improvement_pct = (improvement / baseline_eucl) * 100

    print("\n" + "=" * 80)
    print("  ì„±ëŠ¥ ìš”ì•½")
    print("=" * 80)

    print(f"\nğŸ“Š ìœ í´ë¦¬ë“œ ê±°ë¦¬:")
    print(f"  - Train: {train_eucl:.2f}m")
    print(f"  - Val: {val_eucl:.2f}m")

    print(f"\nğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„:")
    print(f"  - ë² ì´ìŠ¤ë¼ì¸: {baseline_eucl:.2f}m")
    print(f"  - ê°œì„ : {improvement:.2f}m ({improvement_pct:+.1f}%)")

    if val_eucl < baseline_eucl:
        print(f"  âœ… ë² ì´ìŠ¤ë¼ì¸ë³´ë‹¤ {improvement:.2f}m ê°œì„ !")
    else:
        print(f"  âš ï¸  ë² ì´ìŠ¤ë¼ì¸ë³´ë‹¤ {-improvement:.2f}m ë‚˜ì¨")

    print(f"\nğŸ“Š ëª©í‘œ ë‹¬ì„±:")
    print(f"  - ëª©í‘œ: < {target_eucl:.2f}m")
    print(f"  - í˜„ì¬: {val_eucl:.2f}m")

    if val_eucl < target_eucl:
        print(f"  ğŸ¯ ëª©í‘œ ë‹¬ì„±! ({val_eucl:.2f}m < {target_eucl:.2f}m)")
    else:
        gap = val_eucl - target_eucl
        print(f"  â° ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œê¹Œì§€ {gap:.2f}m ë‚¨ìŒ)")

    print("=" * 80)


def save_submission(predictions, output_path='submission.csv', verbose=True):
    """
    ì œì¶œ íŒŒì¼ ìƒì„±

    Args:
        predictions: (N, 2) - ì˜ˆì¸¡ ì¢Œí‘œ [x, y]
        output_path: ì €ì¥ ê²½ë¡œ
    """
    submission = pd.DataFrame({
        'end_x': predictions[:, 0],
        'end_y': predictions[:, 1]
    })

    submission.to_csv(output_path, index=False)

    if verbose:
        print(f"âœ… ì œì¶œ íŒŒì¼ ì €ì¥: {output_path}")
        print(f"  - í–‰ ê°œìˆ˜: {len(submission):,}")
        print(f"  - ì»¬ëŸ¼: {list(submission.columns)}")


def get_feature_group_importance(model_x, model_y, feature_cols, config, top_n=5):
    """
    í”¼ì²˜ ê·¸ë£¹ë³„ ì¤‘ìš”ë„ ê³„ì‚°

    Args:
        model_x: X ì¢Œí‘œ ì˜ˆì¸¡ ëª¨ë¸
        model_y: Y ì¢Œí‘œ ì˜ˆì¸¡ ëª¨ë¸
        feature_cols: í”¼ì²˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        config: FeatureConfig ê°ì²´
        top_n: ê° ê·¸ë£¹ì—ì„œ í‘œì‹œí•  ìƒìœ„ Nê°œ

    Returns:
        dict: ê·¸ë£¹ë³„ ì¤‘ìš”ë„
    """
    # í”¼ì²˜ë³„ ì¤‘ìš”ë„
    importance_x = model_x.feature_importances_
    importance_y = model_y.feature_importances_
    importance_avg = (importance_x + importance_y) / 2

    # í”¼ì²˜ -> ì¤‘ìš”ë„ ë§¤í•‘
    feature_importance = dict(zip(feature_cols, importance_avg))

    # ê·¸ë£¹ë³„ ì§‘ê³„
    group_importance = {}
    feature_groups = config.config.get('feature_groups', {})

    for group_name, group_features in feature_groups.items():
        # ì´ ê·¸ë£¹ì— ì†í•œ í”¼ì²˜ë“¤ì˜ ì¤‘ìš”ë„ í•©
        group_total = sum(feature_importance.get(f, 0) for f in group_features
                         if f in feature_cols)

        # ê°œë³„ í”¼ì²˜ ì¤‘ìš”ë„
        feature_details = []
        for f in group_features:
            if f in feature_cols:
                importance = feature_importance[f]
                feature_details.append((f, importance))

        # ì¤‘ìš”ë„ ìˆœ ì •ë ¬
        feature_details.sort(key=lambda x: x[1], reverse=True)

        group_importance[group_name] = {
            'total': group_total,
            'features': feature_details[:top_n]
        }

    return group_importance


def print_feature_group_importance(group_importance, verbose=True):
    """í”¼ì²˜ ê·¸ë£¹ë³„ ì¤‘ìš”ë„ ì¶œë ¥"""
    if not verbose:
        return

    print("\n" + "=" * 80)
    print("  í”¼ì²˜ ê·¸ë£¹ë³„ ì¤‘ìš”ë„")
    print("=" * 80)

    # ì´ ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_groups = sorted(group_importance.items(),
                          key=lambda x: x[1]['total'],
                          reverse=True)

    for group_name, info in sorted_groups:
        print(f"\nğŸ“Š {group_name} (ì´ {info['total']:.4f})")
        for feature, importance in info['features']:
            print(f"  Â· {feature:30s}: {importance:.4f}")

    print("\n" + "=" * 80)


# ì‹¤ë¬´ íŒ: ì´ë ‡ê²Œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ë‘ë©´
# 1. ì½”ë“œ ì¬ì‚¬ìš©ì„± í–¥ìƒ
# 2. ìœ ì§€ë³´ìˆ˜ ìš©ì´
# 3. ì¼ê´€ì„± ìœ ì§€
# 4. í…ŒìŠ¤íŠ¸ ìš©ì´

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("ğŸ§ª ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ í…ŒìŠ¤íŠ¸\n")

    # ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸
    data, features, targets, config = load_data_and_features()

    print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"  - ë°ì´í„°: {data.shape}")
    print(f"  - í”¼ì²˜: {len(features)}ê°œ")
    print(f"  - íƒ€ê²Ÿ: {len(targets)}ê°œ")

