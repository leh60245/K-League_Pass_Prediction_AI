"""
LightGBM V5 - Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”

ê°œì„ ì‚¬í•­:
- fillna(0) ì™„ì „ ì œê±° (NaN ìœ ì§€ë¡œ LightGBM ìµœì í™”)
- ìµœì  ëª¨ë¸ ë°œê²¬ ì‹œ ì¦‰ì‹œ ì €ì¥ (Ctrl+C ëŒ€ë¹„)
- Optuna DB ê¸°ë°˜ ì¤‘ë‹¨ í›„ ì¬ê°œ ê¸°ëŠ¥
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GroupKFold
import optuna
from optuna.samplers import TPESampler
import pickle
import warnings
import os
from datetime import datetime
warnings.filterwarnings('ignore')


def euclidean_distance(y_true, y_pred):
    """ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°"""
    distances = np.sqrt((y_true[:, 0] - y_pred[:, 0])**2 +
                       (y_true[:, 1] - y_pred[:, 1])**2)
    return distances.mean()


class LightGBMOptimizer:
    def __init__(self, X_train, y_train_x, y_train_y, game_ids, cat_features):
        self.X_train = X_train
        self.y_train_x = y_train_x
        self.y_train_y = y_train_y
        self.game_ids = game_ids
        self.cat_features = cat_features  # ë²”ì£¼í˜• ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ ë°›ê¸°
        self.best_score = float('inf')
        self.best_models_x = []  # [ì¶”ê°€] ìµœì  ëª¨ë¸ ì €ì¥
        self.best_models_y = []
        self.best_params = None
        self.best_fold_scores = []

    def objective(self, trial):
        """Optuna objective function"""

        # í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ê³µê°„
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'verbosity': -1,
            'boosting_type': 'gbdt',
            'n_jobs': -1,  # CPU í’€ê°€ë™

            # í•™ìŠµë¥  (ì •ë°€ íƒìƒ‰)
            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),

            # íŠ¸ë¦¬ êµ¬ì¡° (ê³¼ì í•© ë°©ì§€ í¬í•¨)
            'num_leaves': trial.suggest_int('num_leaves', 31, 127),  # 255ëŠ” ë„ˆë¬´ í¼ -> 127ë¡œ ì¶•ì†Œ
            'max_depth': trial.suggest_int('max_depth', 7, 15),
            'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),  # ì¤‘ìš” íŒŒë¼ë¯¸í„° ì¶”ê°€

            # ì •ê·œí™”
            'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),
            'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),

            # ìƒ˜í”Œë§
            'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 0.95),
            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 0.95),
            'bagging_freq': trial.suggest_int('bagging_freq', 1, 5),

            # ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬ ë°©ì‹ (ê¸°ë³¸ê°’ í™œìš© ê¶Œì¥)
            # 'cat_smooth': trial.suggest_float('cat_smooth', 1.0, 50.0)
        }

        # 5-Fold Cross Validation
        gkf = GroupKFold(n_splits=5)
        fold_scores = []
        models_x = []  # [ì¶”ê°€] ê° foldì˜ ëª¨ë¸ ì €ì¥
        models_y = []

        for fold, (train_idx, val_idx) in enumerate(gkf.split(self.X_train, groups=self.game_ids)):
            X_tr, X_val = self.X_train.iloc[train_idx], self.X_train.iloc[val_idx]
            y_tr_x, y_val_x = self.y_train_x[train_idx], self.y_train_x[val_idx]
            y_tr_y, y_val_y = self.y_train_y[train_idx], self.y_train_y[val_idx]

            # ğŸš¨ [ì¤‘ìš”] categorical_feature ëª…ì‹œ
            dtrain_x = lgb.Dataset(X_tr, label=y_tr_x, categorical_feature=self.cat_features)
            dvalid_x = lgb.Dataset(X_val, label=y_val_x, reference=dtrain_x, categorical_feature=self.cat_features)

            model_x = lgb.train(
                params,
                dtrain_x,
                num_boost_round=3000,
                valid_sets=[dvalid_x],
                callbacks=[
                    lgb.early_stopping(stopping_rounds=50, verbose=False),  # ë¹ ë¥¸ íƒìƒ‰ ìœ„í•´ 50ìœ¼ë¡œ ë‹¨ì¶•
                    lgb.log_evaluation(0)
                ]
            )
            models_x.append(model_x)  # [ì¶”ê°€] ëª¨ë¸ ì €ì¥

            dtrain_y = lgb.Dataset(X_tr, label=y_tr_y, categorical_feature=self.cat_features)
            dvalid_y = lgb.Dataset(X_val, label=y_val_y, reference=dtrain_y, categorical_feature=self.cat_features)

            model_y = lgb.train(
                params,
                dtrain_y,
                num_boost_round=3000,
                valid_sets=[dvalid_y],
                callbacks=[
                    lgb.early_stopping(stopping_rounds=50, verbose=False),
                    lgb.log_evaluation(0)
                ]
            )
            models_y.append(model_y)  # [ì¶”ê°€] ëª¨ë¸ ì €ì¥

            # ê²€ì¦
            pred_x = model_x.predict(X_val, num_iteration=model_x.best_iteration)
            pred_y = model_y.predict(X_val, num_iteration=model_y.best_iteration)

            # ì¢Œí‘œ í´ë¦¬í•‘ (ê²½ê¸°ì¥ ë°– ì˜ˆì¸¡ ë°©ì§€)
            pred_x = np.clip(pred_x, 0, 105)
            pred_y = np.clip(pred_y, 0, 68)

            y_pred = np.column_stack([pred_x, pred_y])
            y_val = np.column_stack([y_val_x, y_val_y])

            eucl_dist = euclidean_distance(y_val, y_pred)
            fold_scores.append(eucl_dist)

        mean_score = np.mean(fold_scores)

        # [ì¶”ê°€] ìµœì  ëª¨ë¸ ë°œê²¬ ì‹œ ì¦‰ì‹œ ì €ì¥
        if mean_score < self.best_score:
            self.best_score = mean_score
            self.best_models_x = models_x
            self.best_models_y = models_y
            self.best_params = params
            self.best_fold_scores = fold_scores

            print(f"\nğŸ¯ New Best Score: {mean_score:.4f}m")

            # [ì¶”ê°€] ì¦‰ì‹œ íŒŒì¼ë¡œ ì €ì¥ (Ctrl+C ëŒ€ë¹„)
            try:
                with open('best_model_v5_optuna_checkpoint.pkl', 'wb') as f:
                    pickle.dump({
                        'models_x': models_x,
                        'models_y': models_y,
                        'params': params,
                        'score': mean_score,
                        'fold_scores': fold_scores
                    }, f)
                print(f"   ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: best_model_v5_optuna_checkpoint.pkl")
            except Exception as e:
                print(f"   âš ï¸  ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

        return mean_score


def main():
    print("=" * 80)
    print("  LightGBM V5 - Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”")
    print("  ëª©í‘œ: 0.2-0.5ì  ì¶”ê°€ ê°œì„ ")
    print("=" * 80)
    print()

    # 1. ë°ì´í„° ë¡œë”©
    print("ğŸ“Š ë°ì´í„° ë¡œë”©...")
    # ğŸš¨ [ì£¼ì˜] ë°˜ë“œì‹œ V5.1 (NaNì´ í¬í•¨ëœ ë°ì´í„°)ë¥¼ ë¡œë“œí•´ì•¼ í•¨
    data = pd.read_csv('processed_train_data_v5.csv')
    print(f"ë°ì´í„°: {data.shape}\n")

    # 2. í”¼ì²˜/íƒ€ê²Ÿ ë¶„ë¦¬
    print("ğŸ“Š í”¼ì²˜/íƒ€ê²Ÿ ë¶„ë¦¬ ë° ì „ì²˜ë¦¬ ìˆ˜ì •...")

    y_train_x = data['target_x'].values
    y_train_y = data['target_y'].values
    game_ids = data['game_id'].values

    drop_cols = ['game_episode', 'game_id', 'target_x', 'target_y', 'final_team_id']
    X_train = data.drop(columns=[c for c in drop_cols if c in data.columns])

    # ğŸš¨ [CRITICAL FIX] fillna(0) ì‚­ì œ!! ğŸš¨
    # X_train = X_train.fillna(0)  <-- ì ˆëŒ€ ê¸ˆì§€ (LightGBMì´ NaNì„ ìŠ¤ìŠ¤ë¡œ ì²˜ë¦¬í•˜ê²Œ ë‘ )

    # ğŸš¨ [CRITICAL FIX] object -> numeric ë³€í™˜ ì‹œì—ë„ fillna(0) ì œê±°
    # ëŒ€ì‹  ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ì°¾ì•„ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¦
    cat_keywords = ['type_id', 'res_id', 'team_id_enc', 'is_home', 'period_id', 'is_last']
    cat_features = [c for c in X_train.columns if any(k in c for k in cat_keywords)]

    print(f"ğŸ“Œ ë²”ì£¼í˜• ë³€ìˆ˜ {len(cat_features)}ê°œ ê°ì§€ë¨ -> category íƒ€ì… ë³€í™˜")

    # 2. [í•µì‹¬ ìˆ˜ì •] ë²”ì£¼í˜• ë³€ìˆ˜ë“¤ì„ 'category' íƒ€ì…ìœ¼ë¡œ ê°•ì œ ë³€í™˜
    # LightGBMì€ object íƒ€ì…ì„ ì‹«ì–´í•˜ì§€ë§Œ, category íƒ€ì…ì€ ì•„ì£¼ ì¢‹ì•„í•©ë‹ˆë‹¤.
    for col in cat_features:
        X_train[col] = X_train[col].astype('category')

    # 3. ë‚˜ë¨¸ì§€ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì¤‘ objectë¡œ ì¡íŒ ê²ƒë“¤ ìˆ«ì ë³€í™˜
    for col in X_train.columns:
        if col not in cat_features and X_train[col].dtype == 'object':
            X_train[col] = pd.to_numeric(X_train[col], errors='coerce')

    print(f"í”¼ì²˜ ìˆ˜: {X_train.shape[1]}")
    print(f"ìƒ˜í”Œ ìˆ˜: {len(X_train):,}\n")

    # 3. Optuna ìµœì í™”
    n_trials = 50  # ì´ˆê¸° í…ŒìŠ¤íŠ¸ë¡œ 50íšŒ ì¶”ì²œ

    # [ì¶”ê°€] Optuna DB íŒŒì¼ ê²½ë¡œ (ì¤‘ë‹¨ í›„ ì¬ê°œ ê°€ëŠ¥)
    study_name = 'lightgbm_v5_optimization'
    storage_name = f'sqlite:///optuna_v5_study.db'

    print("ğŸ”§ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘...")

    # [ì¶”ê°€] ê¸°ì¡´ studyê°€ ìˆëŠ”ì§€ í™•ì¸
    if os.path.exists('optuna_v5_study.db'):
        print(f"ğŸ“‚ ê¸°ì¡´ study ë°œê²¬! ì¤‘ë‹¨ëœ ì§€ì ë¶€í„° ì¬ê°œí•©ë‹ˆë‹¤.")
        try:
            study = optuna.load_study(
                study_name=study_name,
                storage=storage_name,
                sampler=TPESampler(seed=42)
            )
            print(f"   ì´ë¯¸ ì™„ë£Œëœ trial: {len(study.trials)}ê°œ")
            print(f"   í˜„ì¬ ìµœê³  ì ìˆ˜: {study.best_value:.4f}m")
        except:
            print(f"   study ë¡œë”© ì‹¤íŒ¨, ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
            study = optuna.create_study(
                direction='minimize',
                sampler=TPESampler(seed=42),
                study_name=study_name,
                storage=storage_name,
                load_if_exists=True
            )
    else:
        print(f"ğŸ“ ìƒˆë¡œìš´ study ìƒì„±")
        study = optuna.create_study(
            direction='minimize',
            sampler=TPESampler(seed=42),
            study_name=study_name,
            storage=storage_name,
            load_if_exists=True
        )

    print(f"ğŸ’¾ ì§„í–‰ìƒí™© DB ì €ì¥: optuna_v5_study.db")
    print(f"   (Ctrl+Cë¡œ ì¤‘ë‹¨í•´ë„ ì¬ì‹¤í–‰ ì‹œ ì´ì–´ì„œ ì§„í–‰ë©ë‹ˆë‹¤)\n")

    # Optimizer ìƒì„± ì‹œ cat_features ì „ë‹¬
    optimizer = LightGBMOptimizer(X_train, y_train_x, y_train_y, game_ids, cat_features)

    # [ì¶”ê°€] KeyboardInterrupt ì²˜ë¦¬
    try:
        study.optimize(
            optimizer.objective,
            n_trials=n_trials,
            timeout=None,  # ì‹œê°„ ì œí•œ ì—†ìŒ
            show_progress_bar=True
        )
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ì ì¤‘ë‹¨ (Ctrl+C)")
        print(f"ğŸ’¾ í˜„ì¬ê¹Œì§€ ì§„í–‰: {len(study.trials)}ê°œ trial ì™„ë£Œ")
        print(f"ğŸ† í˜„ì¬ ìµœê³  ì ìˆ˜: {study.best_value:.4f}m")
        print(f"\nì¬ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ì´ì–´ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")
        print(f"ì™„ë£Œëœ ê²°ê³¼ëŠ” 'best_model_v5_optuna_checkpoint.pkl'ì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n")

    # 4. ìµœì  íŒŒë¼ë¯¸í„° ì¶œë ¥
    print("\n" + "=" * 80)
    print("  ìµœì í™” ì™„ë£Œ!")
    print("=" * 80)

    best_params = study.best_params
    best_score = study.best_value

    print(f"\nğŸ† Best Score: {best_score:.4f}m")
    print(f"\nğŸ“Š Best Parameters:")
    for key, value in best_params.items():
        print(f"   {key:20s}: {value}")

    # 5. ê°œì„ ë„ ë¶„ì„
    baseline_score = 14.01  # V4.1 baseline
    improvement = baseline_score - best_score
    improvement_pct = (improvement / baseline_score) * 100

    print(f"\nğŸ“ˆ ê°œì„ ë„ ë¶„ì„:")
    print(f"   Baseline (V4.1):     {baseline_score:.4f}m")
    print(f"   Optimized (V5):  {best_score:.4f}m")
    print(f"   Improvement:       {improvement:.4f}m ({improvement_pct:.2f}%)")

    if improvement > 0.2:
        print("\nğŸ‰ ìš°ìˆ˜í•œ ê°œì„ ! ì¦‰ì‹œ V5ë¡œ ì¬í•™ìŠµ ê¶Œì¥")
    elif improvement > 0.1:
        print("\nâœ… ì˜ë¯¸ ìˆëŠ” ê°œì„ ! V5 ì¬í•™ìŠµ ê³ ë ¤")
    else:
        print("\nğŸ“Š ì†Œí­ ê°œì„ . ë‹¤ë¥¸ ì „ëµ ë³‘í–‰ ê¶Œì¥")

    # 6. ìµœì  íŒŒë¼ë¯¸í„° ë° ëª¨ë¸ ì €ì¥
    best_params_full = {
        'objective': 'regression',
        'metric': 'rmse',
        'verbosity': -1,
        **best_params
    }

    # [ì¶”ê°€] ìµœì  ëª¨ë¸ ì „ì²´ ì €ì¥ (íŒŒë¼ë¯¸í„° + ëª¨ë¸ ê°ì²´)
    final_save_path = f'best_model_v5_optuna_final.pkl'
    with open(final_save_path, 'wb') as f:
        pickle.dump({
            'models_x': optimizer.best_models_x,
            'models_y': optimizer.best_models_y,
            'params': best_params_full,
            'score': best_score,
            'fold_scores': optimizer.best_fold_scores,
            'improvement': improvement,
            'study': study
        }, f)

    print(f"\nğŸ’¾ ìµœì  ëª¨ë¸ ìµœì¢… ì €ì¥: {final_save_path}")

    # íŒŒë¼ë¯¸í„°ë§Œ ë”°ë¡œ ì €ì¥
    with open('best_params_v5_optuna.pkl', 'wb') as f:
        pickle.dump({
            'params': best_params_full,
            'score': best_score,
            'improvement': improvement,
            'study': study
        }, f)

    print(f"ğŸ’¾ ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥: best_params_v5_optuna.pkl")

    # 7. Optuna ì‹œê°í™” ì •ë³´
    print("\n" + "=" * 80)
    print("  Optuna ë¶„ì„")
    print("=" * 80)

    print(f"\nì´ ì‹œë„ íšŸìˆ˜: {len(study.trials)}")
    print(f"ì™„ë£Œëœ ì‹œë„: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}")
    print(f"ì‹¤íŒ¨í•œ ì‹œë„: {len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL])}")

    # ì¤‘ìš” íŒŒë¼ë¯¸í„° ë¶„ì„
    print("\nğŸ“Š íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ (ì¶”ì •):")
    try:
        importances = optuna.importance.get_param_importances(study)
        for i, (param, importance) in enumerate(sorted(importances.items(), key=lambda x: x[1], reverse=True)[:10], 1):
            print(f"   {i:2d}. {param:20s}: {importance:.4f}")
    except:
        print("   íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ ê³„ì‚° ì‹¤íŒ¨ (trials ë¶€ì¡±)")

    # 8. ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
    print("\n" + "=" * 80)
    print("  ë‹¤ìŒ ë‹¨ê³„")
    print("=" * 80)

    print("\n1. ìµœì  íŒŒë¼ë¯¸í„°ë¡œ V4.1 ì¬í•™ìŠµ:")
    print("   python train_lightgbm_v5_with_best_params.py")

    print("\n2. Test ì¶”ë¡  ë° ì œì¶œ:")
    print("   python inference_v5.py")

    print("\n3. ë‹¤ë¥¸ ìµœì í™” ì „ëµ:")
    print("   - K ê°’ ìµœì í™” (K=15,25,30)")
    print("   - Feature Selection")
    print("   - XGBoost/CatBoost ì‹¤í—˜")

    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()

