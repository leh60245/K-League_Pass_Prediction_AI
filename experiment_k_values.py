"""
K ê°’ ìµœì í™” ì‹¤í—˜

ëª©í‘œ: ë§ˆì§€ë§‰ Kê°œ ì´ë²¤íŠ¸ì˜ ìµœì  ê°œìˆ˜ ì°¾ê¸°
í›„ë³´: K = [15, 20, 25, 30]
ì˜ˆìƒ ê°œì„ : 0.1-0.3ì 
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GroupKFold
import pickle
import warnings
warnings.filterwarnings('ignore')

# ê¸°ì¡´ preprocessing_v4 import
import sys
sys.path.append('.')
from preprocessing_v4 import DataPreprocessorV4


def euclidean_distance(y_true, y_pred):
    """ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°"""
    distances = np.sqrt((y_true[:, 0] - y_pred[:, 0])**2 +
                       (y_true[:, 1] - y_pred[:, 1])**2)
    return distances.mean()


def quick_train_eval(X_train, y_train_x, y_train_y, game_ids, k_value):
    """ë¹ ë¥¸ í•™ìŠµ ë° í‰ê°€ (1-Foldë§Œ)"""

    print(f"\n{'='*60}")
    print(f"  K = {k_value} í…ŒìŠ¤íŠ¸")
    print(f"{'='*60}")

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° (V4 ê¸°ë³¸ê°’)
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'learning_rate': 0.05,
        'num_leaves': 127,
        'min_data_in_leaf': 80,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 1,
        'verbose': -1,
    }

    # 1-Foldë§Œ ì‚¬ìš© (ë¹ ë¥¸ í‰ê°€)
    gkf = GroupKFold(n_splits=5)
    train_idx, val_idx = next(gkf.split(X_train, groups=game_ids))

    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_tr_x, y_val_x = y_train_x[train_idx], y_train_x[val_idx]
    y_tr_y, y_val_y = y_train_y[train_idx], y_train_y[val_idx]

    print(f"Train: {len(X_tr):,}, Val: {len(X_val):,}")

    # X ì¢Œí‘œ ëª¨ë¸
    print("end_x ëª¨ë¸ í•™ìŠµ ì¤‘...")
    dtrain_x = lgb.Dataset(X_tr, label=y_tr_x)
    dvalid_x = lgb.Dataset(X_val, label=y_val_x, reference=dtrain_x)

    model_x = lgb.train(
        params,
        dtrain_x,
        num_boost_round=3000,
        valid_sets=[dvalid_x],
        callbacks=[
            lgb.early_stopping(stopping_rounds=100, verbose=False),
            lgb.log_evaluation(0)
        ]
    )
    print(f"  -> {model_x.best_iteration} rounds")

    # Y ì¢Œí‘œ ëª¨ë¸
    print("end_y ëª¨ë¸ í•™ìŠµ ì¤‘...")
    dtrain_y = lgb.Dataset(X_tr, label=y_tr_y)
    dvalid_y = lgb.Dataset(X_val, label=y_val_y, reference=dtrain_y)

    model_y = lgb.train(
        params,
        dtrain_y,
        num_boost_round=3000,
        valid_sets=[dvalid_y],
        callbacks=[
            lgb.early_stopping(stopping_rounds=100, verbose=False),
            lgb.log_evaluation(0)
        ]
    )
    print(f"  -> {model_y.best_iteration} rounds")

    # í‰ê°€
    pred_x = model_x.predict(X_val, num_iteration=model_x.best_iteration)
    pred_y = model_y.predict(X_val, num_iteration=model_y.best_iteration)
    y_pred = np.column_stack([pred_x, pred_y])
    y_val = np.column_stack([y_val_x, y_val_y])

    eucl_dist = euclidean_distance(y_val, y_pred)
    mse_x = mean_squared_error(y_val_x, pred_x)
    mse_y = mean_squared_error(y_val_y, pred_y)

    print(f"\nê²°ê³¼:")
    print(f"  - ìœ í´ë¦¬ë“œ ê±°ë¦¬: {eucl_dist:.4f}m")
    print(f"  - MSE X: {mse_x:.4f}")
    print(f"  - MSE Y: {mse_y:.4f}")

    return {
        'k': k_value,
        'euclidean': eucl_dist,
        'mse_x': mse_x,
        'mse_y': mse_y,
        'best_iter_x': model_x.best_iteration,
        'best_iter_y': model_y.best_iteration
    }


def full_train_eval(k_value, n_folds=5):
    """ì „ì²´ 5-Fold í•™ìŠµ ë° í‰ê°€ (ìµœì¢… ê²€ì¦ìš©)"""

    print(f"\n{'='*80}")
    print(f"  K = {k_value} ì „ì²´ 5-Fold í•™ìŠµ")
    print(f"{'='*80}\n")

    # ì „ì²˜ë¦¬ ì‹¤í–‰
    preprocessor = DataPreprocessorV4(data_dir='./data', K=k_value)
    X_train, X_test = preprocessor.preprocess_pipeline(verbose=True)

    # ë°ì´í„° ì €ì¥
    X_train.to_csv(f'processed_train_data_v4_k{k_value}.csv', index=False)
    X_test.to_csv(f'processed_test_data_v4_k{k_value}.csv', index=False)

    # í•™ìŠµ
    print(f"\n5-Fold í•™ìŠµ ì‹œì‘...")

    y_train_x = X_train['target_x'].values
    y_train_y = X_train['target_y'].values
    game_ids = X_train['game_id'].values

    drop_cols = ['game_episode', 'game_id', 'target_x', 'target_y', 'final_team_id']
    X_train_feat = X_train.drop(columns=[c for c in drop_cols if c in X_train.columns])
    X_train_feat = X_train_feat.fillna(0)

    for col in X_train_feat.columns:
        if X_train_feat[col].dtype == 'object':
            X_train_feat[col] = pd.to_numeric(X_train_feat[col], errors='coerce').fillna(0)

    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'learning_rate': 0.05,
        'num_leaves': 127,
        'min_data_in_leaf': 80,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 1,
        'verbose': -1,
    }

    gkf = GroupKFold(n_splits=n_folds)
    fold_scores = []

    for fold, (train_idx, val_idx) in enumerate(gkf.split(X_train_feat, groups=game_ids)):
        print(f"\nFold {fold+1}/{n_folds}")

        X_tr, X_val = X_train_feat.iloc[train_idx], X_train_feat.iloc[val_idx]
        y_tr_x, y_val_x = y_train_x[train_idx], y_train_x[val_idx]
        y_tr_y, y_val_y = y_train_y[train_idx], y_train_y[val_idx]

        # X ëª¨ë¸
        dtrain_x = lgb.Dataset(X_tr, label=y_tr_x)
        dvalid_x = lgb.Dataset(X_val, label=y_val_x, reference=dtrain_x)
        model_x = lgb.train(params, dtrain_x, num_boost_round=3000,
                           valid_sets=[dvalid_x],
                           callbacks=[lgb.early_stopping(100, verbose=False)])

        # Y ëª¨ë¸
        dtrain_y = lgb.Dataset(X_tr, label=y_tr_y)
        dvalid_y = lgb.Dataset(X_val, label=y_val_y, reference=dtrain_y)
        model_y = lgb.train(params, dtrain_y, num_boost_round=3000,
                           valid_sets=[dvalid_y],
                           callbacks=[lgb.early_stopping(100, verbose=False)])

        # í‰ê°€
        pred_x = model_x.predict(X_val, num_iteration=model_x.best_iteration)
        pred_y = model_y.predict(X_val, num_iteration=model_y.best_iteration)
        y_pred = np.column_stack([pred_x, pred_y])
        y_val = np.column_stack([y_val_x, y_val_y])

        eucl_dist = euclidean_distance(y_val, y_pred)
        fold_scores.append(eucl_dist)
        print(f"  Score: {eucl_dist:.4f}m")

    mean_score = np.mean(fold_scores)
    std_score = np.std(fold_scores)

    print(f"\ní‰ê· : {mean_score:.4f}m Â± {std_score:.4f}m")

    return mean_score, std_score


def main():
    print("=" * 80)
    print("  K ê°’ ìµœì í™” ì‹¤í—˜")
    print("  ëª©í‘œ: ìµœì  ì‹œí€€ìŠ¤ ê¸¸ì´ ì°¾ê¸°")
    print("=" * 80)
    print()

    # Phase 1: Quick Test (1-Fold)
    print("ğŸ“Š Phase 1: Quick Test (1-Fold)")
    print("   ê° K ê°’ìœ¼ë¡œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸í•˜ì—¬ ê²½í–¥ íŒŒì•…\n")

    k_candidates = [15, 20, 25, 30]
    quick_results = []

    # K=20 ë°ì´í„° ë¡œë“œ (ì´ë¯¸ ìƒì„±ë¨)
    print("K=20 ë°ì´í„° ë¡œë”© (ê¸°ì¡´)...")
    data_k20 = pd.read_csv('processed_train_data_v4.csv')

    y_train_x = data_k20['target_x'].values
    y_train_y = data_k20['target_y'].values
    game_ids = data_k20['game_id'].values

    drop_cols = ['game_episode', 'game_id', 'target_x', 'target_y', 'final_team_id']
    X_train = data_k20.drop(columns=[c for c in drop_cols if c in data_k20.columns])
    X_train = X_train.fillna(0)

    for col in X_train.columns:
        if X_train[col].dtype == 'object':
            X_train[col] = pd.to_numeric(X_train[col], errors='coerce').fillna(0)

    # K=20ìœ¼ë¡œ Quick Test
    result_k20 = quick_train_eval(X_train, y_train_x, y_train_y, game_ids, 20)
    quick_results.append(result_k20)

    # ë‹¤ë¥¸ K ê°’ë“¤ì€ ì „ì²˜ë¦¬ë¶€í„° í•„ìš”
    print("\nâš ï¸  ë‹¤ë¥¸ K ê°’ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ì„œëŠ” ì „ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    print("   ê° K ê°’ë§ˆë‹¤ 2-3ë¶„ ì†Œìš” ì˜ˆìƒ\n")

    for k in [15, 25, 30]:
        response = input(f"K={k} í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
        if response.lower() != 'y':
            print(f"K={k} ê±´ë„ˆëœ€")
            continue

        print(f"\nK={k} ì „ì²˜ë¦¬ ì‹œì‘...")
        preprocessor = DataPreprocessorV4(data_dir='./data', K=k)
        X_train_k, _ = preprocessor.preprocess_pipeline(verbose=False)

        # í”¼ì²˜ ì¤€ë¹„
        y_train_x = X_train_k['target_x'].values
        y_train_y = X_train_k['target_y'].values
        game_ids = X_train_k['game_id'].values

        X_train_feat = X_train_k.drop(columns=[c for c in drop_cols if c in X_train_k.columns])
        X_train_feat = X_train_feat.fillna(0)

        for col in X_train_feat.columns:
            if X_train_feat[col].dtype == 'object':
                X_train_feat[col] = pd.to_numeric(X_train_feat[col], errors='coerce').fillna(0)

        # Quick Test
        result = quick_train_eval(X_train_feat, y_train_x, y_train_y, game_ids, k)
        quick_results.append(result)

    # Phase 1 ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 80)
    print("  Phase 1 ê²°ê³¼ ìš”ì•½ (Quick Test)")
    print("=" * 80)

    results_df = pd.DataFrame(quick_results)
    results_df = results_df.sort_values('euclidean')

    print("\nìˆœìœ„ (ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ì¤€):")
    for i, row in results_df.iterrows():
        rank = results_df.index.get_loc(i) + 1
        print(f"  {rank}. K={row['k']:2d}: {row['euclidean']:.4f}m")

    best_k = results_df.iloc[0]['k']
    best_score = results_df.iloc[0]['euclidean']

    print(f"\nğŸ† Best K: {int(best_k)} (Score: {best_score:.4f}m)")

    # K=20 ëŒ€ë¹„ ê°œì„ ë„
    k20_score = results_df[results_df['k'] == 20]['euclidean'].values[0]
    improvement = k20_score - best_score

    print(f"\nK=20 ëŒ€ë¹„:")
    print(f"  - K=20: {k20_score:.4f}m")
    print(f"  - K={int(best_k)}: {best_score:.4f}m")
    print(f"  - ê°œì„ : {improvement:.4f}m")

    # Phase 2: Full Test (ì„ íƒ)
    if improvement > 0.05:  # ì˜ë¯¸ ìˆëŠ” ê°œì„ ì´ ìˆëŠ” ê²½ìš°
        print("\n" + "=" * 80)
        print("  Phase 2: Full Test ê¶Œì¥")
        print("=" * 80)
        print(f"\nK={int(best_k)}ê°€ K=20ë³´ë‹¤ {improvement:.4f}m ìš°ìˆ˜í•©ë‹ˆë‹¤.")
        print("5-Fold ì „ì²´ í•™ìŠµìœ¼ë¡œ ê²€ì¦í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
        print("(ì˜ˆìƒ ì‹œê°„: 20-30ë¶„)\n")

        response = input("ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
        if response.lower() == 'y':
            mean_score, std_score = full_train_eval(int(best_k), n_folds=5)

            print("\n" + "=" * 80)
            print("  ìµœì¢… ê²°ê³¼")
            print("=" * 80)
            print(f"\nK={int(best_k)} 5-Fold ì„±ëŠ¥: {mean_score:.4f}m Â± {std_score:.4f}m")

            # V4 baselineê³¼ ë¹„êµ
            v4_baseline = 14.36
            final_improvement = v4_baseline - mean_score

            print(f"\nV4 Baseline ëŒ€ë¹„:")
            print(f"  - V4 (K=20): {v4_baseline:.4f}m")
            print(f"  - V4.2 (K={int(best_k)}): {mean_score:.4f}m")
            print(f"  - ê°œì„ : {final_improvement:.4f}m")

            if final_improvement > 0.1:
                print("\nğŸ‰ ìš°ìˆ˜í•œ ê°œì„ ! V4.2ë¡œ ì—…ë°ì´íŠ¸ ê¶Œì¥")
            elif final_improvement > 0:
                print("\nâœ… ì†Œí­ ê°œì„ ! ìƒí™©ì— ë”°ë¼ ì„ íƒ")
            else:
                print("\nğŸ“Š K=20 ìœ ì§€ ê¶Œì¥")
    else:
        print("\nğŸ“Š ê²°ë¡ : K=20ì´ ìµœì ì´ê±°ë‚˜ í° ì°¨ì´ ì—†ìŒ")
        print("   ë‹¤ë¥¸ ìµœì í™” ì „ëµ (í•˜ì´í¼íŒŒë¼ë¯¸í„°, í”¼ì²˜ ë“±) ì¶”ì²œ")

    # ê²°ê³¼ ì €ì¥
    results_df.to_csv('k_optimization_results.csv', index=False)
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: k_optimization_results.csv")

    # ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
    print("\n" + "=" * 80)
    print("  ë‹¤ìŒ ë‹¨ê³„")
    print("=" * 80)

    print("\n1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”:")
    print("   python train_lightgbm_v4_optuna.py")

    print("\n2. Feature Selection:")
    print("   Feature Importance ê¸°ë°˜ ìƒìœ„ í”¼ì²˜ ì„ íƒ")

    print("\n3. ë‹¤ë¥¸ ëª¨ë¸ ì‹¤í—˜:")
    print("   XGBoost, CatBoost êµ¬í˜„")

    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()

