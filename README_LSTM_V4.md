# PyTorch LSTM/GRU ê¸°ë°˜ Kë¦¬ê·¸ íŒ¨ìŠ¤ ì˜ˆì¸¡ - V4

V4 Wide Format ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ë”¥ëŸ¬ë‹(LSTM/GRU) ê¸°ë°˜ íŒ¨ìŠ¤ ë„ì°© ìœ„ì¹˜ ì˜ˆì¸¡ ëª¨ë¸

## ğŸ“Œ ì£¼ìš” íŠ¹ì§•

### 1. **ë°ì´í„° ì •ê·œí™” (Normalization)**
- **X ì¢Œí‘œ ê´€ë ¨ ì»¬ëŸ¼**: `start_x`, `end_x`, `dx` â†’ `/105` (í•„ë“œ Xì¶• ê¸¸ì´)
- **Y ì¢Œí‘œ ê´€ë ¨ ì»¬ëŸ¼**: `start_y`, `end_y`, `dy` â†’ `/68` (í•„ë“œ Yì¶• ê¸¸ì´)
- **íƒ€ê²Ÿ ì •ê·œí™”**: `target_x`, `target_y`ë„ ë™ì¼í•˜ê²Œ ì •ê·œí™” í›„ í•™ìŠµ
- **íš¨ê³¼**: ë”¥ëŸ¬ë‹ í•™ìŠµ ì•ˆì •ì„± ëŒ€í­ í–¥ìƒ, ìˆ˜ë ´ ì†ë„ ê°œì„ 

### 2. **Input Projection Layer**
- ìˆ˜ì¹˜í˜• í”¼ì²˜ + Embeddingì„ concatenateí•œ í›„, `nn.Linear(input_dim, hidden_dim)`ì„ í†µí•´ ì°¨ì› ë³€í™˜
- LSTM/GRU ì…ë ¥ ì „ì— ì°¨ì›ì„ í†µì¼í•˜ì—¬ í‘œí˜„ë ¥ í–¥ìƒ
- **Architecture**: `[Numerical + Embedding] â†’ Projection â†’ LSTM/GRU â†’ Output Head`

### 3. **NaN ì²˜ë¦¬**
- V4 ë°ì´í„°ì˜ ì•ë¶€ë¶„ íŒ¨ë”©(NaN)ì„ `torch.nan_to_num(x, 0.0)`ìœ¼ë¡œ 0 ë³€í™˜
- ëª¨ë¸ì´ íŒ¨ë”©ì„ ìì—°ìŠ¤ëŸ½ê²Œ í•™ìŠµ

### 4. **Categorical Embedding**
- `type_id`, `res_id`, `team_id_enc`, `is_home`, `is_last`, `period_id` â†’ Embedding ë ˆì´ì–´ ì‚¬ìš©
- ì–´íœ˜ í¬ê¸° ìë™ ê³„ì‚°, Embedding ì°¨ì› íœ´ë¦¬ìŠ¤í‹± ì ìš©

### 5. **Euclidean Distance Loss**
- í‰ê°€ì§€í‘œ(ìœ í´ë¦¬ë“œ ê±°ë¦¬)ì™€ ë™ì¼í•œ ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš©
- MSE ëŒ€ì‹  ì§ì ‘ì ì¸ ê±°ë¦¬ ìµœì í™”

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1. PyTorch ì„¤ì¹˜
```bash
# CPU ë²„ì „ (Windows)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# GPU ë²„ì „ (CUDA 11.8)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 2. í•™ìŠµ ì‹¤í–‰
```bash
python train_lstm_v4.py
```

**ì¶œë ¥:**
- ëª¨ë¸ ì €ì¥: `lstm_model_v4_best.pth`
- Best Validation Loss (ìœ í´ë¦¬ë“œ ê±°ë¦¬)
- í•™ìŠµ ë¡œê·¸ (Epochë³„ Train/Val Loss)

### 3. ì¶”ë¡  ì‹¤í–‰
```bash
python inference_lstm_v4.py
```

**ì¶œë ¥:**
- ì œì¶œ íŒŒì¼: `submission_lstm_v4_YYYYMMDD_HHMMSS.csv`
- ì˜ˆì¸¡ í†µê³„ (end_x, end_y ë¶„í¬)

---

## ğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„°

| Parameter | Value | ì„¤ëª… |
|-----------|-------|------|
| `K` | 20 | ì‹œí€€ìŠ¤ ê¸¸ì´ (ë§ˆì§€ë§‰ 20ê°œ ì´ë²¤íŠ¸) |
| `BATCH_SIZE` | 128 | ë°°ì¹˜ í¬ê¸° |
| `HIDDEN_DIM` | 256 | LSTM/GRU hidden dimension |
| `NUM_LAYERS` | 2 | LSTM/GRU ë ˆì´ì–´ ìˆ˜ |
| `DROPOUT` | 0.3 | Dropout ë¹„ìœ¨ |
| `LEARNING_RATE` | 1e-3 | ì´ˆê¸° í•™ìŠµë¥  |
| `NUM_EPOCHS` | 50 | ìµœëŒ€ ì—í¬í¬ |
| `EARLY_STOPPING_PATIENCE` | 10 | Early stopping patience |
| `USE_LSTM` | False | False: GRU, True: LSTM |

---

## ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜

```
Input: (Batch, SeqLen=20, Features)
  â†“
[Numerical Features]  [Categorical Features]
  â†“                         â†“
  â†“                    Embeddings (type_id, res_id, team_id_enc, ...)
  â†“                         â†“
  â””â”€â”€â”€â”€â”€â”€â”€ Concatenate â”€â”€â”€â”€â”€â”˜
              â†“
     Input Projection (Linear)
              â†“
       GRU/LSTM (2 layers)
              â†“
    Last Hidden State
              â†“
      Output Head (FC)
              â†“
     (target_x, target_y)
```

---

## ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ

| ëª¨ë¸ | Validation Loss | ì˜ˆìƒ Test ì ìˆ˜ | íŠ¹ì§• |
|------|----------------|---------------|------|
| LightGBM V3 | ~1.5m | 14ì ëŒ€ | ì‹œí€€ìŠ¤ ëª¨ë¸ë§, ì•ˆì •ì  |
| LightGBM V4 | ~1.5m | 13~15ì ëŒ€ | V2 í”¼ì²˜ + V3 ì‹œí€€ìŠ¤ |
| **LSTM/GRU V4** | **?** | **13~16ì ëŒ€** | **ë”¥ëŸ¬ë‹ ì‹œí€€ìŠ¤ í•™ìŠµ** |

---

## ğŸ”§ íŠœë‹ í¬ì¸íŠ¸

### 1. Hidden Dimension
- í˜„ì¬: 256
- ì‹œë„: 128, 512 (ëª¨ë¸ ë³µì¡ë„ ì¡°ì ˆ)

### 2. Learning Rate
- í˜„ì¬: 1e-3
- ì‹œë„: 5e-4, 1e-4 (ë” ì•ˆì •ì ì¸ í•™ìŠµ)

### 3. Dropout
- í˜„ì¬: 0.3
- ì‹œë„: 0.2, 0.5 (ê³¼ì í•© ì¡°ì ˆ)

### 4. RNN Type
- í˜„ì¬: GRU
- ì‹œë„: LSTM (ë” ê¸´ ë©”ëª¨ë¦¬ í•„ìš” ì‹œ)

### 5. Num Layers
- í˜„ì¬: 2
- ì‹œë„: 3, 4 (ë” ê¹Šì€ ëª¨ë¸)

---

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
train_lstm_v4.py          # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
inference_lstm_v4.py      # ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
lstm_model_v4_best.pth    # í•™ìŠµëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
processed_train_data_v4.csv  # V4 ì „ì²˜ë¦¬ í•™ìŠµ ë°ì´í„°
processed_test_data_v4.csv   # V4 ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°
submission_lstm_v4_*.csv     # ì œì¶œ íŒŒì¼
```

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

### 1. 5-Fold ì•™ìƒë¸” í•™ìŠµ
- í˜„ì¬: Fold 1ë§Œ ì‚¬ìš© (í”„ë¡œí† íƒ€ì´í•‘)
- ê°œì„ : ì „ì²´ 5-Fold í•™ìŠµ â†’ ì•™ìƒë¸” ì˜ˆì¸¡
- ì˜ˆìƒ ê°œì„ : 0.2~0.5m

### 2. LightGBM + LSTM ì•™ìƒë¸”
```python
# Weighted Average
final_pred = 0.5 * lgbm_pred + 0.5 * lstm_pred
```

### 3. Attention Mechanism ì¶”ê°€
- Self-Attentionìœ¼ë¡œ ì¤‘ìš” ì´ë²¤íŠ¸ ìë™ ê°€ì¤‘ì¹˜ ë¶€ì—¬
- Transformer ê¸°ë°˜ ëª¨ë¸ ì‹œë„

### 4. Data Augmentation
- ì‹œí€€ìŠ¤ ê¸¸ì´ ë³€í™” (K=15, 25)
- ë…¸ì´ì¦ˆ ì¶”ê°€

---

## ğŸ’¡ í•µì‹¬ ê°œì„ ì‚¬í•­

### LightGBM ëŒ€ë¹„ LSTM/GRUì˜ ì¥ì 
1. **ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ**: RNNì˜ hidden stateê°€ ì‹œê°„ì— ë”°ë¥¸ íŒ¨í„´ì„ ë” ì˜ í¬ì°©
2. **ë¹„ì„ í˜• ë³€í™˜**: ë”¥ëŸ¬ë‹ì˜ ê°•ë ¥í•œ í‘œí˜„ë ¥
3. **End-to-End í•™ìŠµ**: Embeddingê¹Œì§€ í•¨ê»˜ ìµœì í™”

### ì£¼ì˜ì‚¬í•­
1. **í•™ìŠµ ì‹œê°„**: LightGBMë³´ë‹¤ ëŠë¦¼ (GPU ê¶Œì¥)
2. **ë°ì´í„° ë¶€ì¡±**: 15,435 ìƒ˜í”Œë¡œëŠ” ê³¼ì í•© ê°€ëŠ¥ â†’ Regularization ì¤‘ìš”
3. **í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°ë„**: íŠœë‹ í•„ìš”

---

## ğŸ“ ë¬¸ì˜

í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ:
1. PyTorch ì„¤ì¹˜ í™•ì¸
2. CUDA ë²„ì „ í™•ì¸ (GPU ì‚¬ìš© ì‹œ)
3. ë©”ëª¨ë¦¬ ë¶€ì¡± â†’ `BATCH_SIZE` ì¤„ì´ê¸°
4. `processed_train_data_v4.csv` ê²½ë¡œ í™•ì¸

---

**ì‘ì„±ì¼**: 2025-12-18  
**ë²„ì „**: V4 (Wide Format + Deep Learning)  
**ëª©í‘œ**: LightGBM ëŒ€ë¹„ 0.3~0.8m ê°œì„  (Test 13~15ì ëŒ€)

