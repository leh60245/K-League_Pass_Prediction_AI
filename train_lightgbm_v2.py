"""
LightGBM ëª¨ë¸ í•™ìŠµ - V2 ë°ì´í„° ì‚¬ìš©

ëª©í‘œ: ê°œì„ ëœ ì „ì²˜ë¦¬ ë°ì´í„°ë¡œ ì„±ëŠ¥ í–¥ìƒ í™•ì¸
"""

import pandas as pd
import numpy as np
import pickle
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings('ignore')

def euclidean_distance(y_true, y_pred):
    """ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°"""
    distances = np.sqrt((y_true[:, 0] - y_pred[:, 0])**2 +
                       (y_true[:, 1] - y_pred[:, 1])**2)
    return distances.mean()

def main():
    print("=" * 80)
    print("  LightGBM V2 - ê°œì„ ëœ í”¼ì²˜ë¡œ í•™ìŠµ")
    print("=" * 80)
    print()

    # 1. ë°ì´í„° ë¡œë”©
    print("ğŸ“Š V2 ë°ì´í„° ë¡œë”©...")
    data = pd.read_csv('processed_train_data_v2.csv')
    print(f"âœ… ë°ì´í„°: {data.shape}")

    # 2. Preprocessor ë¡œë”©
    print("\nğŸ“¦ Preprocessor V2 ë¡œë”©...")
    with open('preprocessor_v2.pkl', 'rb') as f:
        preprocessor_data = pickle.load(f)
    print("âœ… Preprocessor ë¡œë”© ì™„ë£Œ")

    # 3. í”¼ì²˜/íƒ€ê²Ÿ ë¶„ë¦¬
    from preprocessing_v2 import DataPreprocessorV2
    preprocessor = DataPreprocessorV2()
    preprocessor.type_encoder = preprocessor_data['type_encoder']
    preprocessor.result_encoder = preprocessor_data['result_encoder']

    feature_cols = preprocessor.get_feature_columns()

    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í”¼ì²˜ë§Œ ì‚¬ìš©
    available_features = [col for col in feature_cols if col in data.columns]
    print(f"\nğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜: {len(available_features)}ê°œ")

    X = data[available_features]
    y = data[['end_x', 'end_y']].values

    # 4. Train/Val Split (Game-based)
    print("\nğŸ“Š Train/Val Split...")
    from sklearn.model_selection import GroupKFold

    gkf = GroupKFold(n_splits=5)
    fold_scores = []

    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, groups=data['game_id'])):
        print(f"\n{'='*60}")
        print(f"  Fold {fold+1}/5")
        print(f"{'='*60}")

        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        print(f"Train: {X_train.shape}, Val: {X_val.shape}")

        # 5. ëª¨ë¸ í•™ìŠµ (X ì¢Œí‘œ)
        print("\nğŸ”§ X ì¢Œí‘œ ëª¨ë¸ í•™ìŠµ...")
        model_x = LGBMRegressor(
            n_estimators=500,
            learning_rate=0.05,
            max_depth=8,
            num_leaves=63,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            verbose=-1
        )

        model_x.fit(
            X_train, y_train[:, 0],
            eval_set=[(X_val, y_val[:, 0])],
            eval_metric='rmse',
            callbacks=[
                # early_stopping(50, verbose=False)
            ]
        )

        # 6. ëª¨ë¸ í•™ìŠµ (Y ì¢Œí‘œ)
        print("ğŸ”§ Y ì¢Œí‘œ ëª¨ë¸ í•™ìŠµ...")
        model_y = LGBMRegressor(
            n_estimators=500,
            learning_rate=0.05,
            max_depth=8,
            num_leaves=63,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            verbose=-1
        )

        model_y.fit(
            X_train, y_train[:, 1],
            eval_set=[(X_val, y_val[:, 1])],
            eval_metric='rmse',
            callbacks=[
                # early_stopping(50, verbose=False)
            ]
        )

        # 7. ì˜ˆì¸¡ ë° í‰ê°€
        print("\nğŸ“Š í‰ê°€ ì¤‘...")
        y_pred_x = model_x.predict(X_val)
        y_pred_y = model_y.predict(X_val)
        y_pred = np.column_stack([y_pred_x, y_pred_y])

        # ìœ í´ë¦¬ë“œ ê±°ë¦¬
        eucl_dist = euclidean_distance(y_val, y_pred)

        # MSE
        mse_x = mean_squared_error(y_val[:, 0], y_pred[:, 0])
        mse_y = mean_squared_error(y_val[:, 1], y_pred[:, 1])

        print(f"\nâœ… Fold {fold+1} ê²°ê³¼:")
        print(f"  - ìœ í´ë¦¬ë“œ ê±°ë¦¬: {eucl_dist:.4f}m")
        print(f"  - MSE X: {mse_x:.4f}")
        print(f"  - MSE Y: {mse_y:.4f}")

        fold_scores.append({
            'fold': fold + 1,
            'euclidean': eucl_dist,
            'mse_x': mse_x,
            'mse_y': mse_y
        })

        # Feature Importance (ì²« ë²ˆì§¸ foldë§Œ)
        if fold == 0:
            print("\nğŸ“Š Feature Importance Top 20 (X ì¢Œí‘œ):")
            importance_x = model_x.feature_importances_
            importance_df = pd.DataFrame({
                'feature': available_features,
                'importance': importance_x
            }).sort_values('importance', ascending=False)

            for i, row in importance_df.head(20).iterrows():
                print(f"  {row.name+1:2d}. {row['feature']:35s}: {row['importance']:8.1f}")

    # 8. ì „ì²´ ê²°ê³¼ ìš”ì•½
    print("\n" + "="*80)
    print("  ì „ì²´ ê²°ê³¼ ìš”ì•½")
    print("="*80)

    scores_df = pd.DataFrame(fold_scores)
    print(f"\ní‰ê·  ìœ í´ë¦¬ë“œ ê±°ë¦¬: {scores_df['euclidean'].mean():.4f}m Â± {scores_df['euclidean'].std():.4f}m")
    print(f"í‰ê·  MSE X: {scores_df['mse_x'].mean():.4f} Â± {scores_df['mse_x'].std():.4f}")
    print(f"í‰ê·  MSE Y: {scores_df['mse_y'].mean():.4f} Â± {scores_df['mse_y'].std():.4f}")

    print("\nFoldë³„ ìƒì„¸:")
    for _, row in scores_df.iterrows():
        print(f"  Fold {int(row['fold'])}: {row['euclidean']:.4f}m")

    # 9. ìµœì¢… ëª¨ë¸ í•™ìŠµ (ì „ì²´ ë°ì´í„°)
    print("\n" + "="*80)
    print("  ìµœì¢… ëª¨ë¸ í•™ìŠµ (ì „ì²´ ë°ì´í„°)")
    print("="*80)

    model_x_final = LGBMRegressor(
        n_estimators=500,
        learning_rate=0.05,
        max_depth=8,
        num_leaves=63,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        verbose=-1
    )

    model_y_final = LGBMRegressor(
        n_estimators=500,
        learning_rate=0.05,
        max_depth=8,
        num_leaves=63,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        verbose=-1
    )

    print("ğŸ”§ X ì¢Œí‘œ ìµœì¢… ëª¨ë¸ í•™ìŠµ...")
    model_x_final.fit(X, y[:, 0])

    print("ğŸ”§ Y ì¢Œí‘œ ìµœì¢… ëª¨ë¸ í•™ìŠµ...")
    model_y_final.fit(X, y[:, 1])

    # 10. ëª¨ë¸ ì €ì¥
    print("\nğŸ’¾ ëª¨ë¸ ì €ì¥...")
    with open('lightgbm_model_v2.pkl', 'wb') as f:
        pickle.dump({
            'model_x': model_x_final,
            'model_y': model_y_final,
            'feature_cols': available_features
        }, f)
    print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: lightgbm_model_v2.pkl")

    # 11. ì„±ëŠ¥ ë¹„êµ
    print("\n" + "="*80)
    print("  ì„±ëŠ¥ ë¹„êµ (V1 vs V2)")
    print("="*80)
    print("\nê¸°ì¡´ LightGBM (V1):")
    print("  - Validation í‰ê· : 0.93m")
    print(f"\nê°œì„ ëœ LightGBM (V2):")
    print(f"  - Validation í‰ê· : {scores_df['euclidean'].mean():.4f}m")

    improvement = (0.93 - scores_df['euclidean'].mean()) / 0.93 * 100
    if improvement > 0:
        print(f"\nâœ… ì„±ëŠ¥ ê°œì„ : {improvement:.2f}% í–¥ìƒ!")
    else:
        print(f"\nâš ï¸  ì„±ëŠ¥ ë³€í™”: {improvement:.2f}%")

    print("\n" + "="*80)
    print("âœ… V2 ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    print("="*80)

if __name__ == "__main__":
    main()

