"""
K-League Pass Prediction - V7 with Data Augmentation

V5/V6ì˜ ë¬¸ì œì  ê·¼ë³¸ í•´ê²°:
âœ… Data Augmentation (í•™ìŠµ ë°ì´í„° ë¶€ì¡± ë¬¸ì œ í•´ê²°)
âœ… Mixup (ìƒ˜í”Œ ê°„ ë³´ê°„ìœ¼ë¡œ ì¼ë°˜í™” í–¥ìƒ)
âœ… Sequence Augmentation (ì‹œê°„ì¶• ì¡°ì‘)
âœ… ìµœì í™”ëœ Transformer
âœ… Multi-Task Learning (ê±°ë¦¬ + ê°ë„ ì˜ˆì¸¡)

ì‘ì„±ì¼: 2025-12-19
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import GroupKFold
import warnings
import re
from tqdm import tqdm
import random

warnings.filterwarnings('ignore')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”§ Using device: {device}")


class SoccerDatasetV7(Dataset):
    """V7 Dataset with Data Augmentation"""

    def __init__(self, data, K=20, is_train=True, augment=True):
        self.data = data.reset_index(drop=True)
        self.K = K
        self.is_train = is_train
        self.augment = augment and is_train  # Train ì‹œì—ë§Œ Augmentation

        if 'target_x' in data.columns and 'target_y' in data.columns:
            self.targets = data[['target_x', 'target_y']].values.astype(np.float32)
            self.targets[:, 0] /= 105.0
            self.targets[:, 1] /= 68.0
        else:
            self.targets = None

        exclude_cols = ['game_episode', 'game_id', 'target_x', 'target_y', 'final_team_id']
        feature_data = data.drop(columns=[c for c in exclude_cols if c in data.columns])

        self.numerical_features, self.categorical_features = self._classify_columns(feature_data.columns)
        self.numerical_tensor = self._prepare_numerical_features(feature_data)
        self.categorical_tensor = self._prepare_categorical_features(feature_data)
        self.padding_mask = self._create_padding_mask()

    def _classify_columns(self, columns):
        pattern = re.compile(r'^(.+)_(\d+)$')
        feature_names = set()

        for col in columns:
            match = pattern.match(col)
            if match:
                feature_names.add(match.group(1))

        categorical_keywords = ['type_id', 'res_id', 'team_id_enc', 'is_home', 'is_last', 'period_id']
        categorical_features = []
        numerical_features = []

        for feat in sorted(feature_names):
            if any(keyword in feat for keyword in categorical_keywords):
                categorical_features.append(feat)
            else:
                numerical_features.append(feat)

        return numerical_features, categorical_features

    def _prepare_numerical_features(self, data):
        tensors = []
        x_coord_keywords = ['start_x', 'end_x', 'dx']
        y_coord_keywords = ['start_y', 'end_y', 'dy']

        for feat_name in self.numerical_features:
            cols = [f"{feat_name}_{i}" for i in range(self.K)]
            cols = [c for c in cols if c in data.columns]
            if not cols:
                continue

            feat_data = data[cols].values.astype(np.float32)
            if feat_data.shape[1] < self.K:
                padding = np.zeros((feat_data.shape[0], self.K - feat_data.shape[1]), dtype=np.float32)
                feat_data = np.concatenate([feat_data, padding], axis=1)

            if any(kw in feat_name for kw in x_coord_keywords):
                feat_data = feat_data / 105.0
            elif any(kw in feat_name for kw in y_coord_keywords):
                feat_data = feat_data / 68.0
            elif 'speed' in feat_name.lower():
                feat_data = np.clip(feat_data / 30.0, 0, 1)
            elif 'angle' in feat_name.lower() or 'direction' in feat_name.lower():
                feat_data = feat_data / np.pi
            elif 'time' in feat_name.lower():
                max_time = np.nanmax(feat_data) if not np.all(np.isnan(feat_data)) else 1.0
                if max_time > 0:
                    feat_data = feat_data / max_time

            feat_data = np.nan_to_num(feat_data, nan=0.0)
            tensors.append(feat_data)

        result = np.stack(tensors, axis=-1) if tensors else np.zeros((len(data), self.K, 0), dtype=np.float32)
        return torch.from_numpy(result)

    def _prepare_categorical_features(self, data):
        tensors = []
        for feat_name in self.categorical_features:
            cols = [f"{feat_name}_{i}" for i in range(self.K)]
            cols = [c for c in cols if c in data.columns]
            if not cols:
                continue

            feat_data = data[cols].values.astype(np.float32)
            if feat_data.shape[1] < self.K:
                padding = np.zeros((feat_data.shape[0], self.K - feat_data.shape[1]), dtype=np.float32)
                feat_data = np.concatenate([feat_data, padding], axis=1)

            feat_data = np.nan_to_num(feat_data, nan=0.0)
            tensors.append(feat_data)

        result = np.stack(tensors, axis=-1) if tensors else np.zeros((len(data), self.K, 0), dtype=np.float32)
        return torch.from_numpy(result).long()

    def _create_padding_mask(self):
        mask = (self.numerical_tensor.sum(dim=-1) == 0)
        return mask

    def _apply_augmentation(self, num_feat, target):
        """Data Augmentation ì ìš©"""

        # 1. Gaussian Noise (20% í™•ë¥ )
        if random.random() < 0.2:
            noise = torch.randn_like(num_feat) * 0.02  # ì‘ì€ ë…¸ì´ì¦ˆ
            num_feat = num_feat + noise

        # 2. ì¢Œìš° ë°˜ì „ (50% í™•ë¥ )
        if random.random() < 0.5:
            # Y ì¢Œí‘œ ê´€ë ¨ í”¼ì²˜ë§Œ ë°˜ì „
            # (ì‹¤ì œë¡œëŠ” í”¼ì²˜ë³„ë¡œ êµ¬ë¶„í•´ì•¼ í•˜ì§€ë§Œ ê°„ë‹¨íˆ ì²˜ë¦¬)
            # ì—¬ê¸°ì„œëŠ” targetë§Œ ë°˜ì „ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
            target_y_flipped = 1.0 - target[1]  # ì •ê·œí™”ëœ y ì¢Œí‘œ ë°˜ì „
            target = torch.tensor([target[0], target_y_flipped], dtype=torch.float32)

        # 3. ì‹œí€€ìŠ¤ ìë¥´ê¸° (10% í™•ë¥ ) - ë§ˆì§€ë§‰ ëª‡ ê°œ ì‹œì  ì œê±°
        if random.random() < 0.1:
            cut_length = random.randint(1, 3)
            num_feat[-cut_length:] = 0  # ë§ˆì§€ë§‰ ì‹œì  ì œê±°

        return num_feat, target

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        num_feat = self.numerical_tensor[idx].clone()
        cat_feat = self.categorical_tensor[idx].clone()
        padding_mask = self.padding_mask[idx].clone()

        if self.targets is not None:
            target = torch.from_numpy(self.targets[idx].copy())

            # Augmentation ì ìš©
            if self.augment:
                num_feat, target = self._apply_augmentation(num_feat, target)

            return num_feat, cat_feat, padding_mask, target
        else:
            return num_feat, cat_feat, padding_mask


def mixup_data(num_feat, cat_feat, target, alpha=0.2):
    """Mixup Augmentation"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = num_feat.size(0)
    index = torch.randperm(batch_size).to(num_feat.device)

    mixed_num_feat = lam * num_feat + (1 - lam) * num_feat[index]
    # categoricalì€ mixup ì•ˆí•¨ (ì˜ë¯¸ ì—†ìŒ)
    target_a, target_b = target, target[index]

    return mixed_num_feat, cat_feat, target_a, target_b, lam


class ImprovedTransformer(nn.Module):
    """V7: ê°œì„ ëœ Transformer + Multi-Task Learning"""

    def __init__(self, num_numerical_features, categorical_vocab_sizes, embedding_dims,
                 hidden_dim=512, num_layers=6, num_heads=8, dropout=0.1):
        super(ImprovedTransformer, self).__init__()

        # Embeddings
        self.embeddings = nn.ModuleDict()
        total_embedding_dim = 0

        for feat_name, vocab_size in categorical_vocab_sizes.items():
            emb_dim = embedding_dims[feat_name]
            self.embeddings[feat_name] = nn.Embedding(vocab_size, emb_dim, padding_idx=0)
            total_embedding_dim += emb_dim

        input_dim = num_numerical_features + total_embedding_dim

        # Input Projection
        self.input_projection = nn.Linear(input_dim, hidden_dim)
        self.input_norm = nn.LayerNorm(hidden_dim)

        # Positional Encoding
        self.pos_encoder = nn.Parameter(torch.randn(1, 100, hidden_dim) * 0.02)

        # Transformer
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim, nhead=num_heads,
            dim_feedforward=hidden_dim * 4, dropout=dropout,
            activation='gelu', batch_first=True, norm_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Main Task Head (ì¢Œí‘œ ì˜ˆì¸¡)
        self.coord_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 2)
        )

        # Auxiliary Task Head (ê±°ë¦¬ ì˜ˆì¸¡ - ì •ê·œí™” ë„ì›€)
        self.distance_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1)
        )

    def forward(self, num_feat, cat_feat, padding_mask):
        batch_size, seq_len, _ = num_feat.shape

        # Embedding
        embedded = []
        for i, feat_name in enumerate(self.embeddings.keys()):
            emb = self.embeddings[feat_name](cat_feat[:, :, i])
            embedded.append(emb)

        if embedded:
            embedded = torch.cat(embedded, dim=-1)
            x = torch.cat([num_feat, embedded], dim=-1)
        else:
            x = num_feat

        # Projection + PE
        x = self.input_projection(x)
        x = self.input_norm(x)
        x = x + self.pos_encoder[:, :seq_len, :]

        # Transformer
        x = self.transformer(x, src_key_padding_mask=padding_mask)

        # Pooling (CLS token style - ì²« ë²ˆì§¸ í† í° ì‚¬ìš©)
        # ë˜ëŠ” Global Average Pooling
        mask_expanded = (~padding_mask).unsqueeze(-1).float()
        x_masked = x * mask_expanded
        x_pooled = x_masked.sum(dim=1) / mask_expanded.sum(dim=1).clamp(min=1)

        # Outputs
        coord_pred = self.coord_head(x_pooled)
        distance_pred = self.distance_head(x_pooled)

        return coord_pred, distance_pred


class MultiTaskLoss(nn.Module):
    """Multi-Task Loss: ì¢Œí‘œ + ê±°ë¦¬"""

    def forward(self, coord_pred, distance_pred, target):
        # ì¢Œí‘œ ë³µì›
        pred_real = coord_pred.clone()
        pred_real[:, 0] *= 105.0
        pred_real[:, 1] *= 68.0

        target_real = target.clone()
        target_real[:, 0] *= 105.0
        target_real[:, 1] *= 68.0

        # Main Loss: ìœ í´ë¦¬ë“œ ê±°ë¦¬
        distances = torch.sqrt(torch.sum((pred_real - target_real) ** 2, dim=1))
        coord_loss = distances.mean()

        # Auxiliary Loss: ê±°ë¦¬ ì˜ˆì¸¡ (ì›ì ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬)
        target_dist = torch.sqrt(target_real[:, 0]**2 + target_real[:, 1]**2).unsqueeze(1)
        distance_loss = nn.functional.mse_loss(distance_pred, target_dist / 100.0)  # ì •ê·œí™”

        # Combined Loss
        total_loss = coord_loss + 0.1 * distance_loss

        return total_loss


def get_categorical_info(data, categorical_features, K=20):
    vocab_sizes = {}
    embedding_dims = {}

    for feat_name in categorical_features:
        cols = [f"{feat_name}_{i}" for i in range(K)]
        cols = [c for c in cols if c in data.columns]
        if not cols:
            continue

        max_val = data[cols].max().max()
        vocab_size = int(max_val) + 2
        emb_dim = min(max(vocab_size // 2, 4), 50)

        vocab_sizes[feat_name] = vocab_size
        embedding_dims[feat_name] = emb_dim

    return vocab_sizes, embedding_dims


def train_one_epoch(model, dataloader, criterion, optimizer, device, use_mixup=True):
    model.train()
    total_loss = 0.0

    for num_feat, cat_feat, padding_mask, target in tqdm(dataloader, desc="Training", leave=False):
        num_feat = num_feat.to(device)
        cat_feat = cat_feat.to(device)
        padding_mask = padding_mask.to(device)
        target = target.to(device)

        optimizer.zero_grad()

        # Mixup (50% í™•ë¥ )
        if use_mixup and random.random() < 0.5:
            num_feat, cat_feat, target_a, target_b, lam = mixup_data(num_feat, cat_feat, target)

            coord_pred, dist_pred = model(num_feat, cat_feat, padding_mask)

            loss_a = criterion(coord_pred, dist_pred, target_a)
            loss_b = criterion(coord_pred, dist_pred, target_b)
            loss = lam * loss_a + (1 - lam) * loss_b
        else:
            coord_pred, dist_pred = model(num_feat, cat_feat, padding_mask)
            loss = criterion(coord_pred, dist_pred, target)

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(dataloader)


def validate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0.0

    with torch.no_grad():
        for num_feat, cat_feat, padding_mask, target in tqdm(dataloader, desc="Validation", leave=False):
            num_feat = num_feat.to(device)
            cat_feat = cat_feat.to(device)
            padding_mask = padding_mask.to(device)
            target = target.to(device)

            coord_pred, dist_pred = model(num_feat, cat_feat, padding_mask)
            loss = criterion(coord_pred, dist_pred, target)
            total_loss += loss.item()

    return total_loss / len(dataloader)


def main():
    print("=" * 80)
    print("  V7 Model - Data Augmentation + Multi-Task Learning")
    print("  ëª©í‘œ: LightGBM (14.138m) ì´ˆê³¼")
    print("=" * 80)
    print()

    # í•˜ì´í¼íŒŒë¼ë¯¸í„°
    K = 20
    BATCH_SIZE = 32
    HIDDEN_DIM = 512
    NUM_LAYERS = 6  # ë” ê¹Šê²Œ
    NUM_HEADS = 8
    DROPOUT = 0.1  # ë‚®ì€ dropout (augmentationì´ regularization ì—­í• )
    LEARNING_RATE = 5e-4
    NUM_EPOCHS = 200  # ë” ê¸¸ê²Œ
    EARLY_STOPPING_PATIENCE = 40

    print(f"ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° (V7 - Data Augmentation):")
    print(f"   - Data Augmentation: âœ… (Noise, Flip, Cut, Mixup)")
    print(f"   - Multi-Task Learning: âœ… (Coord + Distance)")
    print(f"   - Hidden Dim: {HIDDEN_DIM}")
    print(f"   - Num Layers: {NUM_LAYERS}")
    print(f"   - Dropout: {DROPOUT} (ë‚®ìŒ - Augmentationì´ ëŒ€ì‹ )")
    print(f"   - Epochs: {NUM_EPOCHS}")
    print()

    # ë°ì´í„° ë¡œë”©
    print("ğŸ“Š ë°ì´í„° ë¡œë”©...")
    data = pd.read_csv('processed_train_data_v4.csv')
    print(f"ë°ì´í„° Shape: {data.shape}\n")

    game_ids = data['game_id'].values

    # ì²« ë²ˆì§¸ Fold
    print("ğŸ”§ First Fold í•™ìŠµ...")
    gkf = GroupKFold(n_splits=5)
    train_idx, val_idx = next(gkf.split(data, groups=game_ids))

    train_data = data.iloc[train_idx].copy()
    val_data = data.iloc[val_idx].copy()

    print(f"Train: {len(train_data):,}")
    print(f"Val: {len(val_data):,}")
    print()

    # Dataset (Augmentation ON)
    print("ğŸ“¦ Dataset V7 ìƒì„± (with Augmentation)...")
    train_dataset = SoccerDatasetV7(train_data, K=K, is_train=True, augment=True)
    val_dataset = SoccerDatasetV7(val_data, K=K, is_train=True, augment=False)  # Valì€ augment ì•ˆí•¨
    print()

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)

    # ë²”ì£¼í˜• ì •ë³´
    vocab_sizes, embedding_dims = get_categorical_info(data, train_dataset.categorical_features, K=K)

    # ëª¨ë¸
    print("ğŸ—ï¸ V7 ëª¨ë¸ ìƒì„±...")
    model = ImprovedTransformer(
        num_numerical_features=train_dataset.numerical_tensor.shape[2],
        categorical_vocab_sizes=vocab_sizes,
        embedding_dims=embedding_dims,
        hidden_dim=HIDDEN_DIM,
        num_layers=NUM_LAYERS,
        num_heads=NUM_HEADS,
        dropout=DROPOUT
    ).to(device)

    print(f"âœ… Parameters: {sum(p.numel() for p in model.parameters()):,}")
    print()

    criterion = MultiTaskLoss()
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, eta_min=1e-6)

    # í•™ìŠµ
    print("ğŸš€ í•™ìŠµ ì‹œì‘ (V7)...\n")
    best_val_loss = float('inf')
    patience_counter = 0

    for epoch in range(NUM_EPOCHS):
        print(f"Epoch {epoch+1}/{NUM_EPOCHS}")
        print("-" * 60)

        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, use_mixup=True)
        val_loss = validate(model, val_loader, criterion, device)
        scheduler.step()

        print(f"Train: {train_loss:.4f}m | Val: {val_loss:.4f}m")

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0

            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'val_loss': val_loss,
                'vocab_sizes': vocab_sizes,
                'embedding_dims': embedding_dims,
                'num_numerical_features': train_dataset.numerical_tensor.shape[2],
                'categorical_features': train_dataset.categorical_features,
                'numerical_features': train_dataset.numerical_features,
                'hyperparameters': {
                    'hidden_dim': HIDDEN_DIM,
                    'num_layers': NUM_LAYERS,
                    'num_heads': NUM_HEADS,
                    'dropout': DROPOUT,
                    'K': K,
                    'model_type': 'transformer_v7'
                }
            }, 'transformer_model_v7_augmented_best.pth')

            print(f"ğŸ’¾ Best! ({val_loss:.4f}m)")
        else:
            patience_counter += 1
            print(f"â³ {patience_counter}/{EARLY_STOPPING_PATIENCE}")

            if patience_counter >= EARLY_STOPPING_PATIENCE:
                print("\nâš ï¸ Early stopping!")
                break

        print()

    # ê²°ê³¼
    print("=" * 80)
    print("  í•™ìŠµ ì™„ë£Œ (V7)")
    print("=" * 80)
    print(f"\nâœ… Best Val Loss: {best_val_loss:.4f}m")

    print("\nğŸ“Š ì „ì²´ ë¹„êµ:")
    print(f"   - LightGBM V4: 14.138m")
    print(f"   - LSTM V5: 15.3157m")
    print(f"   - Transformer V7: {best_val_loss:.4f}m")

    if best_val_loss < 14.138:
        print("\nğŸ‰ğŸ‰ğŸ‰ LightGBM ì´ˆê³¼!")
    elif best_val_loss < 14.5:
        print("\nâœ… ë§¤ìš° ê·¼ì ‘! 5-Foldë¡œ ì´ˆê³¼ ê°€ëŠ¥!")
    else:
        print("\nğŸ“ˆ ì§€ì†ì  ê°œì„  ì¤‘...")

    print("=" * 80)


if __name__ == "__main__":
    main()

