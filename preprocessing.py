"""
K-League Pass Prediction - Data Preprocessing Pipeline

ëª©í‘œ: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
ê¸°ëŠ¥:
- ë°ì´í„° ë¡œë”© ë° ì •ë ¬
- ì¢Œí‘œ ì •ê·œí™”
- í”¼ì²˜ ìƒì„±
- Train/Val Split (Game-based)
"""

import pandas as pd
import numpy as np
import os
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import GroupKFold
from scipy.spatial.distance import cdist
import pickle
import warnings
warnings.filterwarnings('ignore')

class DataPreprocessor:
    def __init__(self, data_dir='./data'):
        self.data_dir = data_dir
        self.scaler_x = StandardScaler()
        self.scaler_y = StandardScaler()
        self.type_encoder = LabelEncoder()
        self.result_encoder = LabelEncoder()

    def load_data(self, verbose=True):
        """ë°ì´í„° ë¡œë”©"""
        if verbose:
            print("ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...")

        # Train ë°ì´í„°
        train_path = os.path.join(self.data_dir, 'train.csv')
        train_data = pd.read_csv(train_path)

        # Match info
        match_info_path = os.path.join(self.data_dir, 'match_info.csv')
        match_info = pd.read_csv(match_info_path)

        if verbose:
            print(f"âœ… Train: {len(train_data):,} ì´ë²¤íŠ¸, {train_data['game_episode'].nunique():,} ì—í”¼ì†Œë“œ")
            print(f"âœ… Match Info: {len(match_info)} ê²½ê¸°\n")

        return train_data, match_info

    def sort_by_time(self, data, verbose=True):
        """ì‹œê°„ ìˆœì„œë¡œ ì •ë ¬"""
        if verbose:
            print("â° ì‹œê°„ ìˆœì„œë¡œ ì •ë ¬ ì¤‘...")

        data = data.sort_values(['game_episode', 'time_seconds']).reset_index(drop=True)

        if verbose:
            print("âœ… ì •ë ¬ ì™„ë£Œ\n")

        return data

    def create_basic_features(self, data, verbose=True):
        """ê¸°ë³¸ í”¼ì²˜ ìƒì„±"""
        if verbose:
            print("ğŸ”§ ê¸°ë³¸ í”¼ì²˜ ìƒì„± ì¤‘...")

        # 1. ì´ë™ ê±°ë¦¬/ë°©í–¥
        data['delta_x'] = data['end_x'] - data['start_x']
        data['delta_y'] = data['end_y'] - data['start_y']
        data['distance'] = np.sqrt(data['delta_x']**2 + data['delta_y']**2)

        # 2. ê³¨ë¬¸ ê±°ë¦¬ (ì˜¤ë¥¸ìª½ ê³¨ë¬¸ ê¸°ì¤€: 105, 34)
        goal_x, goal_y = 105, 34
        data['distance_to_goal_start'] = np.sqrt(
            (data['start_x'] - goal_x)**2 +
            (data['start_y'] - goal_y)**2
        )
        data['distance_to_goal_end'] = np.sqrt(
            (data['end_x'] - goal_x)**2 +
            (data['end_y'] - goal_y)**2
        )

        # 2-1. ê³¨ë¬¸ ì§„í–‰ë„ (ê³¨ë¬¸ìœ¼ë¡œ ê°€ê¹Œì›Œì§€ëŠ” ì •ë„)
        data['goal_approach'] = data['distance_to_goal_start'] - data['distance_to_goal_end']

        # 3. ê²½ê¸°ì¥ ì˜ì—­ (3ë“±ë¶„)
        data['start_x_zone'] = pd.cut(data['start_x'], bins=[0, 35, 70, 105], labels=[0, 1, 2])
        data['start_y_zone'] = pd.cut(data['start_y'], bins=[0, 22.67, 45.33, 68], labels=[0, 1, 2])

        # 3-1. ì„¸ë°€í•œ ì˜ì—­ (5ë“±ë¶„) - ì „ìˆ ì  ì¤‘ìš”ë„
        data['start_x_zone_fine'] = pd.cut(data['start_x'], bins=[0, 21, 42, 63, 84, 105], labels=[0, 1, 2, 3, 4])

        # 3-2. ìœ„í—˜ ì§€ì—­ í”Œë˜ê·¸ (í˜ë„í‹° ë°•ìŠ¤: x > 87.5, 22.9 < y < 45.1)
        data['in_penalty_area'] = ((data['start_x'] > 87.5) &
                                   (data['start_y'] > 22.9) &
                                   (data['start_y'] < 45.1)).astype(int)

        # 3-3. ìµœì¢… 1/3 ì§€ì—­ (Final Third)
        data['in_final_third'] = (data['start_x'] > 70).astype(int)

        # 4. ì—í”¼ì†Œë“œ ë‚´ ìˆœì„œ
        data['event_order'] = data.groupby('game_episode').cumcount()

        # 5. ê³¨ ê°ë„ (Shooting Angle) - ê³¨ëŒ€ ì–‘ í¬ìŠ¤íŠ¸ì™€ ì´ë£¨ëŠ” ê°ë„
        # ê³¨ëŒ€: (105, 30.34) ~ (105, 37.66) - ì•½ 7.32m
        post_left_y = 30.34
        post_right_y = 37.66

        # ì–‘ í¬ìŠ¤íŠ¸ê¹Œì§€ì˜ ë²¡í„°
        vec_left_x = goal_x - data['start_x']
        vec_left_y = post_left_y - data['start_y']
        vec_right_x = goal_x - data['start_x']
        vec_right_y = post_right_y - data['start_y']

        # ë‚´ì ê³¼ ì™¸ì ìœ¼ë¡œ ê°ë„ ê³„ì‚°
        dot_product = vec_left_x * vec_right_x + vec_left_y * vec_right_y
        cross_product = vec_left_x * vec_right_y - vec_left_y * vec_right_x

        data['shooting_angle'] = np.abs(np.arctan2(cross_product, dot_product))

        # 6. ì •ê·œí™”ëœ ì¢Œí‘œ (0~1 ìŠ¤ì¼€ì¼) - ê²½ê¸°ì¥ ê·œê²© ë°˜ì˜
        data['start_x_norm'] = data['start_x'] / 105.0
        data['start_y_norm'] = data['start_y'] / 68.0
        data['end_x_norm'] = data['end_x'] / 105.0
        data['end_y_norm'] = data['end_y'] / 68.0

        if verbose:
            print(f"âœ… ìƒì„±ëœ í”¼ì²˜: delta_x/y, distance, distance_to_goal, goal_approach,")
            print(f"   x/y_zone, penalty_area, final_third, shooting_angle, normalized_coords\n")

        return data

    def create_sequence_features(self, data, verbose=True):
        """ì‹œí€€ìŠ¤ í”¼ì²˜ ìƒì„± (ê° ì—í”¼ì†Œë“œë³„) - ì „ìˆ ì  ìš”ì†Œ ê°•í™”"""
        if verbose:
            print("ğŸ”„ ì‹œí€€ìŠ¤ í”¼ì²˜ ìƒì„± ì¤‘...")

        # ì—í”¼ì†Œë“œë³„ ì²˜ë¦¬
        episode_features = []

        for episode_id, group in data.groupby('game_episode'):
            group = group.copy()

            # 1. ì—í”¼ì†Œë“œ ê¸¸ì´
            episode_length = len(group)
            group['episode_length'] = episode_length

            # 2. Xì¶• ëˆ„ì  ì§„í–‰ë„ (ê³µê²© ì „ê°œ)
            first_x = group.iloc[0]['start_x']
            last_x = group.iloc[-1]['end_x']
            group['x_progression'] = group['start_x'] - first_x
            group['x_total_progression'] = last_x - first_x  # ì „ì²´ ì§„í–‰ë„

            # 3. ìƒëŒ€ ì‹œê°„ (ì—í”¼ì†Œë“œ ë‚´)
            start_time = group.iloc[0]['time_seconds']
            group['relative_time'] = group['time_seconds'] - start_time

            # 4. í…œí¬ (ì´ë²¤íŠ¸ë‹¹ í‰ê·  ì‹œê°„)
            if episode_length > 1:
                duration = group.iloc[-1]['time_seconds'] - group.iloc[0]['time_seconds']
                tempo = duration / episode_length if episode_length > 0 else 0
            else:
                tempo = 0
            group['tempo'] = tempo

            # 5. ì†ë„ ê³„ì‚° (m/s) - ì´ë²¤íŠ¸ ê°„
            time_diff = group['time_seconds'].diff()
            group['velocity_x'] = group['start_x'].diff() / time_diff.replace(0, np.nan)
            group['velocity_y'] = group['start_y'].diff() / time_diff.replace(0, np.nan)
            group['velocity'] = np.sqrt(group['velocity_x']**2 + group['velocity_y']**2)

            # 6. ê°€ì†ë„ (m/sÂ²)
            group['acceleration'] = group['velocity'].diff() / time_diff.replace(0, np.nan)

            # 7. í…œí¬ ë³€í™”ìœ¨ (ê¸‰ê²©í•œ í”Œë ˆì´ ë³€í™” ê°ì§€)
            group['tempo_change'] = tempo - group['relative_time'].rolling(2).mean().fillna(0)

            # 8. ì§„í–‰ ë°©í–¥ ë²¡í„°ì˜ ì¼ê´€ì„± (ê³µê²© ì „ê°œ ë°©í–¥ì„±)
            # ì´ì „ ì´ë²¤íŠ¸ì™€ í˜„ì¬ ì´ë²¤íŠ¸ì˜ ë°©í–¥ ìœ ì‚¬ë„
            prev_delta_x = group['delta_x'].shift(1)
            prev_delta_y = group['delta_y'].shift(1)

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            dot_prod = group['delta_x'] * prev_delta_x + group['delta_y'] * prev_delta_y
            mag_curr = np.sqrt(group['delta_x']**2 + group['delta_y']**2)
            mag_prev = np.sqrt(prev_delta_x**2 + prev_delta_y**2)
            group['direction_consistency'] = dot_prod / (mag_curr * mag_prev + 1e-10)

            # 9. ì§€ì—­ë³„ ì²´ë¥˜ ì‹œê°„
            # ìµœì¢… 1/3 ì§€ì—­ì—ì„œì˜ ì‹œê°„ ë¹„ìœ¨
            if 'in_final_third' in group.columns:
                final_third_time = group[group['in_final_third'] == 1]['relative_time'].sum()
                total_time = group['relative_time'].iloc[-1] if len(group) > 0 else 1
                group['final_third_time_ratio'] = final_third_time / (total_time + 1e-10)

            # 10. ìˆ˜í‰/ìˆ˜ì§ ì§„í–‰ë„ ë¹„ìœ¨
            total_horizontal = group['delta_x'].abs().sum()
            total_vertical = group['delta_y'].abs().sum()
            group['horizontal_vertical_ratio'] = total_horizontal / (total_vertical + 1e-10)

            episode_features.append(group)

        data = pd.concat(episode_features, ignore_index=True)

        if verbose:
            print(f"âœ… ìƒì„±ëœ í”¼ì²˜: episode_length, x_progression, velocity, acceleration,")
            print(f"   tempo_change, direction_consistency, spatial_ratios\n")

        return data

    def create_previous_event_features(self, data, verbose=True):
        """ì§ì „ ì´ë²¤íŠ¸ í”¼ì²˜ ìƒì„±"""
        if verbose:
            print("â¬…ï¸  ì§ì „ ì´ë²¤íŠ¸ í”¼ì²˜ ìƒì„± ì¤‘...")

        # ì—í”¼ì†Œë“œë³„ ì²˜ë¦¬
        episode_features = []

        for episode_id, group in data.groupby('game_episode'):
            group = group.copy()

            # ì§ì „ ì´ë²¤íŠ¸ ì •ë³´
            group['prev_type_name'] = group['type_name'].shift(1)
            group['prev_result_name'] = group['result_name'].shift(1)
            group['prev_start_x'] = group['start_x'].shift(1)
            group['prev_start_y'] = group['start_y'].shift(1)
            group['prev_end_x'] = group['end_x'].shift(1)
            group['prev_end_y'] = group['end_y'].shift(1)

            # ì§ì „ 2ê°œ ì´ë²¤íŠ¸
            group['prev2_type_name'] = group['type_name'].shift(2)

            episode_features.append(group)

        data = pd.concat(episode_features, ignore_index=True)

        if verbose:
            print(f"âœ… ìƒì„±ëœ í”¼ì²˜: prev_type_name, prev_result_name, prev_start_x/y, prev_end_x/y\n")

        return data

    def create_advanced_tactical_features(self, data, verbose=True):
        """ê³ ê¸‰ ì „ìˆ  í”¼ì²˜ ìƒì„± - ì••ë°•, ê³µê°„ ì°½ì¶œ, íŒ¨ìŠ¤ ë„¤íŠ¸ì›Œí¬"""
        if verbose:
            print("âš½ ê³ ê¸‰ ì „ìˆ  í”¼ì²˜ ìƒì„± ì¤‘...")

        episode_features = []

        for episode_id, group in data.groupby('game_episode'):
            group = group.copy()

            # ===== 1. ì••ë°• ê°•ë„ (Pressure Intensity) =====
            # ë‹¨ìˆœí™”: ê°™ì€ íŒ€ì˜ ì´ë²¤íŠ¸ ë°€ë„ë¥¼ ì••ë°•ìœ¼ë¡œ ê°„ì£¼
            # ì‹¤ì œë¡œëŠ” ìƒëŒ€ íŒ€ ì„ ìˆ˜ ë°ì´í„°ê°€ í•„ìš”í•˜ì§€ë§Œ,
            # ì—¬ê¸°ì„œëŠ” ì´ë²¤íŠ¸ ê°„ ê±°ë¦¬ì™€ í…œí¬ë¡œ ì••ë°•ì„ ì¶”ì •

            # 1-1. ì´ë²¤íŠ¸ ë°€ë„ (ë‹¨ìœ„ ë©´ì ë‹¹ ì´ë²¤íŠ¸ ìˆ˜)
            if len(group) > 1:
                # ì—í”¼ì†Œë“œ ì „ì²´ì˜ í™œë™ ì˜ì—­ ê³„ì‚°
                x_range = group['start_x'].max() - group['start_x'].min() + 1
                y_range = group['start_y'].max() - group['start_y'].min() + 1
                area = x_range * y_range
                event_density = len(group) / area
            else:
                event_density = 0
            group['event_density'] = event_density

            # 1-2. ë¡œì»¬ ì••ë°• ì ìˆ˜ (ì£¼ë³€ ë°˜ê²½ ë‚´ ì´ë²¤íŠ¸ ìˆ˜) - ìµœì í™” ë²„ì „
            pressure_radius = 10  # 10m ë°˜ê²½

            # ì—í”¼ì†Œë“œê°€ ë„ˆë¬´ ê¸¸ë©´ ìƒ˜í”Œë§ (ì„±ëŠ¥ ìµœì í™”)
            if len(group) > 50:
                # ëŒ€í‘œê°’ ì‚¬ìš©: í‰ê· ì ì¸ ì••ë°• ê°•ë„
                group['local_pressure'] = event_density * 10  # ê·¼ì‚¬ê°’
                group['weighted_pressure'] = event_density * 5
            else:
                # ì •í™•í•œ ê³„ì‚° (ì—í”¼ì†Œë“œê°€ ì§§ì„ ë•Œë§Œ)
                positions = group[['start_x', 'start_y']].values
                n_events = len(positions)

                # ê±°ë¦¬ í–‰ë ¬ ê³„ì‚° (ë²¡í„°í™”)
                dist_matrix = cdist(positions, positions, metric='euclidean')

                # ë¡œì»¬ ì••ë°• (ë°˜ê²½ ë‚´ ê°œìˆ˜)
                nearby_mask = (dist_matrix <= pressure_radius) & (dist_matrix > 0)
                group['local_pressure'] = nearby_mask.sum(axis=1)

                # ê°€ì¤‘ ì••ë°• (ê±°ë¦¬ ì—­ìˆ˜ í•©)
                weights_matrix = 1 / (dist_matrix + 1)
                np.fill_diagonal(weights_matrix, 0)  # ìê¸° ìì‹  ì œì™¸
                group['weighted_pressure'] = weights_matrix.sum(axis=1)

            # ===== 2. ê³µê°„ ì°½ì¶œ ì§€í‘œ (Space Creation) =====
            # 2-1. ì´ë²¤íŠ¸ ê°„ ê±°ë¦¬ ë³€í™”ìœ¨ (ê³µê°„ì´ ì—´ë¦¬ëŠ”ì§€)
            prev_distance = group['distance'].shift(1)
            group['distance_change_rate'] = (group['distance'] - prev_distance) / (prev_distance + 1e-10)

            # 2-2. ìˆ˜ì§ ê³µê°„ í™œìš© (ë„“ì´ í™•ì¥)
            group['vertical_spread'] = group['start_y'].rolling(window=3, min_periods=1).std()

            # 2-3. ê³µê²© í­ (Attack Width) - ìµœê·¼ Nê°œ ì´ë²¤íŠ¸ì˜ Y ë²”ìœ„
            group['attack_width'] = group['start_y'].rolling(window=5, min_periods=1).apply(
                lambda x: x.max() - x.min(), raw=True
            )

            # ===== 3. ì „ìˆ ì  ë²¡í„° (Tactical Vectors) =====
            # 3-1. ê³µê²© ëª¨ë©˜í…€ (ëˆ„ì  ì „ì§„ ê±°ë¦¬)
            group['forward_momentum'] = group['delta_x'].rolling(window=3, min_periods=1).sum()

            # 3-2. íŒ¨ìŠ¤ ì²´ì¸ ê°ë„ ë³€í™” (ì „ìˆ ì  ë³€í™”)
            # ì´ì „ íŒ¨ìŠ¤ ê°ë„ì™€ í˜„ì¬ íŒ¨ìŠ¤ ê°ë„ì˜ ì°¨ì´
            current_angle = np.arctan2(group['delta_y'], group['delta_x'])
            prev_angle = current_angle.shift(1)
            angle_change = np.abs(current_angle - prev_angle)
            # ë¼ë””ì•ˆ ë²”ìœ„ë¥¼ 0~pië¡œ ì •ê·œí™”
            angle_change = np.minimum(angle_change, 2*np.pi - angle_change)
            group['pass_angle_change'] = angle_change

            # ===== 4. íˆìŠ¤í† ë¦¬ ê¸°ë°˜ íŒ¨í„´ (Rolling Statistics) =====
            # 4-1. ìµœê·¼ 3ê°œ ì´ë²¤íŠ¸ì˜ í‰ê·  ì†ë„
            if 'velocity' in group.columns:
                group['avg_velocity_3'] = group['velocity'].rolling(window=3, min_periods=1).mean()

            # 4-2. ìµœê·¼ 3ê°œ ì´ë²¤íŠ¸ì˜ ê³¨ ì ‘ê·¼ë„
            if 'distance_to_goal_end' in group.columns:
                group['goal_approach_trend'] = group['distance_to_goal_end'].rolling(
                    window=3, min_periods=1
                ).apply(lambda x: x.iloc[0] - x.iloc[-1] if len(x) > 1 else 0, raw=False)

            # ===== 5. ìµœì  ê²½ë¡œ íƒìƒ‰ (Optimal Path) =====
            # 5-1. ê³¨ë¬¸ê¹Œì§€ ì§ì„  ê±°ë¦¬ vs ì‹¤ì œ ì´ë™ ê±°ë¦¬ ë¹„ìœ¨ (íš¨ìœ¨ì„±)
            if len(group) > 1:
                first_goal_dist = group.iloc[0]['distance_to_goal_start']
                last_goal_dist = group.iloc[-1]['distance_to_goal_end']
                actual_distance = group['distance'].sum()

                # ì§ì„  ê±°ë¦¬ ë³€í™” vs ì‹¤ì œ ì´ë™
                direct_progress = first_goal_dist - last_goal_dist
                path_efficiency = direct_progress / (actual_distance + 1e-10)
            else:
                path_efficiency = 0

            group['path_efficiency'] = path_efficiency

            # ===== 6. íŒ€ ì¤‘ì‹¬ì  ëŒ€ë¹„ ìœ„ì¹˜ (Relative Positioning) =====
            # ì—í”¼ì†Œë“œ ë‚´ ëª¨ë“  ì´ë²¤íŠ¸ì˜ í‰ê·  ìœ„ì¹˜ë¥¼ íŒ€ ì¤‘ì‹¬ìœ¼ë¡œ ê°„ì£¼
            team_center_x = group['start_x'].mean()
            team_center_y = group['start_y'].mean()

            group['dist_from_team_center'] = np.sqrt(
                (group['start_x'] - team_center_x)**2 +
                (group['start_y'] - team_center_y)**2
            )

            # ===== 7. ê²½ê¸° í˜ì´ì¦ˆ ë¶„ì„ =====
            # ì‹œê°„ëŒ€ë³„ ê²½ê¸° íŠ¹ì„± (ì´ˆë°˜/ì¤‘ë°˜/í›„ë°˜)
            if 'time_seconds' in group.columns:
                max_time = 5400  # 90ë¶„ = 5400ì´ˆ
                group['match_phase'] = pd.cut(
                    group['time_seconds'],
                    bins=[0, 1800, 3600, max_time],
                    labels=[0, 1, 2]  # ì´ˆë°˜/ì¤‘ë°˜/í›„ë°˜
                ).astype(float)

            episode_features.append(group)

        data = pd.concat(episode_features, ignore_index=True)

        if verbose:
            print(f"âœ… ìƒì„±ëœ ê³ ê¸‰ í”¼ì²˜: pressure, space_creation, momentum, path_efficiency,")
            print(f"   team_positioning, match_phase\n")

        return data

    def extract_last_events(self, data, verbose=True):
        """ê° ì—í”¼ì†Œë“œì˜ ë§ˆì§€ë§‰ ì´ë²¤íŠ¸ë§Œ ì¶”ì¶œ (ì˜ˆì¸¡ ëŒ€ìƒ)"""
        if verbose:
            print("ğŸ¯ ë§ˆì§€ë§‰ ì´ë²¤íŠ¸ ì¶”ì¶œ ì¤‘...")

        last_events = data.groupby('game_episode').tail(1).copy()

        if verbose:
            print(f"âœ… {len(last_events):,}ê°œ ì—í”¼ì†Œë“œì˜ ë§ˆì§€ë§‰ ì´ë²¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ\n")

        return last_events

    def encode_categorical(self, data, fit=True, verbose=True):
        """ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© (fit=Falseì¼ ë•ŒëŠ” ì´ë¯¸ í•™ìŠµëœ ì¸ì½”ë” ì‚¬ìš©)"""
        if verbose:
            print("ğŸ”¤ ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© ì ìš© ì¤‘...")

        # type_name ì¸ì½”ë”© (Unknownìœ¼ë¡œ fillna)
        data['type_name'] = data['type_name'].fillna('Unknown')
        data['type_name_encoded'] = self.type_encoder.transform(data['type_name'])

        if 'prev_type_name' in data.columns:
            data['prev_type_name'] = data['prev_type_name'].fillna('Unknown')
            data['prev_type_name_encoded'] = self.type_encoder.transform(data['prev_type_name'])

        if 'prev2_type_name' in data.columns:
            data['prev2_type_name'] = data['prev2_type_name'].fillna('Unknown')
            data['prev2_type_name_encoded'] = self.type_encoder.transform(data['prev2_type_name'])

        # result_name ì¸ì½”ë”©
        if 'result_name' in data.columns and len(self.result_encoder.classes_) > 0:
            data['result_name'] = data['result_name'].fillna('Unknown')
            data['result_name_encoded'] = self.result_encoder.transform(data['result_name'])

            if 'prev_result_name' in data.columns:
                data['prev_result_name'] = data['prev_result_name'].fillna('Unknown')
                data['prev_result_name_encoded'] = self.result_encoder.transform(data['prev_result_name'])

        if verbose:
            print(f"âœ… ì¸ì½”ë”© ì ìš© ì™„ë£Œ\n")

        return data

    def fill_missing(self, data, verbose=True):
        """ê²°ì¸¡ì¹˜ ì²˜ë¦¬"""
        if verbose:
            print("ğŸ”§ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì¤‘...")

        # ì§ì „ ì´ë²¤íŠ¸ í”¼ì²˜ì˜ NaN (ì²« ë²ˆì§¸ ì´ë²¤íŠ¸)
        prev_cols = [col for col in data.columns if col.startswith('prev_')]

        for col in prev_cols:
            if col.endswith('_encoded'):
                data[col] = data[col].fillna(-1)  # íŠ¹ìˆ˜ ê°’
            elif data[col].dtype in ['float64', 'int64']:
                data[col] = data[col].fillna(0)  # ìˆ«ìëŠ” 0
            else:
                data[col] = data[col].fillna('Unknown')  # ë¬¸ìëŠ” Unknown

        # zone í”¼ì²˜ (categoricalì„ numericìœ¼ë¡œ ë³€í™˜)
        if 'start_x_zone' in data.columns:
            data['start_x_zone'] = data['start_x_zone'].astype(float).fillna(-1)
            data['start_y_zone'] = data['start_y_zone'].astype(float).fillna(-1)

        if verbose:
            print(f"âœ… ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ\n")

        return data

    def normalize_coordinates(self, data, fit=True, verbose=True):
        """ì¢Œí‘œ ì •ê·œí™” (ì„ íƒì )"""
        if verbose:
            print("ğŸ“ ì¢Œí‘œ ì •ê·œí™” ì¤‘...")

        coord_cols = ['start_x', 'start_y']

        if fit:
            data[coord_cols] = self.scaler_x.fit_transform(data[coord_cols])
        else:
            data[coord_cols] = self.scaler_x.transform(data[coord_cols])

        # ì§ì „ ì´ë²¤íŠ¸ ì¢Œí‘œë„ ì •ê·œí™”
        if 'prev_start_x' in data.columns:
            prev_coord_cols = ['prev_start_x', 'prev_start_y', 'prev_end_x', 'prev_end_y']
            prev_coord_cols = [col for col in prev_coord_cols if col in data.columns]

            # NaNì´ ì•„ë‹Œ ê°’ë§Œ ì •ê·œí™”
            for col in prev_coord_cols:
                mask = data[col].notna()
                if mask.sum() > 0:
                    if 'x' in col:
                        data.loc[mask, col] = self.scaler_x.transform(data.loc[mask, [col.replace('prev_', '').replace('end_', 'start_')]])[col.replace('prev_', '').replace('end_', 'start_')]

        if verbose:
            print(f"âœ… ì¢Œí‘œ ì •ê·œí™” ì™„ë£Œ\n")

        return data

    def create_train_val_split(self, data, n_splits=5, random_state=42, verbose=True):
        """Game-based K-Fold Split"""
        if verbose:
            print(f"ğŸ“Š {n_splits}-Fold Game-based Split ìƒì„± ì¤‘...")

        # ê²Œì„ ID ì¶”ì¶œ
        games = data['game_id'].unique()

        # GroupKFold (ê²Œì„ ë‹¨ìœ„)
        gkf = GroupKFold(n_splits=n_splits)

        splits = []
        for fold, (train_idx, val_idx) in enumerate(gkf.split(data, groups=data['game_id'])):
            train_games = data.iloc[train_idx]['game_id'].unique()
            val_games = data.iloc[val_idx]['game_id'].unique()

            splits.append({
                'fold': fold,
                'train_idx': train_idx,
                'val_idx': val_idx,
                'train_games': train_games,
                'val_games': val_games
            })

            if verbose:
                print(f"  Fold {fold+1}: Train {len(train_games)} games ({len(train_idx):,} episodes), "
                      f"Val {len(val_games)} games ({len(val_idx):,} episodes)")

        if verbose:
            print()

        return splits

    def fit_encoders(self, data, verbose=True):
        """ì „ì²´ ë°ì´í„°ì—ì„œ ì¸ì½”ë”ë¥¼ ë¨¼ì € fit"""
        if verbose:
            print("ğŸ”¤ ì¸ì½”ë” í•™ìŠµ ì¤‘ (ì „ì²´ ë°ì´í„°)...")

        # type_name ìˆ˜ì§‘
        all_types = set()
        for col in ['type_name', 'prev_type_name', 'prev2_type_name']:
            if col in data.columns:
                all_types.update(data[col].fillna('Unknown').unique())

        self.type_encoder.fit(list(all_types))

        # result_name ìˆ˜ì§‘
        all_results = set()
        for col in ['result_name', 'prev_result_name']:
            if col in data.columns:
                all_results.update(data[col].fillna('Unknown').unique())

        if len(all_results) > 0:
            self.result_encoder.fit(list(all_results))

        if verbose:
            print(f"âœ… ì¸ì½”ë” í•™ìŠµ ì™„ë£Œ: {len(self.type_encoder.classes_)}ê°œ ì´ë²¤íŠ¸ íƒ€ì…, "
                  f"{len(self.result_encoder.classes_) if len(all_results) > 0 else 0}ê°œ ê²°ê³¼ íƒ€ì…\n")

    def preprocess_pipeline(self, normalize_coords=False, verbose=True):
        """ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        print("=" * 80)
        print("  K-League Pass Prediction - ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸")
        print("=" * 80)
        print()

        # 1. ë°ì´í„° ë¡œë”©
        train_data, match_info = self.load_data(verbose=verbose)

        # 2. ì‹œê°„ ì •ë ¬
        train_data = self.sort_by_time(train_data, verbose=verbose)

        # 3. ê¸°ë³¸ í”¼ì²˜ ìƒì„±
        train_data = self.create_basic_features(train_data, verbose=verbose)

        # 4. ì‹œí€€ìŠ¤ í”¼ì²˜ ìƒì„±
        train_data = self.create_sequence_features(train_data, verbose=verbose)

        # 5. ì§ì „ ì´ë²¤íŠ¸ í”¼ì²˜
        train_data = self.create_previous_event_features(train_data, verbose=verbose)

        # 5.5. ê³ ê¸‰ ì „ìˆ  í”¼ì²˜ ìƒì„± (NEW!)
        train_data = self.create_advanced_tactical_features(train_data, verbose=verbose)

        # 5.6. ì¸ì½”ë” í•™ìŠµ (ë§ˆì§€ë§‰ ì´ë²¤íŠ¸ ì¶”ì¶œ ì „ì— ì „ì²´ ë°ì´í„°ë¡œ fit)
        self.fit_encoders(train_data, verbose=verbose)

        # 6. ë§ˆì§€ë§‰ ì´ë²¤íŠ¸ë§Œ ì¶”ì¶œ
        last_events = self.extract_last_events(train_data, verbose=verbose)

        # 7. ë²”ì£¼í˜• ì¸ì½”ë”©
        last_events = self.encode_categorical(last_events, fit=False, verbose=verbose)

        # 8. ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        last_events = self.fill_missing(last_events, verbose=verbose)

        # 9. ì¢Œí‘œ ì •ê·œí™” (ì„ íƒì )
        if normalize_coords:
            last_events = self.normalize_coordinates(last_events, fit=True, verbose=verbose)

        # 10. Train/Val Split
        splits = self.create_train_val_split(last_events, n_splits=5, verbose=verbose)

        print("=" * 80)
        print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print("=" * 80)

        return last_events, splits

    def get_feature_columns(self):
        """í”¼ì²˜ ì»¬ëŸ¼ ëª©ë¡ ë°˜í™˜ (ëª¨ë“  ì „ìˆ  í”¼ì²˜ í¬í•¨)"""
        feature_cols = [
            # ===== ê¸°ë³¸ ìœ„ì¹˜ ë° ì´ë™ =====
            'start_x', 'start_y',
            'delta_x', 'delta_y', 'distance',

            # ì •ê·œí™” ì¢Œí‘œ
            'start_x_norm', 'start_y_norm',

            # ===== ê³¨ ê´€ë ¨ =====
            'distance_to_goal_start', 'distance_to_goal_end',
            'goal_approach',
            'shooting_angle',

            # ===== ì˜ì—­ ë¶„í•  =====
            'start_x_zone', 'start_y_zone', 'start_x_zone_fine',
            'in_penalty_area', 'in_final_third',

            # ===== ì—í”¼ì†Œë“œ ì •ë³´ =====
            'episode_length', 'event_order',
            'x_progression', 'x_total_progression',
            'relative_time', 'tempo',

            # ===== ì†ë„ ë° ê°€ì†ë„ =====
            'velocity', 'velocity_x', 'velocity_y',
            'acceleration',

            # ===== ì „ìˆ ì  íë¦„ =====
            'tempo_change',
            'direction_consistency',
            'horizontal_vertical_ratio',
            'final_third_time_ratio',

            # ===== ì••ë°• ê°•ë„ (Pressure) =====
            'event_density',
            'local_pressure',
            'weighted_pressure',

            # ===== ê³µê°„ ì°½ì¶œ =====
            'distance_change_rate',
            'vertical_spread',
            'attack_width',

            # ===== ì „ìˆ ì  ë²¡í„° =====
            'forward_momentum',
            'pass_angle_change',

            # ===== íˆìŠ¤í† ë¦¬ ê¸°ë°˜ =====
            'avg_velocity_3',
            'goal_approach_trend',

            # ===== ìµœì  ê²½ë¡œ =====
            'path_efficiency',

            # ===== íŒ€ í¬ì§€ì…”ë‹ =====
            'dist_from_team_center',

            # ===== ê²½ê¸° í˜ì´ì¦ˆ =====
            'match_phase',

            # ===== ì´ë²¤íŠ¸ íƒ€ì… =====
            'type_name_encoded',

            # ===== ì§ì „ ì´ë²¤íŠ¸ =====
            'prev_type_name_encoded',
            'prev_start_x', 'prev_start_y',
            'prev_end_x', 'prev_end_y',

            # ===== ì§ì „ 2ê°œ =====
            'prev2_type_name_encoded',

            # ===== ê²½ê¸° ì •ë³´ =====
            'period_id', 'is_home'
        ]

        # result_nameì´ ìˆìœ¼ë©´ ì¶”ê°€
        if hasattr(self, 'result_encoder') and len(self.result_encoder.classes_) > 0:
            feature_cols.extend(['result_name_encoded', 'prev_result_name_encoded'])

        return feature_cols

    def save_preprocessor(self, filename='preprocessor.pkl'):
        """ì „ì²˜ë¦¬ ê°ì²´ ì €ì¥"""
        with open(filename, 'wb') as f:
            pickle.dump({
                'scaler_x': self.scaler_x,
                'scaler_y': self.scaler_y,
                'type_encoder': self.type_encoder,
                'result_encoder': self.result_encoder
            }, f)
        print(f"âœ… Preprocessor ì €ì¥: {filename}")

    def load_preprocessor(self, filename='preprocessor.pkl'):
        """ì „ì²˜ë¦¬ ê°ì²´ ë¡œë”©"""
        with open(filename, 'rb') as f:
            saved = pickle.load(f)
            self.scaler_x = saved['scaler_x']
            self.scaler_y = saved['scaler_y']
            self.type_encoder = saved['type_encoder']
            self.result_encoder = saved['result_encoder']
        print(f"âœ… Preprocessor ë¡œë”©: {filename}")

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    # Preprocessor ì´ˆê¸°í™”
    preprocessor = DataPreprocessor(data_dir='./data')

    # ì „ì²˜ë¦¬ ì‹¤í–‰
    processed_data, splits = preprocessor.preprocess_pipeline(
        normalize_coords=False,  # ì¢Œí‘œ ì •ê·œí™” ì•ˆí•¨ (XGBoostëŠ” ë¶ˆí•„ìš”)
        verbose=True
    )

    # í”¼ì²˜ ì»¬ëŸ¼ í™•ì¸
    feature_cols = preprocessor.get_feature_columns()

    print("\n" + "=" * 80)
    print(f"ğŸ“Š ìµœì¢… ë°ì´í„° Shape: {processed_data.shape}")
    print(f"ğŸ“Š í”¼ì²˜ ê°œìˆ˜: {len(feature_cols)}")
    print(f"ğŸ“Š Fold ê°œìˆ˜: {len(splits)}")
    print("=" * 80)

    print("\nâœ… ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜:")
    for i, col in enumerate(feature_cols, 1):
        if col in processed_data.columns:
            print(f"  {i:2d}. {col}")
        else:
            print(f"  {i:2d}. {col} (ì—†ìŒ)")

    # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
    print("\n" + "=" * 80)
    print("ğŸ“Š ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 5í–‰):")
    print("=" * 80)
    sample_cols = ['game_episode', 'start_x', 'start_y', 'end_x', 'end_y',
                   'episode_length', 'type_name_encoded', 'prev_type_name_encoded']
    print(processed_data[sample_cols].head())

    # Preprocessor ì €ì¥
    preprocessor.save_preprocessor('preprocessor.pkl')

    # ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
    processed_data.to_csv('processed_train_data.csv', index=False)
    print(f"\nâœ… ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥: processed_train_data.csv")

    return processed_data, splits, preprocessor

if __name__ == "__main__":
    processed_data, splits, preprocessor = main()

