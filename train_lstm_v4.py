"""
K-League Pass Prediction - PyTorch LSTM/GRU í•™ìŠµ íŒŒì´í”„ë¼ì¸

V4 Wide Format ë°ì´í„°ë¥¼ 3D ì‹œí€€ìŠ¤ í…ì„œë¡œ ë³€í™˜í•˜ì—¬ ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ
âœ… ë°ì´í„° ì •ê·œí™” (ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§)
âœ… Input Projection Layer
âœ… NaN ì²˜ë¦¬ (íŒ¨ë”© â†’ 0 ë³€í™˜)
âœ… Embedding for Categorical Features

ì‘ì„±ì¼: 2025-12-18
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import GroupKFold
import warnings
import re
from tqdm import tqdm

warnings.filterwarnings('ignore')

# Device ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”§ Using device: {device}")


class SoccerDatasetV4(Dataset):
    """
    V4 Wide Format ë°ì´í„°ë¥¼ 3D ì‹œí€€ìŠ¤ í…ì„œë¡œ ë³€í™˜í•˜ëŠ” Dataset

    Features:
    - Wide format â†’ (Batch, SeqLen=20, Features) 3D tensor
    - NaN â†’ 0.0 ë³€í™˜
    - ì¢Œí‘œ ì •ê·œí™” (start_x/end_x â†’ /105, start_y/end_y â†’ /68)
    - Categorical/Numerical ìë™ ë¶„ë¥˜
    """

    def __init__(self, data, K=20, is_train=True):
        """
        Args:
            data: pd.DataFrame (Wide format V4 data)
            K: ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸ 20)
            is_train: Train/Val êµ¬ë¶„
        """
        self.data = data.reset_index(drop=True)
        self.K = K
        self.is_train = is_train

        # íƒ€ê²Ÿ ì¶”ì¶œ
        if 'target_x' in data.columns and 'target_y' in data.columns:
            self.targets = data[['target_x', 'target_y']].values.astype(np.float32)
            # íƒ€ê²Ÿë„ ì •ê·œí™”
            self.targets[:, 0] /= 105.0  # target_x
            self.targets[:, 1] /= 68.0   # target_y
        else:
            self.targets = None

        # ë©”íƒ€ ì •ë³´ ì œì™¸ (game_episode, game_id, target_x, target_y, final_team_id)
        exclude_cols = ['game_episode', 'game_id', 'target_x', 'target_y', 'final_team_id']
        feature_data = data.drop(columns=[c for c in exclude_cols if c in data.columns])

        # ì»¬ëŸ¼ ë¶„ë¥˜ (ìë™)
        self.numerical_features, self.categorical_features = self._classify_columns(feature_data.columns)

        # 3D í…ì„œë¡œ ë³€í™˜ + ì •ê·œí™”
        self.numerical_tensor = self._prepare_numerical_features(feature_data)
        self.categorical_tensor = self._prepare_categorical_features(feature_data)

        print(f"âœ… Dataset ì¤€ë¹„ ì™„ë£Œ:")
        print(f"   - ìƒ˜í”Œ ìˆ˜: {len(self.data)}")
        print(f"   - ìˆ˜ì¹˜í˜• í”¼ì²˜: {len(self.numerical_features)} â†’ Shape: {self.numerical_tensor.shape}")
        print(f"   - ë²”ì£¼í˜• í”¼ì²˜: {len(self.categorical_features)} â†’ Shape: {self.categorical_tensor.shape}")

    def _classify_columns(self, columns):
        """ì»¬ëŸ¼ëª…ì—ì„œ _{index} íŒ¨í„´ ì¶”ì¶œí•˜ì—¬ ìˆ˜ì¹˜í˜•/ë²”ì£¼í˜• ë¶„ë¥˜"""
        pattern = re.compile(r'^(.+)_(\d+)$')

        # ê³ ìœ  feature ì´ë¦„ ì¶”ì¶œ
        feature_names = set()
        for col in columns:
            match = pattern.match(col)
            if match:
                feature_names.add(match.group(1))

        # ë²”ì£¼í˜• í‚¤ì›Œë“œ (Embedding ì‚¬ìš© ëŒ€ìƒ)
        categorical_keywords = ['type_id', 'res_id', 'team_id_enc', 'is_home', 'is_last', 'period_id']

        categorical_features = []
        numerical_features = []

        for feat in sorted(feature_names):
            if any(keyword in feat for keyword in categorical_keywords):
                categorical_features.append(feat)
            else:
                numerical_features.append(feat)

        return numerical_features, categorical_features

    def _prepare_numerical_features(self, data):
        """ìˆ˜ì¹˜í˜• í”¼ì²˜ë¥¼ 3D í…ì„œë¡œ ë³€í™˜ + ì •ê·œí™”"""
        tensors = []

        # ì¢Œí‘œ ê´€ë ¨ ì»¬ëŸ¼ ì‹ë³„ (ì •ê·œí™” ëŒ€ìƒ)
        x_coord_keywords = ['start_x', 'end_x', 'dx']  # X ì¢Œí‘œ ê´€ë ¨ (105ë¡œ ë‚˜ëˆ”)
        y_coord_keywords = ['start_y', 'end_y', 'dy']  # Y ì¢Œí‘œ ê´€ë ¨ (68ë¡œ ë‚˜ëˆ”)

        for feat_name in self.numerical_features:
            # ê° ì‹œì ë³„ ì»¬ëŸ¼ (0~19)
            cols = [f"{feat_name}_{i}" for i in range(self.K)]
            cols = [c for c in cols if c in data.columns]

            if not cols:
                continue

            # ë°ì´í„° ì¶”ì¶œ
            feat_data = data[cols].values.astype(np.float32)

            # ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ Kë³´ë‹¤ ì§§ì€ ê²½ìš° ì˜¤ë¥¸ìª½ì— 0 íŒ¨ë”©
            if feat_data.shape[1] < self.K:
                padding = np.zeros((feat_data.shape[0], self.K - feat_data.shape[1]), dtype=np.float32)
                feat_data = np.concatenate([feat_data, padding], axis=1)

            # ì •ê·œí™” ì ìš©
            if any(kw in feat_name for kw in x_coord_keywords):
                # X ì¢Œí‘œ ê´€ë ¨: 105ë¡œ ë‚˜ëˆ”
                feat_data = feat_data / 105.0
            elif any(kw in feat_name for kw in y_coord_keywords):
                # Y ì¢Œí‘œ ê´€ë ¨: 68ë¡œ ë‚˜ëˆ”
                feat_data = feat_data / 68.0
            # ë‚˜ë¨¸ì§€ ìˆ˜ì¹˜í˜•ì€ ê·¸ëŒ€ë¡œ (ì´ë¯¸ ì ì ˆí•œ ë²”ìœ„ì´ê±°ë‚˜ ë¹„ìœ¨)

            # NaN â†’ 0.0
            feat_data = np.nan_to_num(feat_data, nan=0.0)

            tensors.append(feat_data)

        # (N, K, num_features)
        result = np.stack(tensors, axis=-1) if tensors else np.zeros((len(data), self.K, 0), dtype=np.float32)
        return torch.from_numpy(result)

    def _prepare_categorical_features(self, data):
        """ë²”ì£¼í˜• í”¼ì²˜ë¥¼ 3D í…ì„œë¡œ ë³€í™˜ (ì •ìˆ˜ ì¸ì½”ë”© ìœ ì§€)"""
        tensors = []

        for feat_name in self.categorical_features:
            cols = [f"{feat_name}_{i}" for i in range(self.K)]
            cols = [c for c in cols if c in data.columns]

            if not cols:
                continue

            feat_data = data[cols].values.astype(np.float32)

            # ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ Kë³´ë‹¤ ì§§ì€ ê²½ìš° ì˜¤ë¥¸ìª½ì— 0 íŒ¨ë”©
            if feat_data.shape[1] < self.K:
                padding = np.zeros((feat_data.shape[0], self.K - feat_data.shape[1]), dtype=np.float32)
                feat_data = np.concatenate([feat_data, padding], axis=1)

            # NaN â†’ 0 (Unknown ë²”ì£¼)
            feat_data = np.nan_to_num(feat_data, nan=0.0)

            tensors.append(feat_data)

        # (N, K, cat_features)
        result = np.stack(tensors, axis=-1) if tensors else np.zeros((len(data), self.K, 0), dtype=np.float32)
        return torch.from_numpy(result).long()  # Long tensor for embedding

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        num_feat = self.numerical_tensor[idx]  # (K, num_features)
        cat_feat = self.categorical_tensor[idx]  # (K, cat_features)

        if self.targets is not None:
            target = torch.from_numpy(self.targets[idx])
            return num_feat, cat_feat, target
        else:
            return num_feat, cat_feat


class SoccerRNN(nn.Module):
    """
    Embedding + Input Projection + GRU/LSTM ê¸°ë°˜ íŒ¨ìŠ¤ ì˜ˆì¸¡ ëª¨ë¸

    Architecture:
    1. Categorical Embedding
    2. Concatenate (Numerical + Embedded)
    3. Input Projection (Linear)
    4. GRU/LSTM
    5. Output Head (ë§ˆì§€ë§‰ hidden state â†’ target_x, target_y)
    """

    def __init__(self,
                 num_numerical_features,
                 categorical_vocab_sizes,
                 embedding_dims,
                 hidden_dim=256,
                 num_layers=2,
                 dropout=0.3,
                 use_lstm=False):
        """
        Args:
            num_numerical_features: ìˆ˜ì¹˜í˜• í”¼ì²˜ ê°œìˆ˜
            categorical_vocab_sizes: ë²”ì£¼í˜• ë³€ìˆ˜ë³„ ì–´íœ˜ í¬ê¸° (dict)
            embedding_dims: ë²”ì£¼í˜• ë³€ìˆ˜ë³„ ì„ë² ë”© ì°¨ì› (dict)
            hidden_dim: RNN hidden dimension
            num_layers: RNN ë ˆì´ì–´ ìˆ˜
            dropout: Dropout ë¹„ìœ¨
            use_lstm: Trueë©´ LSTM, Falseë©´ GRU
        """
        super(SoccerRNN, self).__init__()

        self.num_numerical_features = num_numerical_features
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.use_lstm = use_lstm

        # Embedding layers
        self.embeddings = nn.ModuleDict()
        total_embedding_dim = 0

        for feat_name, vocab_size in categorical_vocab_sizes.items():
            emb_dim = embedding_dims[feat_name]
            self.embeddings[feat_name] = nn.Embedding(vocab_size, emb_dim, padding_idx=0)
            total_embedding_dim += emb_dim

        # Input dimension
        input_dim = num_numerical_features + total_embedding_dim

        # Input Projection Layer (í•µì‹¬ ê°œì„ ì‚¬í•­)
        self.input_projection = nn.Linear(input_dim, hidden_dim)

        # RNN Layer
        if use_lstm:
            self.rnn = nn.LSTM(
                hidden_dim,
                hidden_dim,
                num_layers=num_layers,
                dropout=dropout if num_layers > 1 else 0,
                batch_first=True
            )
        else:
            self.rnn = nn.GRU(
                hidden_dim,
                hidden_dim,
                num_layers=num_layers,
                dropout=dropout if num_layers > 1 else 0,
                batch_first=True
            )

        # Output Head
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 2)  # (target_x, target_y)
        )

    def forward(self, num_feat, cat_feat):
        """
        Args:
            num_feat: (batch, seq_len, num_features)
            cat_feat: (batch, seq_len, cat_features)

        Returns:
            (batch, 2) - (target_x, target_y)
        """
        batch_size, seq_len, _ = num_feat.shape

        # Embedding
        embedded = []
        for i, feat_name in enumerate(self.embeddings.keys()):
            emb = self.embeddings[feat_name](cat_feat[:, :, i])  # (batch, seq_len, emb_dim)
            embedded.append(emb)

        # Concatenate
        if embedded:
            embedded = torch.cat(embedded, dim=-1)  # (batch, seq_len, total_emb_dim)
            x = torch.cat([num_feat, embedded], dim=-1)  # (batch, seq_len, input_dim)
        else:
            x = num_feat

        # Input Projection
        x = self.input_projection(x)  # (batch, seq_len, hidden_dim)

        # RNN
        rnn_out, _ = self.rnn(x)  # (batch, seq_len, hidden_dim)

        # ë§ˆì§€ë§‰ ì‹œì ì˜ hidden state
        last_hidden = rnn_out[:, -1, :]  # (batch, hidden_dim)

        # Output
        output = self.fc(last_hidden)  # (batch, 2)

        return output


class EuclideanDistanceLoss(nn.Module):
    """ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜ ì†ì‹¤ í•¨ìˆ˜ (í‰ê°€ì§€í‘œì™€ ì¼ì¹˜)"""

    def forward(self, pred, target):
        """
        Args:
            pred: (batch, 2) - (pred_x, pred_y) [0~1 normalized]
            target: (batch, 2) - (target_x, target_y) [0~1 normalized]

        Returns:
            í‰ê·  ìœ í´ë¦¬ë“œ ê±°ë¦¬ (ì‹¤ì œ ë¯¸í„° ë‹¨ìœ„)
        """
        # ì‹¤ì œ ì¢Œí‘œë¡œ ë³µì›
        pred_real = pred.clone()
        pred_real[:, 0] *= 105.0
        pred_real[:, 1] *= 68.0

        target_real = target.clone()
        target_real[:, 0] *= 105.0
        target_real[:, 1] *= 68.0

        # ìœ í´ë¦¬ë“œ ê±°ë¦¬
        distances = torch.sqrt(torch.sum((pred_real - target_real) ** 2, dim=1))
        return distances.mean()


def get_categorical_info(data, categorical_features, K=20):
    """ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ì–´íœ˜ í¬ê¸°ì™€ ì„ë² ë”© ì°¨ì› ê³„ì‚°"""
    vocab_sizes = {}
    embedding_dims = {}

    for feat_name in categorical_features:
        cols = [f"{feat_name}_{i}" for i in range(K)]
        cols = [c for c in cols if c in data.columns]

        if not cols:
            continue

        # ìµœëŒ€ê°’ (ì–´íœ˜ í¬ê¸°)
        max_val = data[cols].max().max()
        vocab_size = int(max_val) + 2  # 0: padding, 1~max_val: ì‹¤ì œ ê°’

        # ì„ë² ë”© ì°¨ì› (íœ´ë¦¬ìŠ¤í‹±: min(vocab_size // 2, 50))
        emb_dim = min(max(vocab_size // 2, 4), 50)

        vocab_sizes[feat_name] = vocab_size
        embedding_dims[feat_name] = emb_dim

    return vocab_sizes, embedding_dims


def train_one_epoch(model, dataloader, criterion, optimizer, device):
    """1 ì—í¬í¬ í•™ìŠµ"""
    model.train()
    total_loss = 0.0

    for num_feat, cat_feat, target in tqdm(dataloader, desc="Training", leave=False):
        num_feat = num_feat.to(device)
        cat_feat = cat_feat.to(device)
        target = target.to(device)

        optimizer.zero_grad()

        output = model(num_feat, cat_feat)
        loss = criterion(output, target)

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(dataloader)


def validate(model, dataloader, criterion, device):
    """ê²€ì¦"""
    model.eval()
    total_loss = 0.0

    with torch.no_grad():
        for num_feat, cat_feat, target in tqdm(dataloader, desc="Validation", leave=False):
            num_feat = num_feat.to(device)
            cat_feat = cat_feat.to(device)
            target = target.to(device)

            output = model(num_feat, cat_feat)
            loss = criterion(output, target)

            total_loss += loss.item()

    return total_loss / len(dataloader)


def main():
    print("=" * 80)
    print("  PyTorch LSTM/GRU - V4 Wide Format í•™ìŠµ")
    print("  ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì‹œí€€ìŠ¤ ëª¨ë¸ë§")
    print("=" * 80)
    print()

    # í•˜ì´í¼íŒŒë¼ë¯¸í„°
    K = 20
    BATCH_SIZE = 128
    HIDDEN_DIM = 256
    NUM_LAYERS = 2
    DROPOUT = 0.3
    LEARNING_RATE = 1e-3
    NUM_EPOCHS = 50
    EARLY_STOPPING_PATIENCE = 10
    USE_LSTM = False  # False: GRU, True: LSTM

    print(f"ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    print(f"   - Sequence Length: {K}")
    print(f"   - Batch Size: {BATCH_SIZE}")
    print(f"   - Hidden Dim: {HIDDEN_DIM}")
    print(f"   - Num Layers: {NUM_LAYERS}")
    print(f"   - Dropout: {DROPOUT}")
    print(f"   - Learning Rate: {LEARNING_RATE}")
    print(f"   - Epochs: {NUM_EPOCHS}")
    print(f"   - RNN Type: {'LSTM' if USE_LSTM else 'GRU'}")
    print()

    # 1. ë°ì´í„° ë¡œë”©
    print("ğŸ“Š ë°ì´í„° ë¡œë”©...")
    data = pd.read_csv('processed_train_data_v4.csv')
    print(f"ë°ì´í„° Shape: {data.shape}")
    print()

    # game_id ì¶”ì¶œ (GroupKFoldìš©)
    game_ids = data['game_id'].values

    # 2. ì²« ë²ˆì§¸ Foldë§Œ ì‚¬ìš© (í”„ë¡œí† íƒ€ì´í•‘)
    print("ğŸ”§ First Fold í•™ìŠµ (ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘)...")
    gkf = GroupKFold(n_splits=5)
    train_idx, val_idx = next(gkf.split(data, groups=game_ids))

    train_data = data.iloc[train_idx].copy()
    val_data = data.iloc[val_idx].copy()

    print(f"Train: {len(train_data):,} ìƒ˜í”Œ")
    print(f"Val: {len(val_data):,} ìƒ˜í”Œ")
    print()

    # 3. Dataset ìƒì„±
    print("ğŸ“¦ Dataset ìƒì„± ì¤‘...")
    train_dataset = SoccerDatasetV4(train_data, K=K, is_train=True)
    val_dataset = SoccerDatasetV4(val_data, K=K, is_train=True)
    print()

    # DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=0,  # Windows í˜¸í™˜
        pin_memory=True if torch.cuda.is_available() else False
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=0,
        pin_memory=True if torch.cuda.is_available() else False
    )

    # 4. ë²”ì£¼í˜• ì •ë³´ ì¶”ì¶œ
    print("ğŸ”¤ ë²”ì£¼í˜• ë³€ìˆ˜ ì •ë³´ ì¶”ì¶œ ì¤‘...")
    vocab_sizes, embedding_dims = get_categorical_info(
        data, train_dataset.categorical_features, K=K
    )

    print("ë²”ì£¼í˜• ë³€ìˆ˜:")
    for feat_name in vocab_sizes.keys():
        print(f"   - {feat_name:20s}: Vocab={vocab_sizes[feat_name]:3d}, Emb_Dim={embedding_dims[feat_name]:2d}")
    print()

    # 5. ëª¨ë¸ ìƒì„±
    print("ğŸ—ï¸ ëª¨ë¸ ìƒì„± ì¤‘...")
    model = SoccerRNN(
        num_numerical_features=train_dataset.numerical_tensor.shape[2],
        categorical_vocab_sizes=vocab_sizes,
        embedding_dims=embedding_dims,
        hidden_dim=HIDDEN_DIM,
        num_layers=NUM_LAYERS,
        dropout=DROPOUT,
        use_lstm=USE_LSTM
    ).to(device)

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ:")
    print(f"   - Total Parameters: {total_params:,}")
    print(f"   - Trainable Parameters: {trainable_params:,}")
    print()

    # 6. Loss & Optimizer
    criterion = EuclideanDistanceLoss()
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5
    )

    # 7. í•™ìŠµ ë£¨í”„
    print("ğŸš€ í•™ìŠµ ì‹œì‘...\n")
    best_val_loss = float('inf')
    patience_counter = 0

    for epoch in range(NUM_EPOCHS):
        print(f"Epoch {epoch+1}/{NUM_EPOCHS}")
        print("-" * 60)

        # Train
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)

        # Validate
        val_loss = validate(model, val_loader, criterion, device)

        print(f"Train Loss: {train_loss:.4f}m | Val Loss: {val_loss:.4f}m")

        # Learning Rate Scheduler
        scheduler.step(val_loss)

        # Early Stopping & Model Saving
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0

            # ëª¨ë¸ ì €ì¥
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'vocab_sizes': vocab_sizes,
                'embedding_dims': embedding_dims,
                'num_numerical_features': train_dataset.numerical_tensor.shape[2],
                'categorical_features': train_dataset.categorical_features,
                'numerical_features': train_dataset.numerical_features,
                'hyperparameters': {
                    'hidden_dim': HIDDEN_DIM,
                    'num_layers': NUM_LAYERS,
                    'dropout': DROPOUT,
                    'use_lstm': USE_LSTM,
                    'K': K
                }
            }, 'lstm_model_v4_best.pth')

            print(f"ğŸ’¾ Best model saved! (Val Loss: {val_loss:.4f}m)")
        else:
            patience_counter += 1
            print(f"â³ Patience: {patience_counter}/{EARLY_STOPPING_PATIENCE}")

            if patience_counter >= EARLY_STOPPING_PATIENCE:
                print(f"\nâš ï¸ Early stopping triggered!")
                break

        print()

    # 8. ìµœì¢… ê²°ê³¼
    print("=" * 80)
    print("  í•™ìŠµ ì™„ë£Œ!")
    print("=" * 80)
    print(f"\nâœ… Best Validation Loss: {best_val_loss:.4f}m")
    print(f"âœ… ëª¨ë¸ ì €ì¥: lstm_model_v4_best.pth")

    print("\nğŸ“Š ì„±ëŠ¥ ë¹„êµ:")
    print(f"   - LightGBM V4 (5-Fold): ~1.5m")
    print(f"   - LSTM/GRU V4 (Fold 1): {best_val_loss:.4f}m")

    if best_val_loss < 1.5:
        print("\nğŸ‰ ë§¤ìš° ìš°ìˆ˜í•œ ì„±ëŠ¥! ë”¥ëŸ¬ë‹ì´ íŠ¸ë¦¬ ëª¨ë¸ë³´ë‹¤ íš¨ê³¼ì ì…ë‹ˆë‹¤!")
    elif best_val_loss < 2.0:
        print("\nâœ… ì¢‹ì€ ì„±ëŠ¥! ì¶”ê°€ íŠœë‹ìœ¼ë¡œ ê°œì„  ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        print("\nğŸ“ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš” (Hidden Dim, Learning Rate ë“±)")

    print("\n" + "=" * 80)
    print("ë‹¤ìŒ ë‹¨ê³„:")
    print("   1. inference_lstm_v4.py ì‘ì„± (Test ì¶”ë¡ )")
    print("   2. 5-Fold ì „ì²´ í•™ìŠµ (lstm_model_v4_5fold.py)")
    print("   3. LightGBM vs LSTM ì•™ìƒë¸”")
    print("=" * 80)


if __name__ == "__main__":
    main()

