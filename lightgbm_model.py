"""
K-League Pass Prediction - LightGBM Model

ëª©í‘œ: XGBoost ëŒ€ë¹„ ì„±ëŠ¥ ë¹„êµ ë° ì•™ìƒë¸” ì¤€ë¹„
ì˜ˆìƒ ì„±ëŠ¥: 1.0 ~ 1.3m (ë‹¨ë…), 0.9 ~ 1.0m (ì•™ìƒë¸”)
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.metrics import mean_squared_error
import pickle
import warnings
warnings.filterwarnings('ignore')

def euclidean_distance(y_true, y_pred):
    """ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°"""
    true_x, true_y = y_true[:, 0], y_true[:, 1]
    pred_x, pred_y = y_pred[:, 0], y_pred[:, 1]
    return np.mean(np.sqrt((true_x - pred_x)**2 + (true_y - pred_y)**2))

class LightGBMModel:
    def __init__(self):
        self.model_x = None
        self.model_y = None

    def train(self, X_train, y_train, X_val, y_val, params=None, verbose=True):
        """LightGBM ëª¨ë¸ í•™ìŠµ"""

        if params is None:
            params = {
                'objective': 'regression',
                'metric': 'rmse',
                'boosting_type': 'gbdt',
                'num_leaves': 127,  # ì°¸ê³  ì½”ë“œì²˜ëŸ¼ ì¦ê°€
                'learning_rate': 0.05,  # ë” ì‘ì€ learning rate
                'min_data_in_leaf': 80,
                'feature_fraction': 0.8,
                'bagging_fraction': 0.8,
                'bagging_freq': 1,
                'verbose': -1,
                'random_state': 42,
                'n_estimators': 3000  # ì°¸ê³  ì½”ë“œì²˜ëŸ¼ í¬ê²Œ ì¦ê°€
            }

        if verbose:
            print("=" * 80)
            print("  LightGBM ëª¨ë¸ í•™ìŠµ")
            print("=" * 80)
            print(f"\nğŸ“Š í•™ìŠµ ë°ì´í„°: {X_train.shape}")
            print(f"ğŸ“Š ê²€ì¦ ë°ì´í„°: {X_val.shape}\n")

        # end_x ì˜ˆì¸¡ ëª¨ë¸
        if verbose:
            print("ğŸ”µ end_x ëª¨ë¸ í•™ìŠµ ì¤‘... (ìµœëŒ€ 3000 rounds)")

        self.model_x = lgb.LGBMRegressor(**params)
        self.model_x.fit(
            X_train, y_train[:, 0],
            eval_set=[(X_val, y_val[:, 0])],
            callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]  # ë” ì—¬ìœ ìˆê²Œ
        )

        if verbose:
            print(f"  â†’ ìµœì¢… {self.model_x.best_iteration_} rounds í•™ìŠµ ì™„ë£Œ")

        # end_y ì˜ˆì¸¡ ëª¨ë¸
        if verbose:
            print("ğŸ”´ end_y ëª¨ë¸ í•™ìŠµ ì¤‘... (ìµœëŒ€ 3000 rounds)")

        self.model_y = lgb.LGBMRegressor(**params)
        self.model_y.fit(
            X_train, y_train[:, 1],
            eval_set=[(X_val, y_val[:, 1])],
            callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]
        )

        if verbose:
            print(f"  â†’ ìµœì¢… {self.model_y.best_iteration_} rounds í•™ìŠµ ì™„ë£Œ")
            print("âœ… í•™ìŠµ ì™„ë£Œ!\n")

    def predict(self, X):
        """ì˜ˆì¸¡"""
        pred_x = self.model_x.predict(X)
        pred_y = self.model_y.predict(X)
        return np.column_stack([pred_x, pred_y])

    def evaluate(self, X, y_true, verbose=True):
        """í‰ê°€"""
        y_pred = self.predict(X)

        # ìœ í´ë¦¬ë“œ ê±°ë¦¬
        eucl_dist = euclidean_distance(y_true, y_pred)

        # MSE (ê°œë³„)
        mse_x = mean_squared_error(y_true[:, 0], y_pred[:, 0])
        mse_y = mean_squared_error(y_true[:, 1], y_pred[:, 1])

        if verbose:
            print(f"ğŸ“Š í‰ê°€ ê²°ê³¼:")
            print(f"  - ìœ í´ë¦¬ë“œ ê±°ë¦¬: {eucl_dist:.2f}m")
            print(f"  - MSE X: {mse_x:.2f}")
            print(f"  - MSE Y: {mse_y:.2f}")

        return eucl_dist, mse_x, mse_y

    def get_feature_importance(self, feature_names, top_n=20):
        """í”¼ì²˜ ì¤‘ìš”ë„"""
        importance_x = self.model_x.feature_importances_
        importance_y = self.model_y.feature_importances_

        # í‰ê·  ì¤‘ìš”ë„
        importance_avg = (importance_x + importance_y) / 2

        # DataFrameìœ¼ë¡œ ì •ë¦¬
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance_x': importance_x,
            'importance_y': importance_y,
            'importance_avg': importance_avg
        }).sort_values('importance_avg', ascending=False)

        return importance_df.head(top_n)

    def save_model(self, filename='lightgbm_model.pkl'):
        """ëª¨ë¸ ì €ì¥"""
        with open(filename, 'wb') as f:
            pickle.dump({
                'model_x': self.model_x,
                'model_y': self.model_y
            }, f)
        print(f"âœ… ëª¨ë¸ ì €ì¥: {filename}")

    def load_model(self, filename='lightgbm_model.pkl'):
        """ëª¨ë¸ ë¡œë”©"""
        with open(filename, 'rb') as f:
            saved = pickle.load(f)
            self.model_x = saved['model_x']
            self.model_y = saved['model_y']
        print(f"âœ… ëª¨ë¸ ë¡œë”©: {filename}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 80)
    print("  K-League Pass Prediction - LightGBM Model")
    print("  ëª©í‘œ: XGBoost ëŒ€ë¹„ ì„±ëŠ¥ ë¹„êµ")
    print("=" * 80)
    print()

    # ğŸ”¥ ì‹¤ë¬´ íŒ¨í„´: ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš©ìœ¼ë¡œ ì½”ë“œ ê°„ì†Œí™”
    from train_utils import (
        load_data_and_features,
        prepare_train_val_split,
        euclidean_distance,
        print_performance_summary,
        get_feature_group_importance,
        print_feature_group_importance
    )

    # 1. ë°ì´í„° ë° í”¼ì²˜ ì„¤ì • ë¡œë”© (JSON ìë™ ì‚¬ìš©)
    data, feature_cols, target_cols, config = load_data_and_features()

    # 2. Train/Val Split (ê²Œì„ ê¸°ë°˜)
    X_train, y_train, X_val, y_val = prepare_train_val_split(
        data, feature_cols, target_cols, val_ratio=0.2
    )

    # 3. ëª¨ë¸ í•™ìŠµ
    model = LightGBMModel()
    model.train(X_train, y_train, X_val, y_val, verbose=True)

    # 4. í‰ê°€ (ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš©)
    print("\n" + "=" * 80)
    print("  ëª¨ë¸ í‰ê°€")
    print("=" * 80)

    y_train_pred = model.predict(X_train)
    y_val_pred = model.predict(X_val)

    train_eucl = euclidean_distance(y_train, y_train_pred)
    val_eucl = euclidean_distance(y_val, y_val_pred)

    print(f"\n[Train Set]")
    print(f"  - ìœ í´ë¦¬ë“œ ê±°ë¦¬: {train_eucl:.2f}m")

    print(f"\n[Validation Set]")
    print(f"  - ìœ í´ë¦¬ë“œ ê±°ë¦¬: {val_eucl:.2f}m")

    # 5. ì„±ëŠ¥ ìš”ì•½ (ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš©)
    print_performance_summary(train_eucl, val_eucl)


    # 6. XGBoostì™€ ë¹„êµ
    print("\n" + "=" * 80)
    print("  XGBoost vs LightGBM ë¹„êµ")
    print("=" * 80)

    xgb_val_eucl = 1.24  # XGBoost Validation ì„±ëŠ¥ (ìµœì‹ )

    print(f"\nğŸ“Š XGBoost:  {xgb_val_eucl:.2f}m")
    print(f"ğŸ“Š LightGBM: {val_eucl:.2f}m")

    diff = xgb_val_eucl - val_eucl
    if diff > 0:
        print(f"âœ… LightGBMì´ {diff:.2f}m ë” ì¢‹ìŠµë‹ˆë‹¤!")
    elif diff < 0:
        print(f"âš ï¸  XGBoostê°€ {-diff:.2f}m ë” ì¢‹ìŠµë‹ˆë‹¤.")
    else:
        print(f"ğŸ“Š ë™ì¼í•œ ì„±ëŠ¥ì…ë‹ˆë‹¤.")

    # 7. í”¼ì²˜ ê·¸ë£¹ë³„ ì¤‘ìš”ë„ ë¶„ì„
    group_importance = get_feature_group_importance(
        model.model_x, model.model_y, feature_cols, config
    )
    print_feature_group_importance(group_importance)

    # 8. ëª¨ë¸ ì €ì¥
    print("\n" + "=" * 80)
    model.save_model('lightgbm_model.pkl')

    # 9. ìµœì¢… ìš”ì•½
    print("\n" + "=" * 80)
    print("  ì‹¤í–‰ ì™„ë£Œ!")
    print("=" * 80)

    ensemble_expected = (xgb_val_eucl + val_eucl) / 2 * 0.90  # ì•™ìƒë¸” íš¨ê³¼ 10%

    print(f"\nâœ… LightGBM ëª¨ë¸ ê°œë°œ ì™„ë£Œ!")
    print(f"   - Val ì„±ëŠ¥: {val_eucl:.2f}m")
    print(f"   - í”¼ì²˜ ê°œìˆ˜: {len(feature_cols)}")
    print(f"   - ëª¨ë¸ ì €ì¥: lightgbm_model.pkl")
    print(f"\nğŸ“Š ì•™ìƒë¸” ì˜ˆìƒ ì„±ëŠ¥:")
    print(f"   - XGBoost + LightGBM: {ensemble_expected:.2f}m")
    if ensemble_expected < 1.0:
        print(f"   ğŸ¯ 1.0m ì´í•˜ ë‹¬ì„± ê°€ëŠ¥!")

    return model, val_eucl

if __name__ == "__main__":
    model, val_eucl = main()

